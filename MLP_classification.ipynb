{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01c7XMOSIwbT"
      },
      "source": [
        "# Classification with MLPs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5tRpXdLxIwbV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twAxiZanPi0_",
        "outputId": "5253bddc-e99d-445d-8805-0ec6d4189ebe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.12)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.32.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1L7akQOOPltn"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbrmDDzePngr",
        "outputId": "8b0f9f8b-c3f6-4e77-a577-1c14975eb4e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login --relogin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "19_lPMQNIwbW",
        "outputId": "906a7e84-f815-4f48-e5b6-09641cccc4cd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8c4232e6-e0ca-424f-ad4d-065bbd26f810\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.9968</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.9970</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.9980</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c4232e6-e0ca-424f-ad4d-065bbd26f810')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8c4232e6-e0ca-424f-ad4d-065bbd26f810 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8c4232e6-e0ca-424f-ad4d-065bbd26f810');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c2715c77-28aa-463b-93c0-87f32db5c116\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c2715c77-28aa-463b-93c0-87f32db5c116')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c2715c77-28aa-463b-93c0-87f32db5c116 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
              "Id                                                                            \n",
              "0             7.4              0.70         0.00             1.9      0.076   \n",
              "1             7.8              0.88         0.00             2.6      0.098   \n",
              "2             7.8              0.76         0.04             2.3      0.092   \n",
              "3            11.2              0.28         0.56             1.9      0.075   \n",
              "4             7.4              0.70         0.00             1.9      0.076   \n",
              "\n",
              "    free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
              "Id                                                                        \n",
              "0                  11.0                  34.0   0.9978  3.51       0.56   \n",
              "1                  25.0                  67.0   0.9968  3.20       0.68   \n",
              "2                  15.0                  54.0   0.9970  3.26       0.65   \n",
              "3                  17.0                  60.0   0.9980  3.16       0.58   \n",
              "4                  11.0                  34.0   0.9978  3.51       0.56   \n",
              "\n",
              "    alcohol  quality  \n",
              "Id                    \n",
              "0       9.4        5  \n",
              "1       9.8        5  \n",
              "2       9.8        5  \n",
              "3       9.8        6  \n",
              "4       9.4        5  "
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load wine quality dataset\n",
        "wine = pd.read_csv('/content/WineQT.csv', index_col = -1)\n",
        "wine.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZ0GE_VfIwbX",
        "outputId": "7b560d31-47fe-408c-b018-43fe3c94d83e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((731, 17), (229, 17), (183, 17))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# split into train, test, validation\n",
        "# onehot encoding of quality\n",
        "wine = pd.get_dummies(wine, columns=['quality'])\n",
        "\n",
        "# standardize data apart from the last column\n",
        "# standardize each column of the data\n",
        "data = wine.iloc[:, :-6].values\n",
        "data = (data - data.mean(axis=0)) / data.std(axis=0)\n",
        "wine.iloc[:, :-6] = data\n",
        "\n",
        "train, test = train_test_split(wine, test_size=0.2, random_state=42)\n",
        "train, validation = train_test_split(train, test_size=0.2, random_state=42)\n",
        "train_x = train.iloc[:, :-6]\n",
        "train_y = train.iloc[:, -6:]\n",
        "test_x = test.iloc[:, :-6]\n",
        "test_y = test.iloc[:, -6:]\n",
        "validation_x = validation.iloc[:, :-6]\n",
        "validation_y = validation.iloc[:, -6:]\n",
        "# convert to numpy array\n",
        "train_x = np.array(train_x)\n",
        "train_y = np.array(train_y)\n",
        "test_x = np.array(test_x)\n",
        "test_y = np.array(test_y)\n",
        "validation_x = np.array(validation_x)\n",
        "validation_y = np.array(validation_y)\n",
        "train.shape, test.shape, validation.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "riYSdAiRIwbX"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoidprime(x):\n",
        "    return np.exp(-x)/((1 + np.exp(-x))**2)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanhprime(x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def reluprime(x):\n",
        "    return np.where(x > 0, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9y1gNXTDIwbY"
      },
      "outputs": [],
      "source": [
        "class MLP():\n",
        "    def __init__(self, learning_rate, activation_function, optimizers, hidden_layers, neurons):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizers = optimizers\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.neurons = neurons\n",
        "        self.inputlayersize = 0\n",
        "        self.outputlayersize = 0\n",
        "        self.activationfunction = activation_function\n",
        "        self.X = None\n",
        "\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        if activation_function == \"sigmoid\":\n",
        "            self.activation_function = sigmoid\n",
        "            self.backprop_function = sigmoidprime\n",
        "        if activation_function == \"tanh\":\n",
        "            self.activation_function = tanh\n",
        "            self.backprop_function = tanhprime\n",
        "        if activation_function == \"relu\":\n",
        "            self.activation_function = relu\n",
        "            self.backprop_function = reluprime\n",
        "\n",
        "    def weightsbiases(self):\n",
        "        # initialize weights, biases\n",
        "        self.inputlayersize = self.X.shape[1]\n",
        "        self.outputlayersize = self.y.shape[1]\n",
        "        self.weights.append(np.random.randn(self.inputlayersize, self.neurons[0]))\n",
        "        self.biases.append(np.random.randn(self.neurons[0]))\n",
        "        for i in range(0, self.hidden_layers - 1):\n",
        "            self.weights.append(np.random.randn(self.neurons[i], self.neurons[i+1]))\n",
        "            self.biases.append(np.random.randn(self.neurons[i + 1]))\n",
        "        self.weights.append(np.random.randn(self.neurons[-1], self.outputlayersize))\n",
        "        self.biases.append(np.random.randn(self.outputlayersize))\n",
        "        # weights correctly initialized\n",
        "        # biases correctly initialized\n",
        "\n",
        "        self.z = [None]*len(self.weights)\n",
        "        self.a = [None]*len(self.weights)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        s = np.max(x, axis=1)\n",
        "        s = s[:, np.newaxis]\n",
        "        e_x = np.exp(x - s)\n",
        "        div = np.sum(e_x, axis=1)\n",
        "        div = div[:, np.newaxis]\n",
        "        return e_x / div\n",
        "\n",
        "    def forward(self, X):\n",
        "        z = np.dot(X, self.weights[0]) + self.biases[0]\n",
        "        a = self.activation_function(z)\n",
        "        self.z[0] = z\n",
        "        self.a[0] = a\n",
        "        for i in range(1, len(self.weights) - 1):\n",
        "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
        "            a = self.activation_function(z)\n",
        "            self.z[i] = z\n",
        "            self.a[i] = a\n",
        "        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
        "        a = self.softmax(z)\n",
        "        self.z[-1] = z\n",
        "        self.a[-1] = a\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        # initialize weight gradients\n",
        "        weight_gradients = [np.zeros_like(w) for w in self.weights]\n",
        "        bias_gradients = [np.zeros_like(b) for b in self.biases]\n",
        "        # calculate gradients\n",
        "        error = self.a[-1] - y\n",
        "        weight_gradients[-1] = np.dot(self.a[-2].T, error)\n",
        "        # print(error)\n",
        "        bias_gradients[-1] = np.sum(error, axis=0)\n",
        "        # print(bias_gradients[-1])\n",
        "\n",
        "        for i in range(len(self.weights)-2, 0, -1):\n",
        "            error = np.dot(error, self.weights[i+1].T) * self.backprop_function(self.z[i])\n",
        "            weight_gradients[i] = np.dot(self.a[i-1].T, error)\n",
        "            bias_gradients[i] = np.sum(error,axis=0)\n",
        "\n",
        "        error = np.dot(error, self.weights[1].T) * self.backprop_function(self.z[0])\n",
        "        weight_gradients[0] = np.dot(X.T, error)\n",
        "        bias_gradients[0] = np.sum(error,axis=0)\n",
        "\n",
        "        # update weights and biases\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= self.learning_rate * weight_gradients[i] / X.shape[0]\n",
        "            self.biases[i] -= self.learning_rate * bias_gradients[i] / X.shape[0]\n",
        "\n",
        "    def lossandaccuracy(self, X, y):\n",
        "        self.forward(X)\n",
        "        loss = -np.sum(y * np.log(self.a[-1])) / X.shape[0]\n",
        "        prediction = np.argmax(self.a[-1], axis=1)\n",
        "        accuracy = np.sum(prediction == np.argmax(y, axis=1)) / X.shape[0]\n",
        "        return loss, accuracy\n",
        "\n",
        "    def train(self, X, y, val_X, val_y, epochs):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.weightsbiases()\n",
        "\n",
        "        # pick optimizer\n",
        "        if self.optimizers == \"sgd\":\n",
        "            batch_size = 1\n",
        "        if self.optimizers == \"mini-batch\":\n",
        "            batch_size = 32\n",
        "        if self.optimizers == \"batch\":\n",
        "            batch_size = self.X.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "        # initialize loss\n",
        "        loss_val = [None]*epochs\n",
        "        loss_train = [None]*epochs\n",
        "        accuracy_val = [None]*epochs\n",
        "        accuracy_train = [None]*epochs\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, self.X.shape[0], batch_size):\n",
        "                batch_x = self.X[i:i+batch_size]\n",
        "                batch_y = self.y[i:i+batch_size]\n",
        "                self.forward(batch_x)\n",
        "                self.backward(batch_x, batch_y)\n",
        "            # calculate loss, accuracy\n",
        "            loss_val[epoch], accuracy_val[epoch] = self.lossandaccuracy(val_X, val_y)\n",
        "            loss_train[epoch], accuracy_train[epoch] = self.lossandaccuracy(self.X, self.y)\n",
        "            print(\"Epoch: \", epoch, \" Loss: \", loss_val[epoch], \" Accuracy: \", accuracy_val[epoch])\n",
        "            # wandb.log({\"loss\": loss_val[epoch], \"accuracy\": accuracy_val[epoch]})\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.forward(X)\n",
        "        prediction = np.argmax(self.a[-1], axis=1)\n",
        "        return prediction\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffMMTTL5Pcfk"
      },
      "source": [
        "## Hyperparameter tuning Using WandB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "shJPZ5nTTJaT"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'metric': {\n",
        "        'name': 'accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'learning_rate': {\n",
        "            'values': [0.001, 0.01]\n",
        "        },\n",
        "        'optimizers': {\n",
        "            'values': ['sgd', 'batch', 'mini-batch']\n",
        "        },\n",
        "        'neurons': {\n",
        "            'values': [[2, [10, 10]], [3, [20, 10, 7]]]\n",
        "        },\n",
        "        'activation_functions': {\n",
        "            'values': ['sigmoid', 'tanh', 'relu']\n",
        "        },\n",
        "        'epochs': {\n",
        "            'values': [500, 1000]\n",
        "        },\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_MsI5q4TL5d",
        "outputId": "32e73e34-cba8-4e39-a4fb-0eae113b3542"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: re36z05f\n",
            "Sweep URL: https://wandb.ai/sanika-damle/assignment-3%20part-2%20classification%20%28singlelabel%29/sweeps/re36z05f\n"
          ]
        }
      ],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"assignment-3 part-2 classification (singlelabel)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXH9yXg_OyQV"
      },
      "outputs": [],
      "source": [
        "def slc():\n",
        "    with wandb.init() as run:\n",
        "        lr = wandb.config.learning_rate\n",
        "        optimizers = wandb.config.optimizers\n",
        "        hidden_layers = wandb.config.neurons[0]\n",
        "        neurons = wandb.config.neurons[1]\n",
        "        activation_function = wandb.config.activation_functions\n",
        "        epochs = wandb.config.epochs\n",
        "        mlp = MLP(lr, activation_function, optimizers, hidden_layers, neurons)\n",
        "        mlp.train(train_x, train_y, validation_x, validation_y, epochs)\n",
        "        ypred = mlp.predict(test_x)\n",
        "        f1 = f1_score(np.argmax(test_y, axis=1), ypred, average='macro')\n",
        "        precision = precision_score(np.argmax(test_y, axis=1), ypred, average='macro')\n",
        "        recall = recall_score(np.argmax(test_y, axis=1), ypred, average='macro')\n",
        "        accuracy = np.sum(ypred == np.argmax(test_y, axis=1)) / test_x.shape[0]\n",
        "        wandb.log({\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall})\n",
        "\n",
        "wandb.agent(sweep_id, function = slc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmE0fBUzIwbY",
        "outputId": "9688e6d5-9506-49f3-9ccc-e7832b5731bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  0  Loss:  6.029999762014073  Accuracy:  0.2896174863387978\n",
            "Epoch:  1  Loss:  4.3099344310438035  Accuracy:  0.3879781420765027\n",
            "Epoch:  2  Loss:  3.53994012350425  Accuracy:  0.4207650273224044\n",
            "Epoch:  3  Loss:  3.1040496701777056  Accuracy:  0.46994535519125685\n",
            "Epoch:  4  Loss:  2.825814893681584  Accuracy:  0.48633879781420764\n",
            "Epoch:  5  Loss:  2.6171133344851607  Accuracy:  0.4918032786885246\n",
            "Epoch:  6  Loss:  2.4591451505631214  Accuracy:  0.4918032786885246\n",
            "Epoch:  7  Loss:  2.3296257641972535  Accuracy:  0.4972677595628415\n",
            "Epoch:  8  Loss:  2.217090473696862  Accuracy:  0.4918032786885246\n",
            "Epoch:  9  Loss:  2.1225301501807134  Accuracy:  0.4972677595628415\n",
            "Epoch:  10  Loss:  2.0483409809021706  Accuracy:  0.48633879781420764\n",
            "Epoch:  11  Loss:  1.9838049513919964  Accuracy:  0.5027322404371585\n",
            "Epoch:  12  Loss:  1.9295666195622638  Accuracy:  0.4972677595628415\n",
            "Epoch:  13  Loss:  1.8799353730466857  Accuracy:  0.4972677595628415\n",
            "Epoch:  14  Loss:  1.8349621633625561  Accuracy:  0.4972677595628415\n",
            "Epoch:  15  Loss:  1.7939725202672796  Accuracy:  0.4918032786885246\n",
            "Epoch:  16  Loss:  1.755991710931543  Accuracy:  0.48633879781420764\n",
            "Epoch:  17  Loss:  1.720494416017776  Accuracy:  0.4918032786885246\n",
            "Epoch:  18  Loss:  1.6884352161684346  Accuracy:  0.48633879781420764\n",
            "Epoch:  19  Loss:  1.6620013744048503  Accuracy:  0.48633879781420764\n",
            "Epoch:  20  Loss:  1.639254442713846  Accuracy:  0.48633879781420764\n",
            "Epoch:  21  Loss:  1.618920530132535  Accuracy:  0.4918032786885246\n",
            "Epoch:  22  Loss:  1.6008780265900613  Accuracy:  0.4918032786885246\n",
            "Epoch:  23  Loss:  1.5816954645511265  Accuracy:  0.4918032786885246\n",
            "Epoch:  24  Loss:  1.5654932318107713  Accuracy:  0.4918032786885246\n",
            "Epoch:  25  Loss:  1.5492861156371245  Accuracy:  0.4972677595628415\n",
            "Epoch:  26  Loss:  1.536017775514705  Accuracy:  0.4972677595628415\n",
            "Epoch:  27  Loss:  1.5216911887917675  Accuracy:  0.5027322404371585\n",
            "Epoch:  28  Loss:  1.5099686172507296  Accuracy:  0.5027322404371585\n",
            "Epoch:  29  Loss:  1.4980929476886409  Accuracy:  0.5081967213114754\n",
            "Epoch:  30  Loss:  1.4887531234660463  Accuracy:  0.5136612021857924\n",
            "Epoch:  31  Loss:  1.4784316631349208  Accuracy:  0.5027322404371585\n",
            "Epoch:  32  Loss:  1.4694908833479305  Accuracy:  0.5027322404371585\n",
            "Epoch:  33  Loss:  1.4606033087566406  Accuracy:  0.5027322404371585\n",
            "Epoch:  34  Loss:  1.4523402697981136  Accuracy:  0.4972677595628415\n",
            "Epoch:  35  Loss:  1.4436314288112977  Accuracy:  0.5027322404371585\n",
            "Epoch:  36  Loss:  1.4363244070423653  Accuracy:  0.5027322404371585\n",
            "Epoch:  37  Loss:  1.4287697131747645  Accuracy:  0.4972677595628415\n",
            "Epoch:  38  Loss:  1.4217234684657685  Accuracy:  0.5027322404371585\n",
            "Epoch:  39  Loss:  1.4164639059487742  Accuracy:  0.5027322404371585\n",
            "Epoch:  40  Loss:  1.4103315704273867  Accuracy:  0.5027322404371585\n",
            "Epoch:  41  Loss:  1.403870861659447  Accuracy:  0.5027322404371585\n",
            "Epoch:  42  Loss:  1.4006538847022132  Accuracy:  0.5027322404371585\n",
            "Epoch:  43  Loss:  1.395709213955954  Accuracy:  0.5027322404371585\n",
            "Epoch:  44  Loss:  1.3917852688234917  Accuracy:  0.5027322404371585\n",
            "Epoch:  45  Loss:  1.3892274223011134  Accuracy:  0.5027322404371585\n",
            "Epoch:  46  Loss:  1.3841833473908227  Accuracy:  0.5081967213114754\n",
            "Epoch:  47  Loss:  1.3803130381262185  Accuracy:  0.5081967213114754\n",
            "Epoch:  48  Loss:  1.3777196875690125  Accuracy:  0.5027322404371585\n",
            "Epoch:  49  Loss:  1.3761112623338223  Accuracy:  0.4972677595628415\n",
            "Epoch:  50  Loss:  1.372068497946673  Accuracy:  0.4972677595628415\n",
            "Epoch:  51  Loss:  1.369433464355309  Accuracy:  0.4972677595628415\n",
            "Epoch:  52  Loss:  1.366569984309698  Accuracy:  0.4972677595628415\n",
            "Epoch:  53  Loss:  1.3641634705511683  Accuracy:  0.4972677595628415\n",
            "Epoch:  54  Loss:  1.3625458341027394  Accuracy:  0.4972677595628415\n",
            "Epoch:  55  Loss:  1.3583565514912481  Accuracy:  0.4918032786885246\n",
            "Epoch:  56  Loss:  1.355350942491142  Accuracy:  0.4972677595628415\n",
            "Epoch:  57  Loss:  1.3541786948634804  Accuracy:  0.4972677595628415\n",
            "Epoch:  58  Loss:  1.3501544684128814  Accuracy:  0.4972677595628415\n",
            "Epoch:  59  Loss:  1.349682778992373  Accuracy:  0.4972677595628415\n",
            "Epoch:  60  Loss:  1.346089373219425  Accuracy:  0.4972677595628415\n",
            "Epoch:  61  Loss:  1.3450366360953827  Accuracy:  0.4972677595628415\n",
            "Epoch:  62  Loss:  1.3417008923407199  Accuracy:  0.5027322404371585\n",
            "Epoch:  63  Loss:  1.3399873306432502  Accuracy:  0.5027322404371585\n",
            "Epoch:  64  Loss:  1.337980705188985  Accuracy:  0.5027322404371585\n",
            "Epoch:  65  Loss:  1.3369413440438125  Accuracy:  0.5027322404371585\n",
            "Epoch:  66  Loss:  1.3354237639850555  Accuracy:  0.5027322404371585\n",
            "Epoch:  67  Loss:  1.3336534992748714  Accuracy:  0.5027322404371585\n",
            "Epoch:  68  Loss:  1.3327132023124129  Accuracy:  0.4972677595628415\n",
            "Epoch:  69  Loss:  1.3294216730074273  Accuracy:  0.5081967213114754\n",
            "Epoch:  70  Loss:  1.328363966118784  Accuracy:  0.5081967213114754\n",
            "Epoch:  71  Loss:  1.3269472454633784  Accuracy:  0.5081967213114754\n",
            "Epoch:  72  Loss:  1.3248848497320282  Accuracy:  0.5081967213114754\n",
            "Epoch:  73  Loss:  1.3252572645114353  Accuracy:  0.5081967213114754\n",
            "Epoch:  74  Loss:  1.3226430728891165  Accuracy:  0.5081967213114754\n",
            "Epoch:  75  Loss:  1.3216891480185384  Accuracy:  0.5081967213114754\n",
            "Epoch:  76  Loss:  1.319522791830584  Accuracy:  0.5136612021857924\n",
            "Epoch:  77  Loss:  1.3201264356565159  Accuracy:  0.5136612021857924\n",
            "Epoch:  78  Loss:  1.3183065481577525  Accuracy:  0.5136612021857924\n",
            "Epoch:  79  Loss:  1.3162381123347275  Accuracy:  0.5136612021857924\n",
            "Epoch:  80  Loss:  1.3162604795017538  Accuracy:  0.5136612021857924\n",
            "Epoch:  81  Loss:  1.3160925687049645  Accuracy:  0.5136612021857924\n",
            "Epoch:  82  Loss:  1.3139176592602855  Accuracy:  0.5136612021857924\n",
            "Epoch:  83  Loss:  1.3138611428610172  Accuracy:  0.5136612021857924\n",
            "Epoch:  84  Loss:  1.3126908778431474  Accuracy:  0.5136612021857924\n",
            "Epoch:  85  Loss:  1.311756736449607  Accuracy:  0.5136612021857924\n",
            "Epoch:  86  Loss:  1.3108553493658743  Accuracy:  0.5136612021857924\n",
            "Epoch:  87  Loss:  1.3107732113772865  Accuracy:  0.5191256830601093\n",
            "Epoch:  88  Loss:  1.3097786619016683  Accuracy:  0.5191256830601093\n",
            "Epoch:  89  Loss:  1.3096025020353965  Accuracy:  0.5191256830601093\n",
            "Epoch:  90  Loss:  1.3074715006332547  Accuracy:  0.5191256830601093\n",
            "Epoch:  91  Loss:  1.3071915788967499  Accuracy:  0.5191256830601093\n",
            "Epoch:  92  Loss:  1.3060793504869994  Accuracy:  0.5191256830601093\n",
            "Epoch:  93  Loss:  1.305886973873361  Accuracy:  0.5191256830601093\n",
            "Epoch:  94  Loss:  1.305426224086838  Accuracy:  0.5191256830601093\n",
            "Epoch:  95  Loss:  1.3039723620770605  Accuracy:  0.5191256830601093\n",
            "Epoch:  96  Loss:  1.3044965477652084  Accuracy:  0.5191256830601093\n",
            "Epoch:  97  Loss:  1.3046174346041874  Accuracy:  0.5191256830601093\n",
            "Epoch:  98  Loss:  1.3026947731089016  Accuracy:  0.5191256830601093\n",
            "Epoch:  99  Loss:  1.3026580911788685  Accuracy:  0.5245901639344263\n",
            "Epoch:  100  Loss:  1.3028888629805793  Accuracy:  0.5300546448087432\n",
            "Epoch:  101  Loss:  1.3026921538530953  Accuracy:  0.5300546448087432\n",
            "Epoch:  102  Loss:  1.3024605548113786  Accuracy:  0.5245901639344263\n",
            "Epoch:  103  Loss:  1.3019592737537766  Accuracy:  0.5245901639344263\n",
            "Epoch:  104  Loss:  1.3008547219497955  Accuracy:  0.5245901639344263\n",
            "Epoch:  105  Loss:  1.3007479176527756  Accuracy:  0.5245901639344263\n",
            "Epoch:  106  Loss:  1.3003753838893806  Accuracy:  0.5245901639344263\n",
            "Epoch:  107  Loss:  1.3006780037973025  Accuracy:  0.5245901639344263\n",
            "Epoch:  108  Loss:  1.2986505332761822  Accuracy:  0.5191256830601093\n",
            "Epoch:  109  Loss:  1.297567359880506  Accuracy:  0.5191256830601093\n",
            "Epoch:  110  Loss:  1.296798993113931  Accuracy:  0.5191256830601093\n",
            "Epoch:  111  Loss:  1.2970369885834785  Accuracy:  0.5191256830601093\n",
            "Epoch:  112  Loss:  1.294986352409082  Accuracy:  0.5191256830601093\n",
            "Epoch:  113  Loss:  1.2938995902571102  Accuracy:  0.5191256830601093\n",
            "Epoch:  114  Loss:  1.2934190150368137  Accuracy:  0.5245901639344263\n",
            "Epoch:  115  Loss:  1.2925508440063755  Accuracy:  0.5245901639344263\n",
            "Epoch:  116  Loss:  1.29214797618066  Accuracy:  0.5245901639344263\n",
            "Epoch:  117  Loss:  1.2902912131852509  Accuracy:  0.5245901639344263\n",
            "Epoch:  118  Loss:  1.290034988275584  Accuracy:  0.5245901639344263\n",
            "Epoch:  119  Loss:  1.2898843892110539  Accuracy:  0.5300546448087432\n",
            "Epoch:  120  Loss:  1.2887511564500314  Accuracy:  0.5300546448087432\n",
            "Epoch:  121  Loss:  1.28731813353987  Accuracy:  0.5300546448087432\n",
            "Epoch:  122  Loss:  1.2861553875355272  Accuracy:  0.5355191256830601\n",
            "Epoch:  123  Loss:  1.2852747850119708  Accuracy:  0.5355191256830601\n",
            "Epoch:  124  Loss:  1.2851155812400255  Accuracy:  0.5355191256830601\n",
            "Epoch:  125  Loss:  1.2832704776204447  Accuracy:  0.5355191256830601\n",
            "Epoch:  126  Loss:  1.28305321448112  Accuracy:  0.5355191256830601\n",
            "Epoch:  127  Loss:  1.2813113802450224  Accuracy:  0.5355191256830601\n",
            "Epoch:  128  Loss:  1.281440086709038  Accuracy:  0.5355191256830601\n",
            "Epoch:  129  Loss:  1.28067393332651  Accuracy:  0.5409836065573771\n",
            "Epoch:  130  Loss:  1.2809387757314845  Accuracy:  0.5409836065573771\n",
            "Epoch:  131  Loss:  1.2793391699145469  Accuracy:  0.5409836065573771\n",
            "Epoch:  132  Loss:  1.2794252311615835  Accuracy:  0.5409836065573771\n",
            "Epoch:  133  Loss:  1.2778332371761574  Accuracy:  0.5409836065573771\n",
            "Epoch:  134  Loss:  1.2781971610362977  Accuracy:  0.5409836065573771\n",
            "Epoch:  135  Loss:  1.2775498648229924  Accuracy:  0.5409836065573771\n",
            "Epoch:  136  Loss:  1.2777837456288181  Accuracy:  0.5409836065573771\n",
            "Epoch:  137  Loss:  1.276881928842101  Accuracy:  0.5409836065573771\n",
            "Epoch:  138  Loss:  1.2760035651595145  Accuracy:  0.5355191256830601\n",
            "Epoch:  139  Loss:  1.2753930953718904  Accuracy:  0.5355191256830601\n",
            "Epoch:  140  Loss:  1.2743660528025387  Accuracy:  0.5355191256830601\n",
            "Epoch:  141  Loss:  1.274007579784973  Accuracy:  0.5355191256830601\n",
            "Epoch:  142  Loss:  1.274033305296312  Accuracy:  0.5355191256830601\n",
            "Epoch:  143  Loss:  1.2739475160312124  Accuracy:  0.5300546448087432\n",
            "Epoch:  144  Loss:  1.2731105538284613  Accuracy:  0.5300546448087432\n",
            "Epoch:  145  Loss:  1.2727022226911513  Accuracy:  0.5355191256830601\n",
            "Epoch:  146  Loss:  1.2719791296904406  Accuracy:  0.5355191256830601\n",
            "Epoch:  147  Loss:  1.2714492015004462  Accuracy:  0.5355191256830601\n",
            "Epoch:  148  Loss:  1.270041715777185  Accuracy:  0.5300546448087432\n",
            "Epoch:  149  Loss:  1.2691692391497231  Accuracy:  0.5300546448087432\n",
            "Epoch:  150  Loss:  1.2690036588832623  Accuracy:  0.5300546448087432\n",
            "Epoch:  151  Loss:  1.2680080631336303  Accuracy:  0.5300546448087432\n",
            "Epoch:  152  Loss:  1.267700646690253  Accuracy:  0.5300546448087432\n",
            "Epoch:  153  Loss:  1.2667116146681836  Accuracy:  0.5300546448087432\n",
            "Epoch:  154  Loss:  1.2656154106326505  Accuracy:  0.5300546448087432\n",
            "Epoch:  155  Loss:  1.2647909585724437  Accuracy:  0.5300546448087432\n",
            "Epoch:  156  Loss:  1.2637803455104406  Accuracy:  0.5300546448087432\n",
            "Epoch:  157  Loss:  1.2642538178394434  Accuracy:  0.5300546448087432\n",
            "Epoch:  158  Loss:  1.262996863471815  Accuracy:  0.5300546448087432\n",
            "Epoch:  159  Loss:  1.261720665349034  Accuracy:  0.5300546448087432\n",
            "Epoch:  160  Loss:  1.2604371525628104  Accuracy:  0.5300546448087432\n",
            "Epoch:  161  Loss:  1.2599750675586325  Accuracy:  0.5355191256830601\n",
            "Epoch:  162  Loss:  1.2599798810852103  Accuracy:  0.5300546448087432\n",
            "Epoch:  163  Loss:  1.259810265179346  Accuracy:  0.5355191256830601\n",
            "Epoch:  164  Loss:  1.258594159512137  Accuracy:  0.5300546448087432\n",
            "Epoch:  165  Loss:  1.2590495919326856  Accuracy:  0.5300546448087432\n",
            "Epoch:  166  Loss:  1.258385975403628  Accuracy:  0.5355191256830601\n",
            "Epoch:  167  Loss:  1.2579097188296617  Accuracy:  0.5300546448087432\n",
            "Epoch:  168  Loss:  1.257878738833655  Accuracy:  0.5245901639344263\n",
            "Epoch:  169  Loss:  1.256962672065132  Accuracy:  0.5245901639344263\n",
            "Epoch:  170  Loss:  1.2569745254844604  Accuracy:  0.5245901639344263\n",
            "Epoch:  171  Loss:  1.2555140466757115  Accuracy:  0.5300546448087432\n",
            "Epoch:  172  Loss:  1.256112166493784  Accuracy:  0.5300546448087432\n",
            "Epoch:  173  Loss:  1.256218363357973  Accuracy:  0.5245901639344263\n",
            "Epoch:  174  Loss:  1.2556655274190092  Accuracy:  0.5300546448087432\n",
            "Epoch:  175  Loss:  1.255704927368983  Accuracy:  0.5300546448087432\n",
            "Epoch:  176  Loss:  1.2558257318914396  Accuracy:  0.5245901639344263\n",
            "Epoch:  177  Loss:  1.2558525992434386  Accuracy:  0.5300546448087432\n",
            "Epoch:  178  Loss:  1.2552101287777622  Accuracy:  0.5300546448087432\n",
            "Epoch:  179  Loss:  1.2538695524929147  Accuracy:  0.5300546448087432\n",
            "Epoch:  180  Loss:  1.2539267379821128  Accuracy:  0.5300546448087432\n",
            "Epoch:  181  Loss:  1.2533276690992525  Accuracy:  0.5300546448087432\n",
            "Epoch:  182  Loss:  1.2524011172987821  Accuracy:  0.5300546448087432\n",
            "Epoch:  183  Loss:  1.2529242381384875  Accuracy:  0.5300546448087432\n",
            "Epoch:  184  Loss:  1.2531820852724362  Accuracy:  0.5300546448087432\n",
            "Epoch:  185  Loss:  1.2525650113793774  Accuracy:  0.5300546448087432\n",
            "Epoch:  186  Loss:  1.2518489708717897  Accuracy:  0.5300546448087432\n",
            "Epoch:  187  Loss:  1.250869691153276  Accuracy:  0.5300546448087432\n",
            "Epoch:  188  Loss:  1.2525448630875389  Accuracy:  0.5300546448087432\n",
            "Epoch:  189  Loss:  1.2518297928969604  Accuracy:  0.5300546448087432\n",
            "Epoch:  190  Loss:  1.2516419425607976  Accuracy:  0.5300546448087432\n",
            "Epoch:  191  Loss:  1.2517155600135792  Accuracy:  0.5300546448087432\n",
            "Epoch:  192  Loss:  1.2506762546231887  Accuracy:  0.5300546448087432\n",
            "Epoch:  193  Loss:  1.2498266819244501  Accuracy:  0.5300546448087432\n",
            "Epoch:  194  Loss:  1.2502570490592786  Accuracy:  0.5245901639344263\n",
            "Epoch:  195  Loss:  1.2501829106000841  Accuracy:  0.5245901639344263\n",
            "Epoch:  196  Loss:  1.2492137402401462  Accuracy:  0.5245901639344263\n",
            "Epoch:  197  Loss:  1.2499486107664641  Accuracy:  0.5245901639344263\n",
            "Epoch:  198  Loss:  1.2476474513047442  Accuracy:  0.5245901639344263\n",
            "Epoch:  199  Loss:  1.2472321689542927  Accuracy:  0.5245901639344263\n",
            "Epoch:  200  Loss:  1.2474800161196098  Accuracy:  0.5245901639344263\n",
            "Epoch:  201  Loss:  1.2464115980034394  Accuracy:  0.5245901639344263\n",
            "Epoch:  202  Loss:  1.2483556974994328  Accuracy:  0.5245901639344263\n",
            "Epoch:  203  Loss:  1.2459901969359186  Accuracy:  0.5300546448087432\n",
            "Epoch:  204  Loss:  1.2463993710471033  Accuracy:  0.5300546448087432\n",
            "Epoch:  205  Loss:  1.2462170901252936  Accuracy:  0.5300546448087432\n",
            "Epoch:  206  Loss:  1.2459248364561912  Accuracy:  0.5300546448087432\n",
            "Epoch:  207  Loss:  1.2461822927355641  Accuracy:  0.5300546448087432\n",
            "Epoch:  208  Loss:  1.245290320058406  Accuracy:  0.5300546448087432\n",
            "Epoch:  209  Loss:  1.245218312015563  Accuracy:  0.5300546448087432\n",
            "Epoch:  210  Loss:  1.2444740733858664  Accuracy:  0.5300546448087432\n",
            "Epoch:  211  Loss:  1.2447400141347709  Accuracy:  0.5300546448087432\n",
            "Epoch:  212  Loss:  1.2452814449799505  Accuracy:  0.5300546448087432\n",
            "Epoch:  213  Loss:  1.244247184671746  Accuracy:  0.5300546448087432\n",
            "Epoch:  214  Loss:  1.2434542758879812  Accuracy:  0.5300546448087432\n",
            "Epoch:  215  Loss:  1.2436249366880527  Accuracy:  0.5300546448087432\n",
            "Epoch:  216  Loss:  1.2423820138780328  Accuracy:  0.5300546448087432\n",
            "Epoch:  217  Loss:  1.243869941977049  Accuracy:  0.5300546448087432\n",
            "Epoch:  218  Loss:  1.242979039578561  Accuracy:  0.5300546448087432\n",
            "Epoch:  219  Loss:  1.2418141664858557  Accuracy:  0.5300546448087432\n",
            "Epoch:  220  Loss:  1.2437026498159816  Accuracy:  0.5300546448087432\n",
            "Epoch:  221  Loss:  1.2403342648979652  Accuracy:  0.5300546448087432\n",
            "Epoch:  222  Loss:  1.241271063627466  Accuracy:  0.5300546448087432\n",
            "Epoch:  223  Loss:  1.2409943422815717  Accuracy:  0.5300546448087432\n",
            "Epoch:  224  Loss:  1.240845041786767  Accuracy:  0.5300546448087432\n",
            "Epoch:  225  Loss:  1.2397206811891257  Accuracy:  0.5300546448087432\n",
            "Epoch:  226  Loss:  1.2406554662502163  Accuracy:  0.5300546448087432\n",
            "Epoch:  227  Loss:  1.2408419804668585  Accuracy:  0.5245901639344263\n",
            "Epoch:  228  Loss:  1.2414237131912587  Accuracy:  0.5245901639344263\n",
            "Epoch:  229  Loss:  1.241292124473277  Accuracy:  0.5245901639344263\n",
            "Epoch:  230  Loss:  1.240263872451473  Accuracy:  0.5245901639344263\n",
            "Epoch:  231  Loss:  1.2412105270051184  Accuracy:  0.5245901639344263\n",
            "Epoch:  232  Loss:  1.2391797484783253  Accuracy:  0.5245901639344263\n",
            "Epoch:  233  Loss:  1.2391619327675627  Accuracy:  0.5245901639344263\n",
            "Epoch:  234  Loss:  1.2402331216526654  Accuracy:  0.5245901639344263\n",
            "Epoch:  235  Loss:  1.2394772527981877  Accuracy:  0.5245901639344263\n",
            "Epoch:  236  Loss:  1.238665295868689  Accuracy:  0.5245901639344263\n",
            "Epoch:  237  Loss:  1.2395297800940726  Accuracy:  0.5245901639344263\n",
            "Epoch:  238  Loss:  1.23773722454093  Accuracy:  0.5245901639344263\n",
            "Epoch:  239  Loss:  1.2388858680292398  Accuracy:  0.5245901639344263\n",
            "Epoch:  240  Loss:  1.2386227065165867  Accuracy:  0.5245901639344263\n",
            "Epoch:  241  Loss:  1.238202074926072  Accuracy:  0.5245901639344263\n",
            "Epoch:  242  Loss:  1.2369242319042386  Accuracy:  0.5245901639344263\n",
            "Epoch:  243  Loss:  1.2377507070682994  Accuracy:  0.5245901639344263\n",
            "Epoch:  244  Loss:  1.2378156416514448  Accuracy:  0.5245901639344263\n",
            "Epoch:  245  Loss:  1.2374049146328026  Accuracy:  0.5245901639344263\n",
            "Epoch:  246  Loss:  1.2378846263191756  Accuracy:  0.5245901639344263\n",
            "Epoch:  247  Loss:  1.2369941488295686  Accuracy:  0.5245901639344263\n",
            "Epoch:  248  Loss:  1.2374561339255026  Accuracy:  0.5245901639344263\n",
            "Epoch:  249  Loss:  1.2351790108004126  Accuracy:  0.5245901639344263\n",
            "Epoch:  250  Loss:  1.2370082170373637  Accuracy:  0.5245901639344263\n",
            "Epoch:  251  Loss:  1.2355815006920408  Accuracy:  0.5245901639344263\n",
            "Epoch:  252  Loss:  1.2363105706377524  Accuracy:  0.5245901639344263\n",
            "Epoch:  253  Loss:  1.2357904563519726  Accuracy:  0.5245901639344263\n",
            "Epoch:  254  Loss:  1.2355301387428943  Accuracy:  0.5191256830601093\n",
            "Epoch:  255  Loss:  1.2358552657237847  Accuracy:  0.5191256830601093\n",
            "Epoch:  256  Loss:  1.2336271604440836  Accuracy:  0.5191256830601093\n",
            "Epoch:  257  Loss:  1.2350086088368324  Accuracy:  0.5191256830601093\n",
            "Epoch:  258  Loss:  1.2351302209388322  Accuracy:  0.5191256830601093\n",
            "Epoch:  259  Loss:  1.2345087657366598  Accuracy:  0.5191256830601093\n",
            "Epoch:  260  Loss:  1.2341700277150427  Accuracy:  0.5191256830601093\n",
            "Epoch:  261  Loss:  1.2332413480335787  Accuracy:  0.5191256830601093\n",
            "Epoch:  262  Loss:  1.2338030592390827  Accuracy:  0.5191256830601093\n",
            "Epoch:  263  Loss:  1.232760372077717  Accuracy:  0.5191256830601093\n",
            "Epoch:  264  Loss:  1.2312429021301856  Accuracy:  0.5191256830601093\n",
            "Epoch:  265  Loss:  1.2329731079549329  Accuracy:  0.5191256830601093\n",
            "Epoch:  266  Loss:  1.2319537405178342  Accuracy:  0.5191256830601093\n",
            "Epoch:  267  Loss:  1.2313836819273  Accuracy:  0.5191256830601093\n",
            "Epoch:  268  Loss:  1.2325292016704774  Accuracy:  0.5191256830601093\n",
            "Epoch:  269  Loss:  1.230586483899125  Accuracy:  0.5245901639344263\n",
            "Epoch:  270  Loss:  1.2323427550524944  Accuracy:  0.5245901639344263\n",
            "Epoch:  271  Loss:  1.231724761271876  Accuracy:  0.5191256830601093\n",
            "Epoch:  272  Loss:  1.2302201106223452  Accuracy:  0.5191256830601093\n",
            "Epoch:  273  Loss:  1.2310304603672086  Accuracy:  0.5245901639344263\n",
            "Epoch:  274  Loss:  1.2300799350906844  Accuracy:  0.5245901639344263\n",
            "Epoch:  275  Loss:  1.2297630095022472  Accuracy:  0.5191256830601093\n",
            "Epoch:  276  Loss:  1.230428411242651  Accuracy:  0.5245901639344263\n",
            "Epoch:  277  Loss:  1.230700325801813  Accuracy:  0.5245901639344263\n",
            "Epoch:  278  Loss:  1.2292402261680955  Accuracy:  0.5245901639344263\n",
            "Epoch:  279  Loss:  1.2284796887123453  Accuracy:  0.5245901639344263\n",
            "Epoch:  280  Loss:  1.2284549322781948  Accuracy:  0.5245901639344263\n",
            "Epoch:  281  Loss:  1.2291478049837612  Accuracy:  0.5245901639344263\n",
            "Epoch:  282  Loss:  1.2294691508201396  Accuracy:  0.5245901639344263\n",
            "Epoch:  283  Loss:  1.2278916148609142  Accuracy:  0.5245901639344263\n",
            "Epoch:  284  Loss:  1.2297061764374246  Accuracy:  0.5245901639344263\n",
            "Epoch:  285  Loss:  1.2280901119355403  Accuracy:  0.5245901639344263\n",
            "Epoch:  286  Loss:  1.2287905153533343  Accuracy:  0.5245901639344263\n",
            "Epoch:  287  Loss:  1.2287584639983444  Accuracy:  0.5245901639344263\n",
            "Epoch:  288  Loss:  1.2275792058103248  Accuracy:  0.5245901639344263\n",
            "Epoch:  289  Loss:  1.227796892743537  Accuracy:  0.5300546448087432\n",
            "Epoch:  290  Loss:  1.2275944354565025  Accuracy:  0.5245901639344263\n",
            "Epoch:  291  Loss:  1.229359076543646  Accuracy:  0.5300546448087432\n",
            "Epoch:  292  Loss:  1.2279092963530363  Accuracy:  0.5300546448087432\n",
            "Epoch:  293  Loss:  1.2265863143252573  Accuracy:  0.5300546448087432\n",
            "Epoch:  294  Loss:  1.228389348278103  Accuracy:  0.5300546448087432\n",
            "Epoch:  295  Loss:  1.227875256939  Accuracy:  0.5300546448087432\n",
            "Epoch:  296  Loss:  1.2271689063070481  Accuracy:  0.5300546448087432\n",
            "Epoch:  297  Loss:  1.2267745117368412  Accuracy:  0.5300546448087432\n",
            "Epoch:  298  Loss:  1.2267811944143419  Accuracy:  0.5300546448087432\n",
            "Epoch:  299  Loss:  1.2256835384686624  Accuracy:  0.5300546448087432\n",
            "Epoch:  300  Loss:  1.2261303757357747  Accuracy:  0.5300546448087432\n",
            "Epoch:  301  Loss:  1.22531323813903  Accuracy:  0.5300546448087432\n",
            "Epoch:  302  Loss:  1.2266011200244589  Accuracy:  0.5300546448087432\n",
            "Epoch:  303  Loss:  1.2248462838486123  Accuracy:  0.5300546448087432\n",
            "Epoch:  304  Loss:  1.225928517999121  Accuracy:  0.5300546448087432\n",
            "Epoch:  305  Loss:  1.2259039870325865  Accuracy:  0.5300546448087432\n",
            "Epoch:  306  Loss:  1.2248952350865532  Accuracy:  0.5300546448087432\n",
            "Epoch:  307  Loss:  1.2252493800313433  Accuracy:  0.5300546448087432\n",
            "Epoch:  308  Loss:  1.223860779650985  Accuracy:  0.5300546448087432\n",
            "Epoch:  309  Loss:  1.225436880549828  Accuracy:  0.5300546448087432\n",
            "Epoch:  310  Loss:  1.225199173160653  Accuracy:  0.5300546448087432\n",
            "Epoch:  311  Loss:  1.2246835605214617  Accuracy:  0.5300546448087432\n",
            "Epoch:  312  Loss:  1.2237096666176301  Accuracy:  0.5300546448087432\n",
            "Epoch:  313  Loss:  1.2232147845506207  Accuracy:  0.5300546448087432\n",
            "Epoch:  314  Loss:  1.2239635841994634  Accuracy:  0.5300546448087432\n",
            "Epoch:  315  Loss:  1.2233059400240751  Accuracy:  0.5300546448087432\n",
            "Epoch:  316  Loss:  1.2241129845378615  Accuracy:  0.5300546448087432\n",
            "Epoch:  317  Loss:  1.2235219235402472  Accuracy:  0.5300546448087432\n",
            "Epoch:  318  Loss:  1.2238481888039094  Accuracy:  0.5300546448087432\n",
            "Epoch:  319  Loss:  1.222205715652363  Accuracy:  0.5300546448087432\n",
            "Epoch:  320  Loss:  1.2233264962808852  Accuracy:  0.5300546448087432\n",
            "Epoch:  321  Loss:  1.2231040355202751  Accuracy:  0.5300546448087432\n",
            "Epoch:  322  Loss:  1.222621555554821  Accuracy:  0.5300546448087432\n",
            "Epoch:  323  Loss:  1.2217160048939664  Accuracy:  0.5300546448087432\n",
            "Epoch:  324  Loss:  1.2216902868506632  Accuracy:  0.5300546448087432\n",
            "Epoch:  325  Loss:  1.222284152304765  Accuracy:  0.5300546448087432\n",
            "Epoch:  326  Loss:  1.221042252285013  Accuracy:  0.5300546448087432\n",
            "Epoch:  327  Loss:  1.2217222159157932  Accuracy:  0.5300546448087432\n",
            "Epoch:  328  Loss:  1.2202567554020445  Accuracy:  0.5300546448087432\n",
            "Epoch:  329  Loss:  1.2219190257629031  Accuracy:  0.5300546448087432\n",
            "Epoch:  330  Loss:  1.2210712541772633  Accuracy:  0.5300546448087432\n",
            "Epoch:  331  Loss:  1.2197908510369264  Accuracy:  0.5300546448087432\n",
            "Epoch:  332  Loss:  1.2207280620823309  Accuracy:  0.5355191256830601\n",
            "Epoch:  333  Loss:  1.2202166353548902  Accuracy:  0.5355191256830601\n",
            "Epoch:  334  Loss:  1.2210536288337073  Accuracy:  0.5355191256830601\n",
            "Epoch:  335  Loss:  1.2209201528489477  Accuracy:  0.5355191256830601\n",
            "Epoch:  336  Loss:  1.219423346156746  Accuracy:  0.5355191256830601\n",
            "Epoch:  337  Loss:  1.2204936992964417  Accuracy:  0.5355191256830601\n",
            "Epoch:  338  Loss:  1.2203378242435399  Accuracy:  0.5355191256830601\n",
            "Epoch:  339  Loss:  1.2202144285276284  Accuracy:  0.5355191256830601\n",
            "Epoch:  340  Loss:  1.219664364477026  Accuracy:  0.5355191256830601\n",
            "Epoch:  341  Loss:  1.2194124585818886  Accuracy:  0.5355191256830601\n",
            "Epoch:  342  Loss:  1.2195398315124384  Accuracy:  0.5355191256830601\n",
            "Epoch:  343  Loss:  1.2191957252918313  Accuracy:  0.5355191256830601\n",
            "Epoch:  344  Loss:  1.2190917910987227  Accuracy:  0.5409836065573771\n",
            "Epoch:  345  Loss:  1.2182634358906943  Accuracy:  0.5355191256830601\n",
            "Epoch:  346  Loss:  1.2196651043738913  Accuracy:  0.5355191256830601\n",
            "Epoch:  347  Loss:  1.2186722507096954  Accuracy:  0.5355191256830601\n",
            "Epoch:  348  Loss:  1.2191783408690058  Accuracy:  0.5409836065573771\n",
            "Epoch:  349  Loss:  1.2187111421455517  Accuracy:  0.5409836065573771\n",
            "Epoch:  350  Loss:  1.2180049735627991  Accuracy:  0.5409836065573771\n",
            "Epoch:  351  Loss:  1.21996631593687  Accuracy:  0.5409836065573771\n",
            "Epoch:  352  Loss:  1.218365326213531  Accuracy:  0.5409836065573771\n",
            "Epoch:  353  Loss:  1.2169781204800691  Accuracy:  0.5409836065573771\n",
            "Epoch:  354  Loss:  1.2181407086886635  Accuracy:  0.5409836065573771\n",
            "Epoch:  355  Loss:  1.217163894223166  Accuracy:  0.5409836065573771\n",
            "Epoch:  356  Loss:  1.2186221980180159  Accuracy:  0.5409836065573771\n",
            "Epoch:  357  Loss:  1.2187119508545883  Accuracy:  0.5409836065573771\n",
            "Epoch:  358  Loss:  1.217919654006633  Accuracy:  0.5409836065573771\n",
            "Epoch:  359  Loss:  1.2174018916700104  Accuracy:  0.5409836065573771\n",
            "Epoch:  360  Loss:  1.2170518851534404  Accuracy:  0.5409836065573771\n",
            "Epoch:  361  Loss:  1.216567578752532  Accuracy:  0.5409836065573771\n",
            "Epoch:  362  Loss:  1.2166918423318616  Accuracy:  0.5409836065573771\n",
            "Epoch:  363  Loss:  1.2168512522813566  Accuracy:  0.5409836065573771\n",
            "Epoch:  364  Loss:  1.216575660090371  Accuracy:  0.5409836065573771\n",
            "Epoch:  365  Loss:  1.2154732983672738  Accuracy:  0.546448087431694\n",
            "Epoch:  366  Loss:  1.2165917188000985  Accuracy:  0.546448087431694\n",
            "Epoch:  367  Loss:  1.2154828108155806  Accuracy:  0.546448087431694\n",
            "Epoch:  368  Loss:  1.2150684504115563  Accuracy:  0.546448087431694\n",
            "Epoch:  369  Loss:  1.2153613221504393  Accuracy:  0.546448087431694\n",
            "Epoch:  370  Loss:  1.2154141297502714  Accuracy:  0.546448087431694\n",
            "Epoch:  371  Loss:  1.2151186763962392  Accuracy:  0.5519125683060109\n",
            "Epoch:  372  Loss:  1.2150715393412623  Accuracy:  0.5519125683060109\n",
            "Epoch:  373  Loss:  1.2155285038733508  Accuracy:  0.5519125683060109\n",
            "Epoch:  374  Loss:  1.2149019867558257  Accuracy:  0.5519125683060109\n",
            "Epoch:  375  Loss:  1.2140436956616723  Accuracy:  0.5519125683060109\n",
            "Epoch:  376  Loss:  1.215003815294548  Accuracy:  0.5519125683060109\n",
            "Epoch:  377  Loss:  1.2140465652230734  Accuracy:  0.546448087431694\n",
            "Epoch:  378  Loss:  1.214011057596146  Accuracy:  0.5519125683060109\n",
            "Epoch:  379  Loss:  1.213162165363285  Accuracy:  0.546448087431694\n",
            "Epoch:  380  Loss:  1.2142447006835988  Accuracy:  0.546448087431694\n",
            "Epoch:  381  Loss:  1.2129286079308677  Accuracy:  0.5519125683060109\n",
            "Epoch:  382  Loss:  1.2140353217002509  Accuracy:  0.546448087431694\n",
            "Epoch:  383  Loss:  1.2141401078877079  Accuracy:  0.546448087431694\n",
            "Epoch:  384  Loss:  1.213645379803749  Accuracy:  0.546448087431694\n",
            "Epoch:  385  Loss:  1.212440811744817  Accuracy:  0.546448087431694\n",
            "Epoch:  386  Loss:  1.2130318218966623  Accuracy:  0.546448087431694\n",
            "Epoch:  387  Loss:  1.2134939210845634  Accuracy:  0.5409836065573771\n",
            "Epoch:  388  Loss:  1.213522244879931  Accuracy:  0.5409836065573771\n",
            "Epoch:  389  Loss:  1.2131610880555916  Accuracy:  0.5355191256830601\n",
            "Epoch:  390  Loss:  1.2120374908046432  Accuracy:  0.5355191256830601\n",
            "Epoch:  391  Loss:  1.2137600584951886  Accuracy:  0.5355191256830601\n",
            "Epoch:  392  Loss:  1.2133192175374303  Accuracy:  0.5355191256830601\n",
            "Epoch:  393  Loss:  1.2119354061851682  Accuracy:  0.5355191256830601\n",
            "Epoch:  394  Loss:  1.2127245803609763  Accuracy:  0.5355191256830601\n",
            "Epoch:  395  Loss:  1.2125110219886501  Accuracy:  0.5355191256830601\n",
            "Epoch:  396  Loss:  1.2122829195754592  Accuracy:  0.5355191256830601\n",
            "Epoch:  397  Loss:  1.2120761481140854  Accuracy:  0.5355191256830601\n",
            "Epoch:  398  Loss:  1.2123618235670628  Accuracy:  0.5355191256830601\n",
            "Epoch:  399  Loss:  1.2113055483381967  Accuracy:  0.5355191256830601\n",
            "Epoch:  400  Loss:  1.2114114189438374  Accuracy:  0.5355191256830601\n",
            "Epoch:  401  Loss:  1.2115992139707783  Accuracy:  0.5355191256830601\n",
            "Epoch:  402  Loss:  1.211750940815103  Accuracy:  0.5355191256830601\n",
            "Epoch:  403  Loss:  1.2112740938909325  Accuracy:  0.5355191256830601\n",
            "Epoch:  404  Loss:  1.211348378339128  Accuracy:  0.5355191256830601\n",
            "Epoch:  405  Loss:  1.2109058214544541  Accuracy:  0.5355191256830601\n",
            "Epoch:  406  Loss:  1.209982893417516  Accuracy:  0.5355191256830601\n",
            "Epoch:  407  Loss:  1.2106506675856457  Accuracy:  0.5355191256830601\n",
            "Epoch:  408  Loss:  1.2107679625295293  Accuracy:  0.5355191256830601\n",
            "Epoch:  409  Loss:  1.2104304394416145  Accuracy:  0.5355191256830601\n",
            "Epoch:  410  Loss:  1.2097717885827581  Accuracy:  0.5355191256830601\n",
            "Epoch:  411  Loss:  1.2090035903278133  Accuracy:  0.5355191256830601\n",
            "Epoch:  412  Loss:  1.2106586574477918  Accuracy:  0.5300546448087432\n",
            "Epoch:  413  Loss:  1.2101776823432357  Accuracy:  0.5300546448087432\n",
            "Epoch:  414  Loss:  1.208712907960472  Accuracy:  0.5300546448087432\n",
            "Epoch:  415  Loss:  1.2091958277709478  Accuracy:  0.5300546448087432\n",
            "Epoch:  416  Loss:  1.20942295331292  Accuracy:  0.5300546448087432\n",
            "Epoch:  417  Loss:  1.2085969287383898  Accuracy:  0.5300546448087432\n",
            "Epoch:  418  Loss:  1.2088842406814424  Accuracy:  0.5300546448087432\n",
            "Epoch:  419  Loss:  1.2087593905336769  Accuracy:  0.5300546448087432\n",
            "Epoch:  420  Loss:  1.2077382504139995  Accuracy:  0.5300546448087432\n",
            "Epoch:  421  Loss:  1.2083158124326503  Accuracy:  0.5300546448087432\n",
            "Epoch:  422  Loss:  1.2080573848573095  Accuracy:  0.5300546448087432\n",
            "Epoch:  423  Loss:  1.2083830424250688  Accuracy:  0.5300546448087432\n",
            "Epoch:  424  Loss:  1.207233442203732  Accuracy:  0.5300546448087432\n",
            "Epoch:  425  Loss:  1.2082667190426029  Accuracy:  0.5300546448087432\n",
            "Epoch:  426  Loss:  1.2080639266843325  Accuracy:  0.5300546448087432\n",
            "Epoch:  427  Loss:  1.208274334277714  Accuracy:  0.5300546448087432\n",
            "Epoch:  428  Loss:  1.2065475282927607  Accuracy:  0.5300546448087432\n",
            "Epoch:  429  Loss:  1.2074730912781368  Accuracy:  0.5300546448087432\n",
            "Epoch:  430  Loss:  1.207218204841288  Accuracy:  0.5300546448087432\n",
            "Epoch:  431  Loss:  1.2072805450207356  Accuracy:  0.5300546448087432\n",
            "Epoch:  432  Loss:  1.205661502823409  Accuracy:  0.5300546448087432\n",
            "Epoch:  433  Loss:  1.2072595957397159  Accuracy:  0.5300546448087432\n",
            "Epoch:  434  Loss:  1.207387858002819  Accuracy:  0.5300546448087432\n",
            "Epoch:  435  Loss:  1.2054432450877997  Accuracy:  0.5409836065573771\n",
            "Epoch:  436  Loss:  1.2066025895823715  Accuracy:  0.5300546448087432\n",
            "Epoch:  437  Loss:  1.2064783832483308  Accuracy:  0.5409836065573771\n",
            "Epoch:  438  Loss:  1.2067174376835712  Accuracy:  0.5409836065573771\n",
            "Epoch:  439  Loss:  1.2052338594562233  Accuracy:  0.5409836065573771\n",
            "Epoch:  440  Loss:  1.2058858123617782  Accuracy:  0.5409836065573771\n",
            "Epoch:  441  Loss:  1.2057929551262208  Accuracy:  0.5409836065573771\n",
            "Epoch:  442  Loss:  1.2059664366739178  Accuracy:  0.5409836065573771\n",
            "Epoch:  443  Loss:  1.2055469316111025  Accuracy:  0.5409836065573771\n",
            "Epoch:  444  Loss:  1.2049151885990153  Accuracy:  0.5409836065573771\n",
            "Epoch:  445  Loss:  1.205816503812187  Accuracy:  0.5409836065573771\n",
            "Epoch:  446  Loss:  1.2043729412450637  Accuracy:  0.5409836065573771\n",
            "Epoch:  447  Loss:  1.2056825542213503  Accuracy:  0.5409836065573771\n",
            "Epoch:  448  Loss:  1.2042388600331129  Accuracy:  0.5409836065573771\n",
            "Epoch:  449  Loss:  1.2052612194529468  Accuracy:  0.5409836065573771\n",
            "Epoch:  450  Loss:  1.204728451701984  Accuracy:  0.5409836065573771\n",
            "Epoch:  451  Loss:  1.2036814288728597  Accuracy:  0.5409836065573771\n",
            "Epoch:  452  Loss:  1.2040546421413647  Accuracy:  0.5409836065573771\n",
            "Epoch:  453  Loss:  1.204226678556839  Accuracy:  0.5409836065573771\n",
            "Epoch:  454  Loss:  1.2038140074053916  Accuracy:  0.5409836065573771\n",
            "Epoch:  455  Loss:  1.2027988949636441  Accuracy:  0.5409836065573771\n",
            "Epoch:  456  Loss:  1.203642662519704  Accuracy:  0.5409836065573771\n",
            "Epoch:  457  Loss:  1.2036166083849626  Accuracy:  0.5409836065573771\n",
            "Epoch:  458  Loss:  1.2025213667093526  Accuracy:  0.5409836065573771\n",
            "Epoch:  459  Loss:  1.2028911376014768  Accuracy:  0.5409836065573771\n",
            "Epoch:  460  Loss:  1.2032371238666812  Accuracy:  0.5409836065573771\n",
            "Epoch:  461  Loss:  1.2028274546821756  Accuracy:  0.5409836065573771\n",
            "Epoch:  462  Loss:  1.2026515034687795  Accuracy:  0.5409836065573771\n",
            "Epoch:  463  Loss:  1.2022335586108162  Accuracy:  0.5409836065573771\n",
            "Epoch:  464  Loss:  1.2017614991374792  Accuracy:  0.5409836065573771\n",
            "Epoch:  465  Loss:  1.201656902905119  Accuracy:  0.5409836065573771\n",
            "Epoch:  466  Loss:  1.2013565297672368  Accuracy:  0.546448087431694\n",
            "Epoch:  467  Loss:  1.2008908045505973  Accuracy:  0.5409836065573771\n",
            "Epoch:  468  Loss:  1.2010204465419605  Accuracy:  0.546448087431694\n",
            "Epoch:  469  Loss:  1.2011079128716837  Accuracy:  0.546448087431694\n",
            "Epoch:  470  Loss:  1.2009935188239247  Accuracy:  0.546448087431694\n",
            "Epoch:  471  Loss:  1.1997469621875838  Accuracy:  0.546448087431694\n",
            "Epoch:  472  Loss:  1.2010780945723933  Accuracy:  0.546448087431694\n",
            "Epoch:  473  Loss:  1.2007304432088919  Accuracy:  0.546448087431694\n",
            "Epoch:  474  Loss:  1.1995157347886303  Accuracy:  0.546448087431694\n",
            "Epoch:  475  Loss:  1.1998649556949728  Accuracy:  0.546448087431694\n",
            "Epoch:  476  Loss:  1.1998131851014366  Accuracy:  0.546448087431694\n",
            "Epoch:  477  Loss:  1.1990934299727702  Accuracy:  0.546448087431694\n",
            "Epoch:  478  Loss:  1.198518258556213  Accuracy:  0.546448087431694\n",
            "Epoch:  479  Loss:  1.1996338612076636  Accuracy:  0.546448087431694\n",
            "Epoch:  480  Loss:  1.199669481055016  Accuracy:  0.546448087431694\n",
            "Epoch:  481  Loss:  1.1982228826998995  Accuracy:  0.546448087431694\n",
            "Epoch:  482  Loss:  1.1986444293674876  Accuracy:  0.546448087431694\n",
            "Epoch:  483  Loss:  1.199422128211709  Accuracy:  0.546448087431694\n",
            "Epoch:  484  Loss:  1.1977448306278602  Accuracy:  0.546448087431694\n",
            "Epoch:  485  Loss:  1.1981938561300485  Accuracy:  0.546448087431694\n",
            "Epoch:  486  Loss:  1.1972761642276268  Accuracy:  0.546448087431694\n",
            "Epoch:  487  Loss:  1.1969292371112408  Accuracy:  0.546448087431694\n",
            "Epoch:  488  Loss:  1.1966966747401298  Accuracy:  0.546448087431694\n",
            "Epoch:  489  Loss:  1.1964250052219734  Accuracy:  0.546448087431694\n",
            "Epoch:  490  Loss:  1.1953379127997248  Accuracy:  0.546448087431694\n",
            "Epoch:  491  Loss:  1.1954071870042178  Accuracy:  0.546448087431694\n",
            "Epoch:  492  Loss:  1.1948276162035336  Accuracy:  0.5519125683060109\n",
            "Epoch:  493  Loss:  1.1936818973378098  Accuracy:  0.5519125683060109\n",
            "Epoch:  494  Loss:  1.1936856182179076  Accuracy:  0.5519125683060109\n",
            "Epoch:  495  Loss:  1.1935336109927206  Accuracy:  0.5519125683060109\n",
            "Epoch:  496  Loss:  1.1927002956339459  Accuracy:  0.5519125683060109\n",
            "Epoch:  497  Loss:  1.1933233514980008  Accuracy:  0.5519125683060109\n",
            "Epoch:  498  Loss:  1.1928059913239135  Accuracy:  0.5519125683060109\n",
            "Epoch:  499  Loss:  1.192634708768784  Accuracy:  0.5519125683060109\n",
            "Epoch:  500  Loss:  1.191404132272414  Accuracy:  0.5519125683060109\n",
            "Epoch:  501  Loss:  1.1913757578999364  Accuracy:  0.5519125683060109\n",
            "Epoch:  502  Loss:  1.1910523982785755  Accuracy:  0.5519125683060109\n",
            "Epoch:  503  Loss:  1.1906955339615946  Accuracy:  0.5519125683060109\n",
            "Epoch:  504  Loss:  1.190413250977951  Accuracy:  0.5519125683060109\n",
            "Epoch:  505  Loss:  1.1891511597828033  Accuracy:  0.5519125683060109\n",
            "Epoch:  506  Loss:  1.1900949087852661  Accuracy:  0.5519125683060109\n",
            "Epoch:  507  Loss:  1.1899120656507944  Accuracy:  0.5519125683060109\n",
            "Epoch:  508  Loss:  1.1895689685821809  Accuracy:  0.5519125683060109\n",
            "Epoch:  509  Loss:  1.1889997739020355  Accuracy:  0.5519125683060109\n",
            "Epoch:  510  Loss:  1.1877084632410253  Accuracy:  0.5519125683060109\n",
            "Epoch:  511  Loss:  1.1878081268345175  Accuracy:  0.5519125683060109\n",
            "Epoch:  512  Loss:  1.1879980461233897  Accuracy:  0.5519125683060109\n",
            "Epoch:  513  Loss:  1.1875652938245185  Accuracy:  0.5573770491803278\n",
            "Epoch:  514  Loss:  1.1862747106856215  Accuracy:  0.5573770491803278\n",
            "Epoch:  515  Loss:  1.1866617595992803  Accuracy:  0.5573770491803278\n",
            "Epoch:  516  Loss:  1.1870413537652504  Accuracy:  0.5573770491803278\n",
            "Epoch:  517  Loss:  1.1868973497483781  Accuracy:  0.5573770491803278\n",
            "Epoch:  518  Loss:  1.1862834549417567  Accuracy:  0.5573770491803278\n",
            "Epoch:  519  Loss:  1.1846920939026901  Accuracy:  0.5628415300546448\n",
            "Epoch:  520  Loss:  1.1864173567905967  Accuracy:  0.5628415300546448\n",
            "Epoch:  521  Loss:  1.185551362976494  Accuracy:  0.5628415300546448\n",
            "Epoch:  522  Loss:  1.1857047605058062  Accuracy:  0.5628415300546448\n",
            "Epoch:  523  Loss:  1.185628454369806  Accuracy:  0.5628415300546448\n",
            "Epoch:  524  Loss:  1.1852018013969297  Accuracy:  0.5573770491803278\n",
            "Epoch:  525  Loss:  1.184535143836931  Accuracy:  0.5573770491803278\n",
            "Epoch:  526  Loss:  1.184127515556322  Accuracy:  0.5573770491803278\n",
            "Epoch:  527  Loss:  1.1845755684705825  Accuracy:  0.5573770491803278\n",
            "Epoch:  528  Loss:  1.1843830098915056  Accuracy:  0.5573770491803278\n",
            "Epoch:  529  Loss:  1.1828322292307734  Accuracy:  0.5573770491803278\n",
            "Epoch:  530  Loss:  1.1846472406846598  Accuracy:  0.5573770491803278\n",
            "Epoch:  531  Loss:  1.1838212023871724  Accuracy:  0.5573770491803278\n",
            "Epoch:  532  Loss:  1.1829819841951414  Accuracy:  0.5573770491803278\n",
            "Epoch:  533  Loss:  1.1825752230096904  Accuracy:  0.5573770491803278\n",
            "Epoch:  534  Loss:  1.182569777878433  Accuracy:  0.5628415300546448\n",
            "Epoch:  535  Loss:  1.1827095478367933  Accuracy:  0.5628415300546448\n",
            "Epoch:  536  Loss:  1.1816751098152807  Accuracy:  0.5628415300546448\n",
            "Epoch:  537  Loss:  1.183254848242382  Accuracy:  0.5573770491803278\n",
            "Epoch:  538  Loss:  1.182120247529977  Accuracy:  0.5628415300546448\n",
            "Epoch:  539  Loss:  1.1819250731674233  Accuracy:  0.5628415300546448\n",
            "Epoch:  540  Loss:  1.1813171268634621  Accuracy:  0.5628415300546448\n",
            "Epoch:  541  Loss:  1.1809200359480507  Accuracy:  0.5628415300546448\n",
            "Epoch:  542  Loss:  1.1810831961371726  Accuracy:  0.5573770491803278\n",
            "Epoch:  543  Loss:  1.1814730672194464  Accuracy:  0.5628415300546448\n",
            "Epoch:  544  Loss:  1.1814612562679192  Accuracy:  0.5519125683060109\n",
            "Epoch:  545  Loss:  1.1798401176856015  Accuracy:  0.5573770491803278\n",
            "Epoch:  546  Loss:  1.1819400842505536  Accuracy:  0.5573770491803278\n",
            "Epoch:  547  Loss:  1.1813605515496133  Accuracy:  0.5573770491803278\n",
            "Epoch:  548  Loss:  1.1812039628057145  Accuracy:  0.5573770491803278\n",
            "Epoch:  549  Loss:  1.180349167943867  Accuracy:  0.5573770491803278\n",
            "Epoch:  550  Loss:  1.180902459338299  Accuracy:  0.5573770491803278\n",
            "Epoch:  551  Loss:  1.1812843106188553  Accuracy:  0.5573770491803278\n",
            "Epoch:  552  Loss:  1.1806261253820003  Accuracy:  0.5573770491803278\n",
            "Epoch:  553  Loss:  1.1807595888049651  Accuracy:  0.5573770491803278\n",
            "Epoch:  554  Loss:  1.1814577605063978  Accuracy:  0.5519125683060109\n",
            "Epoch:  555  Loss:  1.1809201353714842  Accuracy:  0.5573770491803278\n",
            "Epoch:  556  Loss:  1.1812060005067722  Accuracy:  0.5573770491803278\n",
            "Epoch:  557  Loss:  1.1801392504142223  Accuracy:  0.5573770491803278\n",
            "Epoch:  558  Loss:  1.1817855619121957  Accuracy:  0.5573770491803278\n",
            "Epoch:  559  Loss:  1.180390306305894  Accuracy:  0.5573770491803278\n",
            "Epoch:  560  Loss:  1.1812201462592997  Accuracy:  0.5573770491803278\n",
            "Epoch:  561  Loss:  1.1803340880603186  Accuracy:  0.5573770491803278\n",
            "Epoch:  562  Loss:  1.1811709238722357  Accuracy:  0.5573770491803278\n",
            "Epoch:  563  Loss:  1.1813996278188525  Accuracy:  0.5573770491803278\n",
            "Epoch:  564  Loss:  1.1809686444511183  Accuracy:  0.5573770491803278\n",
            "Epoch:  565  Loss:  1.1796171228551515  Accuracy:  0.5573770491803278\n",
            "Epoch:  566  Loss:  1.1814080675018361  Accuracy:  0.5573770491803278\n",
            "Epoch:  567  Loss:  1.180997640063812  Accuracy:  0.5573770491803278\n",
            "Epoch:  568  Loss:  1.1813339171371475  Accuracy:  0.5573770491803278\n",
            "Epoch:  569  Loss:  1.1812577869841772  Accuracy:  0.5573770491803278\n",
            "Epoch:  570  Loss:  1.1808395154901814  Accuracy:  0.5519125683060109\n",
            "Epoch:  571  Loss:  1.1812871562894514  Accuracy:  0.5519125683060109\n",
            "Epoch:  572  Loss:  1.181857006198892  Accuracy:  0.5519125683060109\n",
            "Epoch:  573  Loss:  1.1811948809964012  Accuracy:  0.5519125683060109\n",
            "Epoch:  574  Loss:  1.1818928463004528  Accuracy:  0.5519125683060109\n",
            "Epoch:  575  Loss:  1.181649791610473  Accuracy:  0.5519125683060109\n",
            "Epoch:  576  Loss:  1.1803584398098008  Accuracy:  0.5519125683060109\n",
            "Epoch:  577  Loss:  1.1818563426533784  Accuracy:  0.5519125683060109\n",
            "Epoch:  578  Loss:  1.1815862601912426  Accuracy:  0.5519125683060109\n",
            "Epoch:  579  Loss:  1.180693096141094  Accuracy:  0.5519125683060109\n",
            "Epoch:  580  Loss:  1.1809654376955065  Accuracy:  0.5519125683060109\n",
            "Epoch:  581  Loss:  1.180934226127966  Accuracy:  0.5519125683060109\n",
            "Epoch:  582  Loss:  1.180012558694633  Accuracy:  0.5519125683060109\n",
            "Epoch:  583  Loss:  1.1802857214714693  Accuracy:  0.5573770491803278\n",
            "Epoch:  584  Loss:  1.180474904547949  Accuracy:  0.5573770491803278\n",
            "Epoch:  585  Loss:  1.1799702360119952  Accuracy:  0.5573770491803278\n",
            "Epoch:  586  Loss:  1.1800716908263462  Accuracy:  0.5573770491803278\n",
            "Epoch:  587  Loss:  1.1788759351887252  Accuracy:  0.5573770491803278\n",
            "Epoch:  588  Loss:  1.1792826791502227  Accuracy:  0.5573770491803278\n",
            "Epoch:  589  Loss:  1.1790843354999916  Accuracy:  0.5573770491803278\n",
            "Epoch:  590  Loss:  1.1783077398810533  Accuracy:  0.5573770491803278\n",
            "Epoch:  591  Loss:  1.1797614651207193  Accuracy:  0.5573770491803278\n",
            "Epoch:  592  Loss:  1.179310231048373  Accuracy:  0.5573770491803278\n",
            "Epoch:  593  Loss:  1.1782483280519602  Accuracy:  0.5573770491803278\n",
            "Epoch:  594  Loss:  1.1788101411353256  Accuracy:  0.5573770491803278\n",
            "Epoch:  595  Loss:  1.1783673305123863  Accuracy:  0.5573770491803278\n",
            "Epoch:  596  Loss:  1.177948973549946  Accuracy:  0.5628415300546448\n",
            "Epoch:  597  Loss:  1.1796119197404495  Accuracy:  0.5573770491803278\n",
            "Epoch:  598  Loss:  1.1789459604395123  Accuracy:  0.5573770491803278\n",
            "Epoch:  599  Loss:  1.1783939404520423  Accuracy:  0.5573770491803278\n",
            "Epoch:  600  Loss:  1.17853694839379  Accuracy:  0.5628415300546448\n",
            "Epoch:  601  Loss:  1.1781440160105505  Accuracy:  0.5628415300546448\n",
            "Epoch:  602  Loss:  1.1788128263440432  Accuracy:  0.5628415300546448\n",
            "Epoch:  603  Loss:  1.1791183981199598  Accuracy:  0.5628415300546448\n",
            "Epoch:  604  Loss:  1.1785193134164391  Accuracy:  0.5628415300546448\n",
            "Epoch:  605  Loss:  1.179336254045436  Accuracy:  0.5628415300546448\n",
            "Epoch:  606  Loss:  1.1798526961772533  Accuracy:  0.5573770491803278\n",
            "Epoch:  607  Loss:  1.1789021356114724  Accuracy:  0.5573770491803278\n",
            "Epoch:  608  Loss:  1.1787730031006305  Accuracy:  0.5628415300546448\n",
            "Epoch:  609  Loss:  1.1795459809368034  Accuracy:  0.5628415300546448\n",
            "Epoch:  610  Loss:  1.1781713208794677  Accuracy:  0.5628415300546448\n",
            "Epoch:  611  Loss:  1.1799631431462028  Accuracy:  0.5573770491803278\n",
            "Epoch:  612  Loss:  1.1792121204055241  Accuracy:  0.5628415300546448\n",
            "Epoch:  613  Loss:  1.1789763875086168  Accuracy:  0.5628415300546448\n",
            "Epoch:  614  Loss:  1.180691908862891  Accuracy:  0.5573770491803278\n",
            "Epoch:  615  Loss:  1.1795258916026057  Accuracy:  0.5628415300546448\n",
            "Epoch:  616  Loss:  1.1799418287179906  Accuracy:  0.5628415300546448\n",
            "Epoch:  617  Loss:  1.1796216827567505  Accuracy:  0.5628415300546448\n",
            "Epoch:  618  Loss:  1.1800898151674473  Accuracy:  0.5628415300546448\n",
            "Epoch:  619  Loss:  1.1799519605170519  Accuracy:  0.5628415300546448\n",
            "Epoch:  620  Loss:  1.1794901863856526  Accuracy:  0.5628415300546448\n",
            "Epoch:  621  Loss:  1.1798480627221068  Accuracy:  0.5628415300546448\n",
            "Epoch:  622  Loss:  1.1801387472007563  Accuracy:  0.5628415300546448\n",
            "Epoch:  623  Loss:  1.1799150339985114  Accuracy:  0.5628415300546448\n",
            "Epoch:  624  Loss:  1.1802103413677403  Accuracy:  0.5628415300546448\n",
            "Epoch:  625  Loss:  1.180352197336486  Accuracy:  0.5683060109289617\n",
            "Epoch:  626  Loss:  1.180729791316468  Accuracy:  0.5683060109289617\n",
            "Epoch:  627  Loss:  1.1798798628210536  Accuracy:  0.5683060109289617\n",
            "Epoch:  628  Loss:  1.1812529325890508  Accuracy:  0.5683060109289617\n",
            "Epoch:  629  Loss:  1.1806622762617949  Accuracy:  0.5683060109289617\n",
            "Epoch:  630  Loss:  1.1808667855589976  Accuracy:  0.5683060109289617\n",
            "Epoch:  631  Loss:  1.1807783613606002  Accuracy:  0.5683060109289617\n",
            "Epoch:  632  Loss:  1.1813000682688672  Accuracy:  0.5683060109289617\n",
            "Epoch:  633  Loss:  1.1808200115113103  Accuracy:  0.5683060109289617\n",
            "Epoch:  634  Loss:  1.181305200722733  Accuracy:  0.5683060109289617\n",
            "Epoch:  635  Loss:  1.1808230467491023  Accuracy:  0.5683060109289617\n",
            "Epoch:  636  Loss:  1.181450312915865  Accuracy:  0.5683060109289617\n",
            "Epoch:  637  Loss:  1.181476965646795  Accuracy:  0.5683060109289617\n",
            "Epoch:  638  Loss:  1.1815694900807499  Accuracy:  0.5683060109289617\n",
            "Epoch:  639  Loss:  1.1813597893419956  Accuracy:  0.5683060109289617\n",
            "Epoch:  640  Loss:  1.1813104187971948  Accuracy:  0.5683060109289617\n",
            "Epoch:  641  Loss:  1.1815883797386901  Accuracy:  0.5683060109289617\n",
            "Epoch:  642  Loss:  1.1818542219228532  Accuracy:  0.5683060109289617\n",
            "Epoch:  643  Loss:  1.1815874905013373  Accuracy:  0.5683060109289617\n",
            "Epoch:  644  Loss:  1.182037388536484  Accuracy:  0.5683060109289617\n",
            "Epoch:  645  Loss:  1.1817796311886968  Accuracy:  0.5628415300546448\n",
            "Epoch:  646  Loss:  1.181872589770037  Accuracy:  0.5683060109289617\n",
            "Epoch:  647  Loss:  1.182123804325979  Accuracy:  0.5573770491803278\n",
            "Epoch:  648  Loss:  1.1820928440594614  Accuracy:  0.5683060109289617\n",
            "Epoch:  649  Loss:  1.1816440089435225  Accuracy:  0.5628415300546448\n",
            "Epoch:  650  Loss:  1.1823046357951956  Accuracy:  0.5573770491803278\n",
            "Epoch:  651  Loss:  1.1829946281989412  Accuracy:  0.5683060109289617\n",
            "Epoch:  652  Loss:  1.1822644984096229  Accuracy:  0.5573770491803278\n",
            "Epoch:  653  Loss:  1.1823151829346903  Accuracy:  0.5683060109289617\n",
            "Epoch:  654  Loss:  1.1821425989272962  Accuracy:  0.5628415300546448\n",
            "Epoch:  655  Loss:  1.1825755058218448  Accuracy:  0.5573770491803278\n",
            "Epoch:  656  Loss:  1.1821374354349894  Accuracy:  0.5628415300546448\n",
            "Epoch:  657  Loss:  1.182588459350748  Accuracy:  0.5573770491803278\n",
            "Epoch:  658  Loss:  1.1823178896822657  Accuracy:  0.5628415300546448\n",
            "Epoch:  659  Loss:  1.182726687674342  Accuracy:  0.5628415300546448\n",
            "Epoch:  660  Loss:  1.1824655662806167  Accuracy:  0.5573770491803278\n",
            "Epoch:  661  Loss:  1.1830796551847649  Accuracy:  0.5519125683060109\n",
            "Epoch:  662  Loss:  1.1823589600319198  Accuracy:  0.5519125683060109\n",
            "Epoch:  663  Loss:  1.182817897893403  Accuracy:  0.5519125683060109\n",
            "Epoch:  664  Loss:  1.1825487025089805  Accuracy:  0.5573770491803278\n",
            "Epoch:  665  Loss:  1.1827418483723682  Accuracy:  0.5573770491803278\n",
            "Epoch:  666  Loss:  1.183420159995933  Accuracy:  0.5519125683060109\n",
            "Epoch:  667  Loss:  1.1828727756218562  Accuracy:  0.5573770491803278\n",
            "Epoch:  668  Loss:  1.1828699196528651  Accuracy:  0.5573770491803278\n",
            "Epoch:  669  Loss:  1.1829745255192472  Accuracy:  0.5573770491803278\n",
            "Epoch:  670  Loss:  1.1831212474003612  Accuracy:  0.5573770491803278\n",
            "Epoch:  671  Loss:  1.1834796268414538  Accuracy:  0.546448087431694\n",
            "Epoch:  672  Loss:  1.1832294555900311  Accuracy:  0.546448087431694\n",
            "Epoch:  673  Loss:  1.1831898687859752  Accuracy:  0.5519125683060109\n",
            "Epoch:  674  Loss:  1.1839750416206591  Accuracy:  0.546448087431694\n",
            "Epoch:  675  Loss:  1.183547261687565  Accuracy:  0.5519125683060109\n",
            "Epoch:  676  Loss:  1.1838188708486594  Accuracy:  0.5519125683060109\n",
            "Epoch:  677  Loss:  1.1846271671932749  Accuracy:  0.546448087431694\n",
            "Epoch:  678  Loss:  1.1845195432265891  Accuracy:  0.546448087431694\n",
            "Epoch:  679  Loss:  1.185317369926462  Accuracy:  0.546448087431694\n",
            "Epoch:  680  Loss:  1.184508879577617  Accuracy:  0.546448087431694\n",
            "Epoch:  681  Loss:  1.1848124470018921  Accuracy:  0.5519125683060109\n",
            "Epoch:  682  Loss:  1.1853262483162457  Accuracy:  0.546448087431694\n",
            "Epoch:  683  Loss:  1.1846553826431034  Accuracy:  0.546448087431694\n",
            "Epoch:  684  Loss:  1.1857453288367545  Accuracy:  0.546448087431694\n",
            "Epoch:  685  Loss:  1.186406517120307  Accuracy:  0.546448087431694\n",
            "Epoch:  686  Loss:  1.1858440907708387  Accuracy:  0.546448087431694\n",
            "Epoch:  687  Loss:  1.1857071892961926  Accuracy:  0.546448087431694\n",
            "Epoch:  688  Loss:  1.1870533861517032  Accuracy:  0.546448087431694\n",
            "Epoch:  689  Loss:  1.1868676049956488  Accuracy:  0.546448087431694\n",
            "Epoch:  690  Loss:  1.1871885428697777  Accuracy:  0.546448087431694\n",
            "Epoch:  691  Loss:  1.1866308695655645  Accuracy:  0.546448087431694\n",
            "Epoch:  692  Loss:  1.1873493604458698  Accuracy:  0.546448087431694\n",
            "Epoch:  693  Loss:  1.1876661341692547  Accuracy:  0.546448087431694\n",
            "Epoch:  694  Loss:  1.1872989768716844  Accuracy:  0.546448087431694\n",
            "Epoch:  695  Loss:  1.1876394744434502  Accuracy:  0.546448087431694\n",
            "Epoch:  696  Loss:  1.1880659707624244  Accuracy:  0.546448087431694\n",
            "Epoch:  697  Loss:  1.1875413406088167  Accuracy:  0.546448087431694\n",
            "Epoch:  698  Loss:  1.1883229965078559  Accuracy:  0.546448087431694\n",
            "Epoch:  699  Loss:  1.1879261319061767  Accuracy:  0.546448087431694\n",
            "Epoch:  700  Loss:  1.188785684308068  Accuracy:  0.546448087431694\n",
            "Epoch:  701  Loss:  1.1884420620240188  Accuracy:  0.546448087431694\n",
            "Epoch:  702  Loss:  1.188796553469162  Accuracy:  0.546448087431694\n",
            "Epoch:  703  Loss:  1.1881555936089974  Accuracy:  0.546448087431694\n",
            "Epoch:  704  Loss:  1.189220521193755  Accuracy:  0.546448087431694\n",
            "Epoch:  705  Loss:  1.189702991900158  Accuracy:  0.546448087431694\n",
            "Epoch:  706  Loss:  1.1895243650523661  Accuracy:  0.546448087431694\n",
            "Epoch:  707  Loss:  1.1898744571062752  Accuracy:  0.546448087431694\n",
            "Epoch:  708  Loss:  1.1896614899550464  Accuracy:  0.546448087431694\n",
            "Epoch:  709  Loss:  1.1898722671404613  Accuracy:  0.546448087431694\n",
            "Epoch:  710  Loss:  1.1901608436376692  Accuracy:  0.546448087431694\n",
            "Epoch:  711  Loss:  1.190570902645323  Accuracy:  0.546448087431694\n",
            "Epoch:  712  Loss:  1.1913630960385817  Accuracy:  0.546448087431694\n",
            "Epoch:  713  Loss:  1.1912964277754625  Accuracy:  0.546448087431694\n",
            "Epoch:  714  Loss:  1.1913793609356402  Accuracy:  0.546448087431694\n",
            "Epoch:  715  Loss:  1.1910006192842857  Accuracy:  0.546448087431694\n",
            "Epoch:  716  Loss:  1.1925301363977003  Accuracy:  0.546448087431694\n",
            "Epoch:  717  Loss:  1.1926478052951672  Accuracy:  0.546448087431694\n",
            "Epoch:  718  Loss:  1.1918894564677225  Accuracy:  0.546448087431694\n",
            "Epoch:  719  Loss:  1.1922862495720512  Accuracy:  0.5519125683060109\n",
            "Epoch:  720  Loss:  1.1925170987392755  Accuracy:  0.546448087431694\n",
            "Epoch:  721  Loss:  1.19246890879366  Accuracy:  0.5519125683060109\n",
            "Epoch:  722  Loss:  1.1919913726978626  Accuracy:  0.5519125683060109\n",
            "Epoch:  723  Loss:  1.1923188639511975  Accuracy:  0.546448087431694\n",
            "Epoch:  724  Loss:  1.192960030100597  Accuracy:  0.5519125683060109\n",
            "Epoch:  725  Loss:  1.193005659206022  Accuracy:  0.546448087431694\n",
            "Epoch:  726  Loss:  1.1938290915230108  Accuracy:  0.5519125683060109\n",
            "Epoch:  727  Loss:  1.192779460569026  Accuracy:  0.5519125683060109\n",
            "Epoch:  728  Loss:  1.193206206787949  Accuracy:  0.546448087431694\n",
            "Epoch:  729  Loss:  1.1926932034787332  Accuracy:  0.546448087431694\n",
            "Epoch:  730  Loss:  1.1935460935148818  Accuracy:  0.546448087431694\n",
            "Epoch:  731  Loss:  1.193641644691072  Accuracy:  0.5573770491803278\n",
            "Epoch:  732  Loss:  1.1929451857036701  Accuracy:  0.5519125683060109\n",
            "Epoch:  733  Loss:  1.1942621806219282  Accuracy:  0.5519125683060109\n",
            "Epoch:  734  Loss:  1.1943361633864842  Accuracy:  0.5519125683060109\n",
            "Epoch:  735  Loss:  1.1946587956806314  Accuracy:  0.5519125683060109\n",
            "Epoch:  736  Loss:  1.1941010945593165  Accuracy:  0.5519125683060109\n",
            "Epoch:  737  Loss:  1.1950048101869515  Accuracy:  0.5519125683060109\n",
            "Epoch:  738  Loss:  1.1948190950147106  Accuracy:  0.5573770491803278\n",
            "Epoch:  739  Loss:  1.1954037193519667  Accuracy:  0.5573770491803278\n",
            "Epoch:  740  Loss:  1.195570294774386  Accuracy:  0.5519125683060109\n",
            "Epoch:  741  Loss:  1.1945814952770994  Accuracy:  0.546448087431694\n",
            "Epoch:  742  Loss:  1.1954279082677237  Accuracy:  0.5519125683060109\n",
            "Epoch:  743  Loss:  1.1952215353677853  Accuracy:  0.5519125683060109\n",
            "Epoch:  744  Loss:  1.1959629877885047  Accuracy:  0.5519125683060109\n",
            "Epoch:  745  Loss:  1.1959061025336797  Accuracy:  0.546448087431694\n",
            "Epoch:  746  Loss:  1.1967612030392296  Accuracy:  0.5519125683060109\n",
            "Epoch:  747  Loss:  1.1969461155809038  Accuracy:  0.5519125683060109\n",
            "Epoch:  748  Loss:  1.1963185331015038  Accuracy:  0.546448087431694\n",
            "Epoch:  749  Loss:  1.196088197788872  Accuracy:  0.5519125683060109\n",
            "Epoch:  750  Loss:  1.1976287265872814  Accuracy:  0.5519125683060109\n",
            "Epoch:  751  Loss:  1.1959822441188566  Accuracy:  0.5519125683060109\n",
            "Epoch:  752  Loss:  1.1977055185242194  Accuracy:  0.5519125683060109\n",
            "Epoch:  753  Loss:  1.197512541528523  Accuracy:  0.5519125683060109\n",
            "Epoch:  754  Loss:  1.197818932392121  Accuracy:  0.546448087431694\n",
            "Epoch:  755  Loss:  1.1973024402656822  Accuracy:  0.546448087431694\n",
            "Epoch:  756  Loss:  1.1979895648811205  Accuracy:  0.5519125683060109\n",
            "Epoch:  757  Loss:  1.1971701210008077  Accuracy:  0.546448087431694\n",
            "Epoch:  758  Loss:  1.1965378652807046  Accuracy:  0.546448087431694\n",
            "Epoch:  759  Loss:  1.197455493617877  Accuracy:  0.546448087431694\n",
            "Epoch:  760  Loss:  1.197725916352155  Accuracy:  0.546448087431694\n",
            "Epoch:  761  Loss:  1.1981446476833697  Accuracy:  0.546448087431694\n",
            "Epoch:  762  Loss:  1.1975328015942088  Accuracy:  0.546448087431694\n",
            "Epoch:  763  Loss:  1.1971357736462247  Accuracy:  0.546448087431694\n",
            "Epoch:  764  Loss:  1.1981667934041165  Accuracy:  0.546448087431694\n",
            "Epoch:  765  Loss:  1.1970298779611313  Accuracy:  0.546448087431694\n",
            "Epoch:  766  Loss:  1.198339726123318  Accuracy:  0.546448087431694\n",
            "Epoch:  767  Loss:  1.197653877469901  Accuracy:  0.546448087431694\n",
            "Epoch:  768  Loss:  1.1985328591250464  Accuracy:  0.546448087431694\n",
            "Epoch:  769  Loss:  1.1978122805857705  Accuracy:  0.5409836065573771\n",
            "Epoch:  770  Loss:  1.1979844405104945  Accuracy:  0.546448087431694\n",
            "Epoch:  771  Loss:  1.1975234614290522  Accuracy:  0.5409836065573771\n",
            "Epoch:  772  Loss:  1.197707486764952  Accuracy:  0.5409836065573771\n",
            "Epoch:  773  Loss:  1.197694645076907  Accuracy:  0.5409836065573771\n",
            "Epoch:  774  Loss:  1.1975928307102712  Accuracy:  0.5409836065573771\n",
            "Epoch:  775  Loss:  1.1982325697689749  Accuracy:  0.5409836065573771\n",
            "Epoch:  776  Loss:  1.19811779256458  Accuracy:  0.5409836065573771\n",
            "Epoch:  777  Loss:  1.1976247279678363  Accuracy:  0.5409836065573771\n",
            "Epoch:  778  Loss:  1.1969635582954832  Accuracy:  0.5409836065573771\n",
            "Epoch:  779  Loss:  1.1964337667392897  Accuracy:  0.5409836065573771\n",
            "Epoch:  780  Loss:  1.1975554896859661  Accuracy:  0.5409836065573771\n",
            "Epoch:  781  Loss:  1.1967660510469549  Accuracy:  0.5409836065573771\n",
            "Epoch:  782  Loss:  1.1976109750733117  Accuracy:  0.5409836065573771\n",
            "Epoch:  783  Loss:  1.1978317174744328  Accuracy:  0.5409836065573771\n",
            "Epoch:  784  Loss:  1.196938631373452  Accuracy:  0.5409836065573771\n",
            "Epoch:  785  Loss:  1.19743836145549  Accuracy:  0.5409836065573771\n",
            "Epoch:  786  Loss:  1.1976924224927938  Accuracy:  0.5409836065573771\n",
            "Epoch:  787  Loss:  1.1975481632525413  Accuracy:  0.5409836065573771\n",
            "Epoch:  788  Loss:  1.1966702054572544  Accuracy:  0.5409836065573771\n",
            "Epoch:  789  Loss:  1.1978118703211615  Accuracy:  0.5409836065573771\n",
            "Epoch:  790  Loss:  1.1970745804414438  Accuracy:  0.5409836065573771\n",
            "Epoch:  791  Loss:  1.1975024806614443  Accuracy:  0.5409836065573771\n",
            "Epoch:  792  Loss:  1.197830502578923  Accuracy:  0.5409836065573771\n",
            "Epoch:  793  Loss:  1.1978577723791852  Accuracy:  0.5409836065573771\n",
            "Epoch:  794  Loss:  1.197968774199449  Accuracy:  0.5409836065573771\n",
            "Epoch:  795  Loss:  1.1977975259205396  Accuracy:  0.5409836065573771\n",
            "Epoch:  796  Loss:  1.1984795978075031  Accuracy:  0.5409836065573771\n",
            "Epoch:  797  Loss:  1.1987100226020708  Accuracy:  0.5409836065573771\n",
            "Epoch:  798  Loss:  1.1989982726416886  Accuracy:  0.5409836065573771\n",
            "Epoch:  799  Loss:  1.1983022933432323  Accuracy:  0.5409836065573771\n",
            "Epoch:  800  Loss:  1.1983416484176614  Accuracy:  0.5409836065573771\n",
            "Epoch:  801  Loss:  1.2000074880457219  Accuracy:  0.5409836065573771\n",
            "Epoch:  802  Loss:  1.1996635196268308  Accuracy:  0.5409836065573771\n",
            "Epoch:  803  Loss:  1.199488149514551  Accuracy:  0.5409836065573771\n",
            "Epoch:  804  Loss:  1.2005471018547484  Accuracy:  0.5409836065573771\n",
            "Epoch:  805  Loss:  1.2007015853336176  Accuracy:  0.5409836065573771\n",
            "Epoch:  806  Loss:  1.1996583555752622  Accuracy:  0.5409836065573771\n",
            "Epoch:  807  Loss:  1.2008705298126148  Accuracy:  0.5409836065573771\n",
            "Epoch:  808  Loss:  1.2004350069172567  Accuracy:  0.5409836065573771\n",
            "Epoch:  809  Loss:  1.201177129182305  Accuracy:  0.5409836065573771\n",
            "Epoch:  810  Loss:  1.2013408256059026  Accuracy:  0.5409836065573771\n",
            "Epoch:  811  Loss:  1.2007661663742988  Accuracy:  0.5409836065573771\n",
            "Epoch:  812  Loss:  1.2018072596441767  Accuracy:  0.5409836065573771\n",
            "Epoch:  813  Loss:  1.2008010995093565  Accuracy:  0.5409836065573771\n",
            "Epoch:  814  Loss:  1.2028091760936968  Accuracy:  0.5409836065573771\n",
            "Epoch:  815  Loss:  1.2025851727166537  Accuracy:  0.5409836065573771\n",
            "Epoch:  816  Loss:  1.2017346009839325  Accuracy:  0.5355191256830601\n",
            "Epoch:  817  Loss:  1.2024567661636956  Accuracy:  0.5409836065573771\n",
            "Epoch:  818  Loss:  1.2026891313537622  Accuracy:  0.5355191256830601\n",
            "Epoch:  819  Loss:  1.2022844033491882  Accuracy:  0.5355191256830601\n",
            "Epoch:  820  Loss:  1.202899814069705  Accuracy:  0.5409836065573771\n",
            "Epoch:  821  Loss:  1.2026623779911954  Accuracy:  0.5355191256830601\n",
            "Epoch:  822  Loss:  1.203232378250451  Accuracy:  0.5355191256830601\n",
            "Epoch:  823  Loss:  1.2026807771526542  Accuracy:  0.5355191256830601\n",
            "Epoch:  824  Loss:  1.2031922435166524  Accuracy:  0.5355191256830601\n",
            "Epoch:  825  Loss:  1.2031689566340766  Accuracy:  0.5355191256830601\n",
            "Epoch:  826  Loss:  1.2034050811796613  Accuracy:  0.5355191256830601\n",
            "Epoch:  827  Loss:  1.203086155478956  Accuracy:  0.5355191256830601\n",
            "Epoch:  828  Loss:  1.203950388551235  Accuracy:  0.5355191256830601\n",
            "Epoch:  829  Loss:  1.2040483174026773  Accuracy:  0.5355191256830601\n",
            "Epoch:  830  Loss:  1.2036782273040427  Accuracy:  0.5355191256830601\n",
            "Epoch:  831  Loss:  1.2045746547157976  Accuracy:  0.5355191256830601\n",
            "Epoch:  832  Loss:  1.2043128233516704  Accuracy:  0.5355191256830601\n",
            "Epoch:  833  Loss:  1.2047360887086236  Accuracy:  0.5355191256830601\n",
            "Epoch:  834  Loss:  1.2047704896005482  Accuracy:  0.5355191256830601\n",
            "Epoch:  835  Loss:  1.205256388464688  Accuracy:  0.5355191256830601\n",
            "Epoch:  836  Loss:  1.2048113021955116  Accuracy:  0.5355191256830601\n",
            "Epoch:  837  Loss:  1.2046006431600644  Accuracy:  0.5409836065573771\n",
            "Epoch:  838  Loss:  1.2045887833205078  Accuracy:  0.5355191256830601\n",
            "Epoch:  839  Loss:  1.204577102006944  Accuracy:  0.5355191256830601\n",
            "Epoch:  840  Loss:  1.2049974915986985  Accuracy:  0.5355191256830601\n",
            "Epoch:  841  Loss:  1.2055812555306347  Accuracy:  0.5409836065573771\n",
            "Epoch:  842  Loss:  1.205110904760479  Accuracy:  0.5409836065573771\n",
            "Epoch:  843  Loss:  1.204641746287066  Accuracy:  0.5409836065573771\n",
            "Epoch:  844  Loss:  1.205320942891846  Accuracy:  0.5409836065573771\n",
            "Epoch:  845  Loss:  1.205133219709689  Accuracy:  0.5409836065573771\n",
            "Epoch:  846  Loss:  1.2049229518942552  Accuracy:  0.5409836065573771\n",
            "Epoch:  847  Loss:  1.205542415875915  Accuracy:  0.5409836065573771\n",
            "Epoch:  848  Loss:  1.20530351802928  Accuracy:  0.5409836065573771\n",
            "Epoch:  849  Loss:  1.2046184875698789  Accuracy:  0.5409836065573771\n",
            "Epoch:  850  Loss:  1.2053376071348496  Accuracy:  0.5409836065573771\n",
            "Epoch:  851  Loss:  1.205412748764916  Accuracy:  0.5409836065573771\n",
            "Epoch:  852  Loss:  1.2056407630456885  Accuracy:  0.5409836065573771\n",
            "Epoch:  853  Loss:  1.2057263782816714  Accuracy:  0.5409836065573771\n",
            "Epoch:  854  Loss:  1.205896572134457  Accuracy:  0.5409836065573771\n",
            "Epoch:  855  Loss:  1.2058716129162588  Accuracy:  0.5409836065573771\n",
            "Epoch:  856  Loss:  1.2047439931568211  Accuracy:  0.5409836065573771\n",
            "Epoch:  857  Loss:  1.2057836898040706  Accuracy:  0.5409836065573771\n",
            "Epoch:  858  Loss:  1.2056107120014288  Accuracy:  0.5409836065573771\n",
            "Epoch:  859  Loss:  1.2055336434130768  Accuracy:  0.5409836065573771\n",
            "Epoch:  860  Loss:  1.2061543069554739  Accuracy:  0.5409836065573771\n",
            "Epoch:  861  Loss:  1.205742643392188  Accuracy:  0.5409836065573771\n",
            "Epoch:  862  Loss:  1.2060495630582038  Accuracy:  0.5409836065573771\n",
            "Epoch:  863  Loss:  1.2055469340055187  Accuracy:  0.5409836065573771\n",
            "Epoch:  864  Loss:  1.2060994670064356  Accuracy:  0.5409836065573771\n",
            "Epoch:  865  Loss:  1.205800219095105  Accuracy:  0.5409836065573771\n",
            "Epoch:  866  Loss:  1.206200326942184  Accuracy:  0.5409836065573771\n",
            "Epoch:  867  Loss:  1.205756183765492  Accuracy:  0.5409836065573771\n",
            "Epoch:  868  Loss:  1.20598218852257  Accuracy:  0.5409836065573771\n",
            "Epoch:  869  Loss:  1.2062810847264376  Accuracy:  0.5409836065573771\n",
            "Epoch:  870  Loss:  1.2054829304317791  Accuracy:  0.5409836065573771\n",
            "Epoch:  871  Loss:  1.2069224721321117  Accuracy:  0.5409836065573771\n",
            "Epoch:  872  Loss:  1.2064446923906633  Accuracy:  0.5409836065573771\n",
            "Epoch:  873  Loss:  1.206661915309995  Accuracy:  0.5409836065573771\n",
            "Epoch:  874  Loss:  1.2072207288971366  Accuracy:  0.5409836065573771\n",
            "Epoch:  875  Loss:  1.2069603960905653  Accuracy:  0.5409836065573771\n",
            "Epoch:  876  Loss:  1.2069502549916804  Accuracy:  0.5409836065573771\n",
            "Epoch:  877  Loss:  1.2064667605396264  Accuracy:  0.5409836065573771\n",
            "Epoch:  878  Loss:  1.2071570294165308  Accuracy:  0.5409836065573771\n",
            "Epoch:  879  Loss:  1.2077925976780748  Accuracy:  0.5409836065573771\n",
            "Epoch:  880  Loss:  1.2073119352975261  Accuracy:  0.5409836065573771\n",
            "Epoch:  881  Loss:  1.2077506044534787  Accuracy:  0.5409836065573771\n",
            "Epoch:  882  Loss:  1.2078828549027807  Accuracy:  0.5409836065573771\n",
            "Epoch:  883  Loss:  1.2082661434707749  Accuracy:  0.5409836065573771\n",
            "Epoch:  884  Loss:  1.2072738631120272  Accuracy:  0.5409836065573771\n",
            "Epoch:  885  Loss:  1.2077883111548833  Accuracy:  0.5409836065573771\n",
            "Epoch:  886  Loss:  1.208219685681447  Accuracy:  0.5409836065573771\n",
            "Epoch:  887  Loss:  1.207946134384925  Accuracy:  0.5409836065573771\n",
            "Epoch:  888  Loss:  1.2085703977568836  Accuracy:  0.5409836065573771\n",
            "Epoch:  889  Loss:  1.2085879409215137  Accuracy:  0.5409836065573771\n",
            "Epoch:  890  Loss:  1.208723546651605  Accuracy:  0.5409836065573771\n",
            "Epoch:  891  Loss:  1.2087632350231787  Accuracy:  0.5409836065573771\n",
            "Epoch:  892  Loss:  1.208944027224624  Accuracy:  0.5409836065573771\n",
            "Epoch:  893  Loss:  1.2087795949763267  Accuracy:  0.5409836065573771\n",
            "Epoch:  894  Loss:  1.2087900206610636  Accuracy:  0.5409836065573771\n",
            "Epoch:  895  Loss:  1.2089340905922246  Accuracy:  0.5409836065573771\n",
            "Epoch:  896  Loss:  1.208714461278523  Accuracy:  0.5409836065573771\n",
            "Epoch:  897  Loss:  1.2096413249545122  Accuracy:  0.5409836065573771\n",
            "Epoch:  898  Loss:  1.2093129589104439  Accuracy:  0.5409836065573771\n",
            "Epoch:  899  Loss:  1.2094399629792667  Accuracy:  0.5409836065573771\n",
            "Epoch:  900  Loss:  1.2095053846797614  Accuracy:  0.5409836065573771\n",
            "Epoch:  901  Loss:  1.209575760124115  Accuracy:  0.5409836065573771\n",
            "Epoch:  902  Loss:  1.2096107269863279  Accuracy:  0.546448087431694\n",
            "Epoch:  903  Loss:  1.2097088219591237  Accuracy:  0.5409836065573771\n",
            "Epoch:  904  Loss:  1.2105243498179643  Accuracy:  0.5409836065573771\n",
            "Epoch:  905  Loss:  1.2099692884576287  Accuracy:  0.546448087431694\n",
            "Epoch:  906  Loss:  1.2106452489259467  Accuracy:  0.546448087431694\n",
            "Epoch:  907  Loss:  1.2105337775814604  Accuracy:  0.546448087431694\n",
            "Epoch:  908  Loss:  1.2114651607957307  Accuracy:  0.5573770491803278\n",
            "Epoch:  909  Loss:  1.210491395219052  Accuracy:  0.5519125683060109\n",
            "Epoch:  910  Loss:  1.211354433384744  Accuracy:  0.5519125683060109\n",
            "Epoch:  911  Loss:  1.2114873868659852  Accuracy:  0.5519125683060109\n",
            "Epoch:  912  Loss:  1.2115090941836246  Accuracy:  0.5519125683060109\n",
            "Epoch:  913  Loss:  1.2113901581819746  Accuracy:  0.5573770491803278\n",
            "Epoch:  914  Loss:  1.211840333635539  Accuracy:  0.5519125683060109\n",
            "Epoch:  915  Loss:  1.2122696792679073  Accuracy:  0.5519125683060109\n",
            "Epoch:  916  Loss:  1.212348303737577  Accuracy:  0.5519125683060109\n",
            "Epoch:  917  Loss:  1.2132916868300825  Accuracy:  0.5573770491803278\n",
            "Epoch:  918  Loss:  1.2129374477569794  Accuracy:  0.5519125683060109\n",
            "Epoch:  919  Loss:  1.2129190034157946  Accuracy:  0.5519125683060109\n",
            "Epoch:  920  Loss:  1.212939492246288  Accuracy:  0.5519125683060109\n",
            "Epoch:  921  Loss:  1.2131795319102772  Accuracy:  0.5519125683060109\n",
            "Epoch:  922  Loss:  1.2139217331214869  Accuracy:  0.5519125683060109\n",
            "Epoch:  923  Loss:  1.213221974569221  Accuracy:  0.5519125683060109\n",
            "Epoch:  924  Loss:  1.214109279961782  Accuracy:  0.5519125683060109\n",
            "Epoch:  925  Loss:  1.2135749064150232  Accuracy:  0.5573770491803278\n",
            "Epoch:  926  Loss:  1.2140353741843217  Accuracy:  0.5573770491803278\n",
            "Epoch:  927  Loss:  1.2142181942132568  Accuracy:  0.5519125683060109\n",
            "Epoch:  928  Loss:  1.2148545542285942  Accuracy:  0.5519125683060109\n",
            "Epoch:  929  Loss:  1.215413488174553  Accuracy:  0.5573770491803278\n",
            "Epoch:  930  Loss:  1.2153268263823618  Accuracy:  0.5519125683060109\n",
            "Epoch:  931  Loss:  1.215332165253694  Accuracy:  0.5519125683060109\n",
            "Epoch:  932  Loss:  1.215239328815121  Accuracy:  0.5519125683060109\n",
            "Epoch:  933  Loss:  1.2156661754883238  Accuracy:  0.5573770491803278\n",
            "Epoch:  934  Loss:  1.2165765722495332  Accuracy:  0.5573770491803278\n",
            "Epoch:  935  Loss:  1.2164597166349371  Accuracy:  0.5573770491803278\n",
            "Epoch:  936  Loss:  1.2163010047479959  Accuracy:  0.5519125683060109\n",
            "Epoch:  937  Loss:  1.2160073541530403  Accuracy:  0.5519125683060109\n",
            "Epoch:  938  Loss:  1.216318133180697  Accuracy:  0.5573770491803278\n",
            "Epoch:  939  Loss:  1.2175436434292664  Accuracy:  0.5573770491803278\n",
            "Epoch:  940  Loss:  1.2170679723478117  Accuracy:  0.5573770491803278\n",
            "Epoch:  941  Loss:  1.2173922220293771  Accuracy:  0.5519125683060109\n",
            "Epoch:  942  Loss:  1.2165823095040234  Accuracy:  0.5573770491803278\n",
            "Epoch:  943  Loss:  1.2173976186971598  Accuracy:  0.5573770491803278\n",
            "Epoch:  944  Loss:  1.2178664523700065  Accuracy:  0.5573770491803278\n",
            "Epoch:  945  Loss:  1.2181344744231661  Accuracy:  0.5573770491803278\n",
            "Epoch:  946  Loss:  1.2183472583585775  Accuracy:  0.5573770491803278\n",
            "Epoch:  947  Loss:  1.2179779680857272  Accuracy:  0.5519125683060109\n",
            "Epoch:  948  Loss:  1.2183287463551526  Accuracy:  0.5573770491803278\n",
            "Epoch:  949  Loss:  1.2187543127853377  Accuracy:  0.5573770491803278\n",
            "Epoch:  950  Loss:  1.2185554421453546  Accuracy:  0.5573770491803278\n",
            "Epoch:  951  Loss:  1.2185570661387402  Accuracy:  0.5573770491803278\n",
            "Epoch:  952  Loss:  1.2184833583577737  Accuracy:  0.5573770491803278\n",
            "Epoch:  953  Loss:  1.2190544977066604  Accuracy:  0.5573770491803278\n",
            "Epoch:  954  Loss:  1.2189042641633971  Accuracy:  0.5573770491803278\n",
            "Epoch:  955  Loss:  1.2190211364175227  Accuracy:  0.5628415300546448\n",
            "Epoch:  956  Loss:  1.2194972088848508  Accuracy:  0.5573770491803278\n",
            "Epoch:  957  Loss:  1.2197759921495128  Accuracy:  0.5573770491803278\n",
            "Epoch:  958  Loss:  1.2194100669333159  Accuracy:  0.5628415300546448\n",
            "Epoch:  959  Loss:  1.2200767888694044  Accuracy:  0.5573770491803278\n",
            "Epoch:  960  Loss:  1.2201568937859897  Accuracy:  0.5573770491803278\n",
            "Epoch:  961  Loss:  1.219891120005316  Accuracy:  0.5628415300546448\n",
            "Epoch:  962  Loss:  1.2204634401073093  Accuracy:  0.5628415300546448\n",
            "Epoch:  963  Loss:  1.220869617982661  Accuracy:  0.5573770491803278\n",
            "Epoch:  964  Loss:  1.2205815584778201  Accuracy:  0.5573770491803278\n",
            "Epoch:  965  Loss:  1.220893186941231  Accuracy:  0.5573770491803278\n",
            "Epoch:  966  Loss:  1.2206845593124158  Accuracy:  0.5573770491803278\n",
            "Epoch:  967  Loss:  1.2214075387917032  Accuracy:  0.5573770491803278\n",
            "Epoch:  968  Loss:  1.221142336674346  Accuracy:  0.5628415300546448\n",
            "Epoch:  969  Loss:  1.2217036093522036  Accuracy:  0.5573770491803278\n",
            "Epoch:  970  Loss:  1.2216567185277547  Accuracy:  0.5573770491803278\n",
            "Epoch:  971  Loss:  1.2216266070698363  Accuracy:  0.5573770491803278\n",
            "Epoch:  972  Loss:  1.2215141163012573  Accuracy:  0.5628415300546448\n",
            "Epoch:  973  Loss:  1.2208959372274846  Accuracy:  0.5573770491803278\n",
            "Epoch:  974  Loss:  1.222596998549308  Accuracy:  0.5573770491803278\n",
            "Epoch:  975  Loss:  1.2225519721765306  Accuracy:  0.5573770491803278\n",
            "Epoch:  976  Loss:  1.2225884314073885  Accuracy:  0.5519125683060109\n",
            "Epoch:  977  Loss:  1.2226633191674807  Accuracy:  0.5573770491803278\n",
            "Epoch:  978  Loss:  1.2230008568468078  Accuracy:  0.5573770491803278\n",
            "Epoch:  979  Loss:  1.2234448084295626  Accuracy:  0.5573770491803278\n",
            "Epoch:  980  Loss:  1.2232843334287533  Accuracy:  0.5573770491803278\n",
            "Epoch:  981  Loss:  1.2235066018034022  Accuracy:  0.5573770491803278\n",
            "Epoch:  982  Loss:  1.224024498401593  Accuracy:  0.5573770491803278\n",
            "Epoch:  983  Loss:  1.223787025456042  Accuracy:  0.5628415300546448\n",
            "Epoch:  984  Loss:  1.2233513469779853  Accuracy:  0.5519125683060109\n",
            "Epoch:  985  Loss:  1.2242415819989105  Accuracy:  0.5573770491803278\n",
            "Epoch:  986  Loss:  1.2240352176255547  Accuracy:  0.5573770491803278\n",
            "Epoch:  987  Loss:  1.2248549235570716  Accuracy:  0.5573770491803278\n",
            "Epoch:  988  Loss:  1.2250576332046041  Accuracy:  0.5573770491803278\n",
            "Epoch:  989  Loss:  1.2244187217422267  Accuracy:  0.5519125683060109\n",
            "Epoch:  990  Loss:  1.2243250151107352  Accuracy:  0.5519125683060109\n",
            "Epoch:  991  Loss:  1.2254185267710096  Accuracy:  0.5628415300546448\n",
            "Epoch:  992  Loss:  1.2258361516897882  Accuracy:  0.5573770491803278\n",
            "Epoch:  993  Loss:  1.2256835354724958  Accuracy:  0.5573770491803278\n",
            "Epoch:  994  Loss:  1.2257791433009688  Accuracy:  0.5573770491803278\n",
            "Epoch:  995  Loss:  1.226149104358987  Accuracy:  0.5573770491803278\n",
            "Epoch:  996  Loss:  1.2257835908843522  Accuracy:  0.5519125683060109\n",
            "Epoch:  997  Loss:  1.226978505560389  Accuracy:  0.5573770491803278\n",
            "Epoch:  998  Loss:  1.2263662517758418  Accuracy:  0.5573770491803278\n",
            "Epoch:  999  Loss:  1.2265871702014686  Accuracy:  0.5573770491803278\n"
          ]
        }
      ],
      "source": [
        "model = MLP(0.01, \"relu\", \"mini-batch\", 2, [10, 10])\n",
        "model.train(train_x, train_y, validation_x, validation_y, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdTMsEFaIwbY",
        "outputId": "704646db-de93-44fb-c57b-5b1481595ddd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on test set:  0.6244541484716157\n"
          ]
        }
      ],
      "source": [
        "# make predictions on test set\n",
        "ypred = model.predict(test_x)\n",
        "# calculate accuracy\n",
        "accuracy = np.sum(ypred == np.argmax(test_y, axis=1)) / test_x.shape[0]\n",
        "print(\"Accuracy on test set: \", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8wG8Fr3gIwbZ"
      },
      "outputs": [],
      "source": [
        "test_y = np.argmax(test_y, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJAedyhIIwbZ",
        "outputId": "d74e91b0-94d0-4183-9385-388a211e158b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00         6\n",
            "           2       0.74      0.73      0.74        96\n",
            "           3       0.59      0.67      0.63        99\n",
            "           4       0.37      0.27      0.31        26\n",
            "           5       0.00      0.00      0.00         2\n",
            "\n",
            "    accuracy                           0.62       229\n",
            "   macro avg       0.34      0.33      0.33       229\n",
            "weighted avg       0.61      0.62      0.61       229\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# printing classification report\n",
        "print(classification_report(test_y, ypred, zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnisuW2ZIwbZ"
      },
      "source": [
        "# MLPs for multilabel classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "MTPJv_WhIwbZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "JI10VemqIwbZ",
        "outputId": "38486f7f-191b-4292-f097-e9533606530d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-65544471-b6aa-484f-a9c5-4bbb8f04f1df\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>income</th>\n",
              "      <th>education</th>\n",
              "      <th>married</th>\n",
              "      <th>children</th>\n",
              "      <th>city</th>\n",
              "      <th>occupation</th>\n",
              "      <th>purchase_amount</th>\n",
              "      <th>most bought item</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>45</td>\n",
              "      <td>Male</td>\n",
              "      <td>61271.953359</td>\n",
              "      <td>Master</td>\n",
              "      <td>False</td>\n",
              "      <td>3</td>\n",
              "      <td>Lake Sheila</td>\n",
              "      <td>Doctor</td>\n",
              "      <td>87.697118</td>\n",
              "      <td>monitor</td>\n",
              "      <td>electronics clothing sports</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24</td>\n",
              "      <td>Female</td>\n",
              "      <td>53229.101074</td>\n",
              "      <td>High School</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>Crystalburgh</td>\n",
              "      <td>Businessman</td>\n",
              "      <td>115.135586</td>\n",
              "      <td>lipstick</td>\n",
              "      <td>furniture beauty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>45</td>\n",
              "      <td>Female</td>\n",
              "      <td>30066.046684</td>\n",
              "      <td>Bachelor</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "      <td>Margaretburgh</td>\n",
              "      <td>Engineer</td>\n",
              "      <td>101.694559</td>\n",
              "      <td>biscuits</td>\n",
              "      <td>clothing electronics food sports</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>19</td>\n",
              "      <td>Male</td>\n",
              "      <td>48950.246384</td>\n",
              "      <td>PhD</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>Williamshaven</td>\n",
              "      <td>Lawyer</td>\n",
              "      <td>97.964887</td>\n",
              "      <td>maggi</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>29</td>\n",
              "      <td>Female</td>\n",
              "      <td>44792.627094</td>\n",
              "      <td>Master</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>New Paul</td>\n",
              "      <td>Businessman</td>\n",
              "      <td>86.847281</td>\n",
              "      <td>carpet</td>\n",
              "      <td>home</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-65544471-b6aa-484f-a9c5-4bbb8f04f1df')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-65544471-b6aa-484f-a9c5-4bbb8f04f1df button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-65544471-b6aa-484f-a9c5-4bbb8f04f1df');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a67f61da-a74b-41cf-be7f-6afc70f80c21\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a67f61da-a74b-41cf-be7f-6afc70f80c21')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a67f61da-a74b-41cf-be7f-6afc70f80c21 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   age  gender        income    education  married  children           city  \\\n",
              "0   45    Male  61271.953359       Master    False         3    Lake Sheila   \n",
              "1   24  Female  53229.101074  High School    False         1   Crystalburgh   \n",
              "2   45  Female  30066.046684     Bachelor     True         3  Margaretburgh   \n",
              "3   19    Male  48950.246384          PhD    False         0  Williamshaven   \n",
              "4   29  Female  44792.627094       Master    False         0       New Paul   \n",
              "\n",
              "    occupation  purchase_amount most bought item  \\\n",
              "0       Doctor        87.697118          monitor   \n",
              "1  Businessman       115.135586         lipstick   \n",
              "2     Engineer       101.694559         biscuits   \n",
              "3       Lawyer        97.964887            maggi   \n",
              "4  Businessman        86.847281           carpet   \n",
              "\n",
              "                             labels  \n",
              "0       electronics clothing sports  \n",
              "1                  furniture beauty  \n",
              "2  clothing electronics food sports  \n",
              "3                              food  \n",
              "4                              home  "
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load advertisement.csv\n",
        "advertisement = pd.read_csv('/content/advertisement.csv')\n",
        "advertisement.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "_vPD9FTMIwba",
        "outputId": "25e9aa18-db06-4c01-9d08-162827cc863d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d3ad22b6-524d-4153-8d8d-8ea1c5be707b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>income</th>\n",
              "      <th>education</th>\n",
              "      <th>married</th>\n",
              "      <th>children</th>\n",
              "      <th>city</th>\n",
              "      <th>occupation</th>\n",
              "      <th>purchase_amount</th>\n",
              "      <th>most bought item</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>45</td>\n",
              "      <td>Male</td>\n",
              "      <td>61271.953359</td>\n",
              "      <td>Master</td>\n",
              "      <td>False</td>\n",
              "      <td>3</td>\n",
              "      <td>Lake Sheila</td>\n",
              "      <td>Doctor</td>\n",
              "      <td>87.697118</td>\n",
              "      <td>monitor</td>\n",
              "      <td>[electronics, clothing, sports]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24</td>\n",
              "      <td>Female</td>\n",
              "      <td>53229.101074</td>\n",
              "      <td>High School</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>Crystalburgh</td>\n",
              "      <td>Businessman</td>\n",
              "      <td>115.135586</td>\n",
              "      <td>lipstick</td>\n",
              "      <td>[furniture, beauty]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>45</td>\n",
              "      <td>Female</td>\n",
              "      <td>30066.046684</td>\n",
              "      <td>Bachelor</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "      <td>Margaretburgh</td>\n",
              "      <td>Engineer</td>\n",
              "      <td>101.694559</td>\n",
              "      <td>biscuits</td>\n",
              "      <td>[clothing, electronics, food, sports]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>19</td>\n",
              "      <td>Male</td>\n",
              "      <td>48950.246384</td>\n",
              "      <td>PhD</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>Williamshaven</td>\n",
              "      <td>Lawyer</td>\n",
              "      <td>97.964887</td>\n",
              "      <td>maggi</td>\n",
              "      <td>[food]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>29</td>\n",
              "      <td>Female</td>\n",
              "      <td>44792.627094</td>\n",
              "      <td>Master</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>New Paul</td>\n",
              "      <td>Businessman</td>\n",
              "      <td>86.847281</td>\n",
              "      <td>carpet</td>\n",
              "      <td>[home]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3ad22b6-524d-4153-8d8d-8ea1c5be707b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d3ad22b6-524d-4153-8d8d-8ea1c5be707b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d3ad22b6-524d-4153-8d8d-8ea1c5be707b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-40e98cfb-bb17-4487-852c-1b9dbae24157\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-40e98cfb-bb17-4487-852c-1b9dbae24157')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-40e98cfb-bb17-4487-852c-1b9dbae24157 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   age  gender        income    education  married  children           city  \\\n",
              "0   45    Male  61271.953359       Master    False         3    Lake Sheila   \n",
              "1   24  Female  53229.101074  High School    False         1   Crystalburgh   \n",
              "2   45  Female  30066.046684     Bachelor     True         3  Margaretburgh   \n",
              "3   19    Male  48950.246384          PhD    False         0  Williamshaven   \n",
              "4   29  Female  44792.627094       Master    False         0       New Paul   \n",
              "\n",
              "    occupation  purchase_amount most bought item  \\\n",
              "0       Doctor        87.697118          monitor   \n",
              "1  Businessman       115.135586         lipstick   \n",
              "2     Engineer       101.694559         biscuits   \n",
              "3       Lawyer        97.964887            maggi   \n",
              "4  Businessman        86.847281           carpet   \n",
              "\n",
              "                                  labels  \n",
              "0        [electronics, clothing, sports]  \n",
              "1                    [furniture, beauty]  \n",
              "2  [clothing, electronics, food, sports]  \n",
              "3                                 [food]  \n",
              "4                                 [home]  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "advertisement['labels'] = advertisement['labels'].str.split()\n",
        "advertisement.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUrq_AuoIwba",
        "outputId": "b4989b85-b294-400f-aacb-91288d054ca7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['electronics', 'clothing', 'sports', 'furniture', 'beauty', 'food',\n",
              "       'home', 'books'], dtype=object)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# print the number of unique labels\n",
        "advertisement['labels'].explode().unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "9_-S8UuWIwba",
        "outputId": "a1063565-6091-4048-8991-cd301d56dc9e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d23564cd-5d04-42bd-82e8-64d1307933a0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>income</th>\n",
              "      <th>education</th>\n",
              "      <th>married</th>\n",
              "      <th>children</th>\n",
              "      <th>city</th>\n",
              "      <th>occupation</th>\n",
              "      <th>purchase_amount</th>\n",
              "      <th>most bought item</th>\n",
              "      <th>...</th>\n",
              "      <th>occupation_Businessman</th>\n",
              "      <th>occupation_Doctor</th>\n",
              "      <th>occupation_Engineer</th>\n",
              "      <th>occupation_HR</th>\n",
              "      <th>occupation_Housewife</th>\n",
              "      <th>occupation_Lawyer</th>\n",
              "      <th>occupation_Retired</th>\n",
              "      <th>occupation_Salesman</th>\n",
              "      <th>occupation_Scientist</th>\n",
              "      <th>occupation_Unemployed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>45</td>\n",
              "      <td>Male</td>\n",
              "      <td>61271.953359</td>\n",
              "      <td>Master</td>\n",
              "      <td>False</td>\n",
              "      <td>3</td>\n",
              "      <td>Lake Sheila</td>\n",
              "      <td>Doctor</td>\n",
              "      <td>87.697118</td>\n",
              "      <td>monitor</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24</td>\n",
              "      <td>Female</td>\n",
              "      <td>53229.101074</td>\n",
              "      <td>High School</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>Crystalburgh</td>\n",
              "      <td>Businessman</td>\n",
              "      <td>115.135586</td>\n",
              "      <td>lipstick</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>45</td>\n",
              "      <td>Female</td>\n",
              "      <td>30066.046684</td>\n",
              "      <td>Bachelor</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "      <td>Margaretburgh</td>\n",
              "      <td>Engineer</td>\n",
              "      <td>101.694559</td>\n",
              "      <td>biscuits</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>19</td>\n",
              "      <td>Male</td>\n",
              "      <td>48950.246384</td>\n",
              "      <td>PhD</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>Williamshaven</td>\n",
              "      <td>Lawyer</td>\n",
              "      <td>97.964887</td>\n",
              "      <td>maggi</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>29</td>\n",
              "      <td>Female</td>\n",
              "      <td>44792.627094</td>\n",
              "      <td>Master</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>New Paul</td>\n",
              "      <td>Businessman</td>\n",
              "      <td>86.847281</td>\n",
              "      <td>carpet</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d23564cd-5d04-42bd-82e8-64d1307933a0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d23564cd-5d04-42bd-82e8-64d1307933a0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d23564cd-5d04-42bd-82e8-64d1307933a0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-beb649b7-48af-4bb9-9c5b-1990bfd739ca\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-beb649b7-48af-4bb9-9c5b-1990bfd739ca')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-beb649b7-48af-4bb9-9c5b-1990bfd739ca button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   age  gender        income    education  married  children           city  \\\n",
              "0   45    Male  61271.953359       Master    False         3    Lake Sheila   \n",
              "1   24  Female  53229.101074  High School    False         1   Crystalburgh   \n",
              "2   45  Female  30066.046684     Bachelor     True         3  Margaretburgh   \n",
              "3   19    Male  48950.246384          PhD    False         0  Williamshaven   \n",
              "4   29  Female  44792.627094       Master    False         0       New Paul   \n",
              "\n",
              "    occupation  purchase_amount most bought item  ... occupation_Businessman  \\\n",
              "0       Doctor        87.697118          monitor  ...                      0   \n",
              "1  Businessman       115.135586         lipstick  ...                      1   \n",
              "2     Engineer       101.694559         biscuits  ...                      0   \n",
              "3       Lawyer        97.964887            maggi  ...                      0   \n",
              "4  Businessman        86.847281           carpet  ...                      1   \n",
              "\n",
              "   occupation_Doctor  occupation_Engineer  occupation_HR  \\\n",
              "0                  1                    0              0   \n",
              "1                  0                    0              0   \n",
              "2                  0                    1              0   \n",
              "3                  0                    0              0   \n",
              "4                  0                    0              0   \n",
              "\n",
              "   occupation_Housewife  occupation_Lawyer  occupation_Retired  \\\n",
              "0                     0                  0                   0   \n",
              "1                     0                  0                   0   \n",
              "2                     0                  0                   0   \n",
              "3                     0                  1                   0   \n",
              "4                     0                  0                   0   \n",
              "\n",
              "   occupation_Salesman  occupation_Scientist  occupation_Unemployed  \n",
              "0                    0                     0                      0  \n",
              "1                    0                     0                      0  \n",
              "2                    0                     0                      0  \n",
              "3                    0                     0                      0  \n",
              "4                    0                     0                      0  \n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# onehot gender\n",
        "gender = pd.get_dummies(advertisement['gender'], prefix='gender')\n",
        "advertisement = pd.concat([advertisement, gender], axis=1)\n",
        "occupation = pd.get_dummies(advertisement['occupation'], prefix='occupation')\n",
        "advertisement = pd.concat([advertisement, occupation], axis=1)\n",
        "advertisement.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "WAAykiQcIwba",
        "outputId": "3cf649f5-88a8-4298-91b5-7b08185f6ae6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d620e5fa-fae5-402e-ae6b-80bca119520a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>income</th>\n",
              "      <th>education</th>\n",
              "      <th>married</th>\n",
              "      <th>children</th>\n",
              "      <th>city</th>\n",
              "      <th>occupation</th>\n",
              "      <th>purchase_amount</th>\n",
              "      <th>most bought item</th>\n",
              "      <th>...</th>\n",
              "      <th>occupation_Scientist</th>\n",
              "      <th>occupation_Unemployed</th>\n",
              "      <th>beauty</th>\n",
              "      <th>books</th>\n",
              "      <th>clothing</th>\n",
              "      <th>electronics</th>\n",
              "      <th>food</th>\n",
              "      <th>furniture</th>\n",
              "      <th>home</th>\n",
              "      <th>sports</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>45</td>\n",
              "      <td>Male</td>\n",
              "      <td>61271.953359</td>\n",
              "      <td>Master</td>\n",
              "      <td>False</td>\n",
              "      <td>3</td>\n",
              "      <td>Lake Sheila</td>\n",
              "      <td>Doctor</td>\n",
              "      <td>87.697118</td>\n",
              "      <td>monitor</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24</td>\n",
              "      <td>Female</td>\n",
              "      <td>53229.101074</td>\n",
              "      <td>High School</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>Crystalburgh</td>\n",
              "      <td>Businessman</td>\n",
              "      <td>115.135586</td>\n",
              "      <td>lipstick</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>45</td>\n",
              "      <td>Female</td>\n",
              "      <td>30066.046684</td>\n",
              "      <td>Bachelor</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "      <td>Margaretburgh</td>\n",
              "      <td>Engineer</td>\n",
              "      <td>101.694559</td>\n",
              "      <td>biscuits</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>19</td>\n",
              "      <td>Male</td>\n",
              "      <td>48950.246384</td>\n",
              "      <td>PhD</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>Williamshaven</td>\n",
              "      <td>Lawyer</td>\n",
              "      <td>97.964887</td>\n",
              "      <td>maggi</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>29</td>\n",
              "      <td>Female</td>\n",
              "      <td>44792.627094</td>\n",
              "      <td>Master</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>New Paul</td>\n",
              "      <td>Businessman</td>\n",
              "      <td>86.847281</td>\n",
              "      <td>carpet</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d620e5fa-fae5-402e-ae6b-80bca119520a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d620e5fa-fae5-402e-ae6b-80bca119520a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d620e5fa-fae5-402e-ae6b-80bca119520a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f721b9c3-6d8b-4c86-b12b-fc4afa832516\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f721b9c3-6d8b-4c86-b12b-fc4afa832516')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f721b9c3-6d8b-4c86-b12b-fc4afa832516 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   age  gender        income    education  married  children           city  \\\n",
              "0   45    Male  61271.953359       Master    False         3    Lake Sheila   \n",
              "1   24  Female  53229.101074  High School    False         1   Crystalburgh   \n",
              "2   45  Female  30066.046684     Bachelor     True         3  Margaretburgh   \n",
              "3   19    Male  48950.246384          PhD    False         0  Williamshaven   \n",
              "4   29  Female  44792.627094       Master    False         0       New Paul   \n",
              "\n",
              "    occupation  purchase_amount most bought item  ... occupation_Scientist  \\\n",
              "0       Doctor        87.697118          monitor  ...                    0   \n",
              "1  Businessman       115.135586         lipstick  ...                    0   \n",
              "2     Engineer       101.694559         biscuits  ...                    0   \n",
              "3       Lawyer        97.964887            maggi  ...                    0   \n",
              "4  Businessman        86.847281           carpet  ...                    0   \n",
              "\n",
              "   occupation_Unemployed  beauty  books  clothing  electronics  food  \\\n",
              "0                      0       0      0         1            1     0   \n",
              "1                      0       1      0         0            0     0   \n",
              "2                      0       0      0         1            1     1   \n",
              "3                      0       0      0         0            0     1   \n",
              "4                      0       0      0         0            0     0   \n",
              "\n",
              "   furniture  home  sports  \n",
              "0          0     0       1  \n",
              "1          1     0       0  \n",
              "2          0     0       1  \n",
              "3          0     0       0  \n",
              "4          0     1       0  \n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# binarize the labels column\n",
        "binarizer = MultiLabelBinarizer()\n",
        "vecs = binarizer.fit_transform(advertisement['labels'])\n",
        "binarized_df = pd.DataFrame(vecs, columns=binarizer.classes_)\n",
        "advertisement = pd.concat([advertisement, binarized_df], axis=1)\n",
        "advertisement.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "L5fNUt7kIwba",
        "outputId": "76888b37-0949-42fc-fdf7-898fa8b19ef0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6d8d506a-213d-4dae-b5a3-02c718add82b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>income</th>\n",
              "      <th>education</th>\n",
              "      <th>married</th>\n",
              "      <th>children</th>\n",
              "      <th>purchase_amount</th>\n",
              "      <th>gender_Female</th>\n",
              "      <th>gender_Male</th>\n",
              "      <th>occupation_Artist</th>\n",
              "      <th>occupation_Businessman</th>\n",
              "      <th>...</th>\n",
              "      <th>occupation_Scientist</th>\n",
              "      <th>occupation_Unemployed</th>\n",
              "      <th>beauty</th>\n",
              "      <th>books</th>\n",
              "      <th>clothing</th>\n",
              "      <th>electronics</th>\n",
              "      <th>food</th>\n",
              "      <th>furniture</th>\n",
              "      <th>home</th>\n",
              "      <th>sports</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>45</td>\n",
              "      <td>61271.953359</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>87.697118</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24</td>\n",
              "      <td>53229.101074</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>115.135586</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>45</td>\n",
              "      <td>30066.046684</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>101.694559</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>19</td>\n",
              "      <td>48950.246384</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>97.964887</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>29</td>\n",
              "      <td>44792.627094</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>86.847281</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>20</td>\n",
              "      <td>51266.767047</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>95.145103</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>27</td>\n",
              "      <td>29578.136416</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>69.022842</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>20</td>\n",
              "      <td>35325.309005</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>110.564517</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>51</td>\n",
              "      <td>40232.564356</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>107.835490</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>19</td>\n",
              "      <td>48053.583882</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>108.854898</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 27 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d8d506a-213d-4dae-b5a3-02c718add82b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6d8d506a-213d-4dae-b5a3-02c718add82b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6d8d506a-213d-4dae-b5a3-02c718add82b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b6af858a-813c-4980-9604-0b1f990cc135\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b6af858a-813c-4980-9604-0b1f990cc135')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b6af858a-813c-4980-9604-0b1f990cc135 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   age        income  education  married  children  purchase_amount  \\\n",
              "0   45  61271.953359          3        0         3        87.697118   \n",
              "1   24  53229.101074          1        0         1       115.135586   \n",
              "2   45  30066.046684          2        1         3       101.694559   \n",
              "3   19  48950.246384          4        0         0        97.964887   \n",
              "4   29  44792.627094          3        0         0        86.847281   \n",
              "5   20  51266.767047          3        1         3        95.145103   \n",
              "6   27  29578.136416          2        1         3        69.022842   \n",
              "7   20  35325.309005          4        0         3       110.564517   \n",
              "8   51  40232.564356          4        1         0       107.835490   \n",
              "9   19  48053.583882          4        1         0       108.854898   \n",
              "\n",
              "   gender_Female  gender_Male  occupation_Artist  occupation_Businessman  ...  \\\n",
              "0              0            1                  0                       0  ...   \n",
              "1              1            0                  0                       1  ...   \n",
              "2              1            0                  0                       0  ...   \n",
              "3              0            1                  0                       0  ...   \n",
              "4              1            0                  0                       1  ...   \n",
              "5              1            0                  0                       0  ...   \n",
              "6              0            1                  0                       0  ...   \n",
              "7              1            0                  0                       0  ...   \n",
              "8              0            1                  0                       0  ...   \n",
              "9              0            1                  1                       0  ...   \n",
              "\n",
              "   occupation_Scientist  occupation_Unemployed  beauty  books  clothing  \\\n",
              "0                     0                      0       0      0         1   \n",
              "1                     0                      0       1      0         0   \n",
              "2                     0                      0       0      0         1   \n",
              "3                     0                      0       0      0         0   \n",
              "4                     0                      0       0      0         0   \n",
              "5                     1                      0       0      1         0   \n",
              "6                     0                      0       1      0         1   \n",
              "7                     0                      0       1      1         0   \n",
              "8                     0                      0       0      0         0   \n",
              "9                     0                      0       0      0         0   \n",
              "\n",
              "   electronics  food  furniture  home  sports  \n",
              "0            1     0          0     0       1  \n",
              "1            0     0          1     0       0  \n",
              "2            1     1          0     0       1  \n",
              "3            0     1          0     0       0  \n",
              "4            0     0          0     1       0  \n",
              "5            1     0          0     0       1  \n",
              "6            0     0          1     0       0  \n",
              "7            0     0          0     0       0  \n",
              "8            1     1          0     1       1  \n",
              "9            0     1          1     0       0  \n",
              "\n",
              "[10 rows x 27 columns]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# drop labels\n",
        "advertisement.drop(['labels', 'most bought item', 'city', 'gender', 'occupation'], axis=1, inplace=True)\n",
        "advertisement['married'] = advertisement['married'].apply(lambda x: 1 if x == True else 0)\n",
        "advertisement['education'] = advertisement['education'].apply(lambda x: 1 if x == 'High School' else 2 if x == 'Bachelor' else 3 if x == 'Master' else 4)\n",
        "# onehot education and gender\n",
        "advertisement.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-1pEzr6Iwba",
        "outputId": "5c426973-5c9b-4637-8c70-db177e5430c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((640, 27), (200, 27), (160, 27))"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = advertisement.iloc[:, :-8].values\n",
        "data = (data - data.mean(axis=0)) / data.std(axis=0)\n",
        "advertisement.iloc[:, :-8] = data\n",
        "\n",
        "train, test = train_test_split(advertisement, test_size=0.2, random_state=42)\n",
        "train, validation = train_test_split(train, test_size=0.2, random_state=42)\n",
        "train_x = train.iloc[:, :-8]\n",
        "train_y = train.iloc[:, -8:]\n",
        "test_x = test.iloc[:, :-8]\n",
        "test_y = test.iloc[:, -8:]\n",
        "validation_x = validation.iloc[:, :-8]\n",
        "validation_y = validation.iloc[:, -8:]\n",
        "# convert to numpy array\n",
        "train_x = np.array(train_x)\n",
        "train_y = np.array(train_y)\n",
        "test_x = np.array(test_x)\n",
        "test_y = np.array(test_y)\n",
        "validation_x = np.array(validation_x)\n",
        "validation_y = np.array(validation_y)\n",
        "train.shape, test.shape, validation.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "qyPQvfJIIwbb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, log_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "VPxHJ_WmIwbb"
      },
      "outputs": [],
      "source": [
        "class MLP_classification():\n",
        "    def __init__(self, learning_rate, activation_function, optimizers, hidden_layers, neurons):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizers = optimizers\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.neurons = neurons\n",
        "        self.inputlayersize = 0\n",
        "        self.outputlayersize = 0\n",
        "        self.X = None\n",
        "        self.activationfunction = activation_function\n",
        "\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        if activation_function == \"sigmoid\":\n",
        "            self.activation_function = sigmoid\n",
        "            self.backprop_function = sigmoidprime\n",
        "        if activation_function == \"tanh\":\n",
        "            self.activation_function = tanh\n",
        "            self.backprop_function = tanhprime\n",
        "        if activation_function == \"relu\":\n",
        "            self.activation_function = relu\n",
        "            self.backprop_function = reluprime\n",
        "\n",
        "    def weightsbiases(self):\n",
        "        # initialize weights, biases\n",
        "        self.inputlayersize = self.X.shape[1]\n",
        "        self.outputlayersize = self.y.shape[1]\n",
        "        self.weights.append(np.random.randn(self.inputlayersize, self.neurons[0]))\n",
        "        self.biases.append(np.random.randn(self.neurons[0]))\n",
        "        for i in range(0, self.hidden_layers - 1):\n",
        "            self.weights.append(np.random.randn(self.neurons[i], self.neurons[i+1]))\n",
        "            self.biases.append(np.random.randn(self.neurons[i + 1]))\n",
        "        self.weights.append(np.random.randn(self.neurons[-1], self.outputlayersize))\n",
        "        self.biases.append(np.random.randn(self.outputlayersize))\n",
        "        # weights correctly initialized\n",
        "        # biases correctly initialized\n",
        "\n",
        "        self.z = [None]*len(self.weights)\n",
        "        self.a = [None]*len(self.weights)\n",
        "\n",
        "    def forward(self, X):\n",
        "        z = np.dot(X, self.weights[0]) + self.biases[0]\n",
        "        a = self.activation_function(z)\n",
        "        self.z[0] = z\n",
        "        self.a[0] = a\n",
        "        for i in range(1, len(self.weights) - 1):\n",
        "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
        "            a = self.activation_function(z)\n",
        "            self.z[i] = z\n",
        "            self.a[i] = a\n",
        "        z = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
        "        a = sigmoid(z)\n",
        "        self.z[-1] = z\n",
        "        self.a[-1] = a\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        # initialize weight gradients\n",
        "        weight_gradients = [np.zeros_like(w) for w in self.weights]\n",
        "        bias_gradients = [np.zeros_like(b) for b in self.biases]\n",
        "        # calculate gradients\n",
        "        error = (self.a[-1] - y)*sigmoidprime(self.z[-1])\n",
        "        weight_gradients[-1] = np.dot(self.a[-2].T, error)\n",
        "        # print(error)\n",
        "        bias_gradients[-1] = np.sum(error, axis=0)\n",
        "        # print(bias_gradients[-1])\n",
        "\n",
        "        for i in range(len(self.weights)-2, 0, -1):\n",
        "            error = np.dot(error, self.weights[i+1].T) * self.backprop_function(self.z[i])\n",
        "            weight_gradients[i] = np.dot(self.a[i-1].T, error)\n",
        "            bias_gradients[i] = np.sum(error,axis=0)\n",
        "\n",
        "        error = np.dot(error, self.weights[1].T) * self.backprop_function(self.z[0])\n",
        "        weight_gradients[0] = np.dot(X.T, error)\n",
        "        bias_gradients[0] = np.sum(error,axis=0)\n",
        "\n",
        "        # update weights and biases\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= self.learning_rate * weight_gradients[i] / X.shape[0]\n",
        "            self.biases[i] -= self.learning_rate * bias_gradients[i] / X.shape[0]\n",
        "\n",
        "    def lossandaccuracy(self, X, y):\n",
        "        self.forward(X)\n",
        "\n",
        "        num_samples = y.shape[0]\n",
        "        num_classes = self.outputlayersize\n",
        "\n",
        "        # Initialize arrays for per-prediction loss and Hamming distance\n",
        "        loss_per_sample = np.zeros(num_samples)\n",
        "        hamming_distance_per_sample = np.zeros(num_samples)\n",
        "\n",
        "        for sample_idx in range(num_samples):\n",
        "            sample_y = y[sample_idx]\n",
        "            sample_prediction = self.a[-1][sample_idx]\n",
        "\n",
        "            # Binary cross-entropy loss for the current prediction\n",
        "            loss_per_sample[sample_idx] = -np.sum(sample_y * np.log(sample_prediction) + (1 - sample_y) * np.log(1 - sample_prediction))\n",
        "\n",
        "            # Hamming distance for the current prediction\n",
        "            binary_predictions = (sample_prediction > 0.5).astype(int)\n",
        "            hamming_distance_per_sample[sample_idx] = np.sum(binary_predictions != sample_y)\n",
        "\n",
        "        # Calculate overall Hamming distance accuracy\n",
        "        overall_hamming_accuracy = 1 - (np.sum(hamming_distance_per_sample) / (num_samples * num_classes))\n",
        "        overall_loss = np.mean(loss_per_sample)\n",
        "\n",
        "        return overall_loss, overall_hamming_accuracy\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, X, y, val_X, val_y, epochs):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.weightsbiases()\n",
        "\n",
        "\n",
        "        # pick optimizer\n",
        "        if self.optimizers == \"sgd\":\n",
        "            batch_size = 1\n",
        "        if self.optimizers == \"mini-batch\":\n",
        "            batch_size = 32\n",
        "        if self.optimizers == \"batch\":\n",
        "            batch_size = self.X.shape[0]\n",
        "\n",
        "        # initialize loss\n",
        "        loss_val = [None]*epochs\n",
        "        loss_train = [None]*epochs\n",
        "        accuracy_val = [None]*epochs\n",
        "        accuracy_train = [None]*epochs\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, self.X.shape[0], batch_size):\n",
        "                batch_x = self.X[i:i+batch_size]\n",
        "                batch_y = self.y[i:i+batch_size]\n",
        "                self.forward(batch_x)\n",
        "                self.backward(batch_x, batch_y)\n",
        "            # calculate loss, accuracy\n",
        "            loss_val[epoch], accuracy_val[epoch] = self.lossandaccuracy(val_X, val_y)\n",
        "            loss_train[epoch], accuracy_train[epoch] = self.lossandaccuracy(self.X, self.y)\n",
        "            print(\"Epoch: \", epoch, \" Loss: \", loss_val[epoch], \" Accuracy: \", accuracy_val[epoch])\n",
        "            # wandb.log({\"loss\": loss_val[epoch], \"accuracy\": accuracy_val[epoch]})\n",
        "\n",
        "    def predict(self, X):\n",
        "        print(X.shape)\n",
        "        self.forward(X)\n",
        "        # print(prediction)\n",
        "        binary_predictions = (self.a[-1] > 0.5).astype(int)\n",
        "        return binary_predictions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJgP87o_rjy4"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16BxYZdxrtB0",
        "outputId": "969817a8-3949-491d-b857-ef192a2ef185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: x803ziei\n",
            "Sweep URL: https://wandb.ai/sanika-damle/assignment-3%20part-2%20multilabelclassification/sweeps/x803ziei\n"
          ]
        }
      ],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"assignment-3 part-2 multilabelclassification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIRhxKKcr8af"
      },
      "outputs": [],
      "source": [
        "def mcc():\n",
        "    with wandb.init() as run:\n",
        "        lr = wandb.config.learning_rate\n",
        "        optimizers = wandb.config.optimizers\n",
        "        hidden_layers = wandb.config.neurons[0]\n",
        "        neurons = wandb.config.neurons[1]\n",
        "        activation_function = wandb.config.activation_functions\n",
        "        epochs = wandb.config.epochs\n",
        "        modelclass = MLP_classification(lr, activation_function, optimizers, hidden_layers, neurons)\n",
        "        modelclass.train(train_x, train_y, validation_x, validation_y, epochs)\n",
        "        ypred = modelclass.predict(test_x)\n",
        "\n",
        "\n",
        "        f1 = f1_score(test_y, ypred, average='macro')\n",
        "        precision = precision_score(test_y, ypred, average='macro')\n",
        "        recall = recall_score(test_y, ypred, average='macro')\n",
        "        accuracy = accuracy_score(test_y, ypred)\n",
        "        wandb.log({\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall})\n",
        "\n",
        "wandb.agent(sweep_id, function = mcc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHkF52iwIwbb",
        "outputId": "96a3e450-f775-4cf8-cbda-97844db522a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  0  Loss:  9.00679207687081  Accuracy:  0.53359375\n",
            "Epoch:  1  Loss:  8.993232108516759  Accuracy:  0.53203125\n",
            "Epoch:  2  Loss:  8.979706944732172  Accuracy:  0.53359375\n",
            "Epoch:  3  Loss:  8.966217041314286  Accuracy:  0.5328125\n",
            "Epoch:  4  Loss:  8.952762777904548  Accuracy:  0.53359375\n",
            "Epoch:  5  Loss:  8.939344467705109  Accuracy:  0.53359375\n",
            "Epoch:  6  Loss:  8.925962365744827  Accuracy:  0.53359375\n",
            "Epoch:  7  Loss:  8.912616675847763  Accuracy:  0.53515625\n",
            "Epoch:  8  Loss:  8.899307556537646  Accuracy:  0.53671875\n",
            "Epoch:  9  Loss:  8.886035126133715  Accuracy:  0.53671875\n",
            "Epoch:  10  Loss:  8.872799467275227  Accuracy:  0.5375\n",
            "Epoch:  11  Loss:  8.85960063106944  Accuracy:  0.53671875\n",
            "Epoch:  12  Loss:  8.8464386410034  Accuracy:  0.53671875\n",
            "Epoch:  13  Loss:  8.833313496703187  Accuracy:  0.53828125\n",
            "Epoch:  14  Loss:  8.82022517757128  Accuracy:  0.53671875\n",
            "Epoch:  15  Loss:  8.807173646287788  Accuracy:  0.53671875\n",
            "Epoch:  16  Loss:  8.7941588521265  Accuracy:  0.53671875\n",
            "Epoch:  17  Loss:  8.781180734012862  Accuracy:  0.53828125\n",
            "Epoch:  18  Loss:  8.768239223238115  Accuracy:  0.53828125\n",
            "Epoch:  19  Loss:  8.755334245740977  Accuracy:  0.5375\n",
            "Epoch:  20  Loss:  8.742465723874512  Accuracy:  0.5390625\n",
            "Epoch:  21  Loss:  8.729633577589432  Accuracy:  0.5390625\n",
            "Epoch:  22  Loss:  8.716837724984696  Accuracy:  0.5390625\n",
            "Epoch:  23  Loss:  8.70407808220004  Accuracy:  0.53984375\n",
            "Epoch:  24  Loss:  8.691354562651375  Accuracy:  0.54140625\n",
            "Epoch:  25  Loss:  8.67866707563703  Accuracy:  0.54140625\n",
            "Epoch:  26  Loss:  8.66601552436917  Accuracy:  0.54140625\n",
            "Epoch:  27  Loss:  8.653399803508913  Accuracy:  0.540625\n",
            "Epoch:  28  Loss:  8.640819796304307  Accuracy:  0.53984375\n",
            "Epoch:  29  Loss:  8.628275371446978  Accuracy:  0.5390625\n",
            "Epoch:  30  Loss:  8.615766379774664  Accuracy:  0.5390625\n",
            "Epoch:  31  Loss:  8.603292650953222  Accuracy:  0.5390625\n",
            "Epoch:  32  Loss:  8.590853990272741  Accuracy:  0.53984375\n",
            "Epoch:  33  Loss:  8.578450175688266  Accuracy:  0.53984375\n",
            "Epoch:  34  Loss:  8.566080955226933  Accuracy:  0.540625\n",
            "Epoch:  35  Loss:  8.553746044870447  Accuracy:  0.540625\n",
            "Epoch:  36  Loss:  8.541445127005417  Accuracy:  0.540625\n",
            "Epoch:  37  Loss:  8.529177849514946  Accuracy:  0.54140625\n",
            "Epoch:  38  Loss:  8.516943825563725  Accuracy:  0.540625\n",
            "Epoch:  39  Loss:  8.504742634106464  Accuracy:  0.54140625\n",
            "Epoch:  40  Loss:  8.492573821126822  Accuracy:  0.5421875\n",
            "Epoch:  41  Loss:  8.480436901591705  Accuracy:  0.5421875\n",
            "Epoch:  42  Loss:  8.468331362084834  Accuracy:  0.5421875\n",
            "Epoch:  43  Loss:  8.45625666406455  Accuracy:  0.5421875\n",
            "Epoch:  44  Loss:  8.444212247674848  Accuracy:  0.5421875\n",
            "Epoch:  45  Loss:  8.432197536025527  Accuracy:  0.5421875\n",
            "Epoch:  46  Loss:  8.4202119398484  Accuracy:  0.54296875\n",
            "Epoch:  47  Loss:  8.4082548624309  Accuracy:  0.54375\n",
            "Epoch:  48  Loss:  8.39632570472675  Accuracy:  0.540625\n",
            "Epoch:  49  Loss:  8.384423870545074  Accuracy:  0.540625\n",
            "Epoch:  50  Loss:  8.372548771723896  Accuracy:  0.5421875\n",
            "Epoch:  51  Loss:  8.360699833201027  Accuracy:  0.54296875\n",
            "Epoch:  52  Loss:  8.34887649790398  Accuracy:  0.54296875\n",
            "Epoch:  53  Loss:  8.337078231390354  Accuracy:  0.54296875\n",
            "Epoch:  54  Loss:  8.325304526180243  Accuracy:  0.54375\n",
            "Epoch:  55  Loss:  8.313554905732321  Accuracy:  0.54375\n",
            "Epoch:  56  Loss:  8.301828928024804  Accuracy:  0.54296875\n",
            "Epoch:  57  Loss:  8.290126188711117  Accuracy:  0.54375\n",
            "Epoch:  58  Loss:  8.278446323828007  Accuracy:  0.54375\n",
            "Epoch:  59  Loss:  8.26678901204051  Accuracy:  0.54375\n",
            "Epoch:  60  Loss:  8.255153976414073  Accuracy:  0.54375\n",
            "Epoch:  61  Loss:  8.243540985709226  Accuracy:  0.54375\n",
            "Epoch:  62  Loss:  8.231949855198732  Accuracy:  0.54453125\n",
            "Epoch:  63  Loss:  8.220380447011257  Accuracy:  0.54375\n",
            "Epoch:  64  Loss:  8.208832670009631  Accuracy:  0.5453125\n",
            "Epoch:  65  Loss:  8.19730647921574  Accuracy:  0.54609375\n",
            "Epoch:  66  Loss:  8.185801874798358  Accuracy:  0.54609375\n",
            "Epoch:  67  Loss:  8.17431890064443  Accuracy:  0.54609375\n",
            "Epoch:  68  Loss:  8.1628576425391  Accuracy:  0.54609375\n",
            "Epoch:  69  Loss:  8.15141822598448  Accuracy:  0.546875\n",
            "Epoch:  70  Loss:  8.140000813692009  Accuracy:  0.546875\n",
            "Epoch:  71  Loss:  8.128605602788095  Accuracy:  0.54609375\n",
            "Epoch:  72  Loss:  8.11723282177722  Accuracy:  0.54609375\n",
            "Epoch:  73  Loss:  8.105882727310753  Accuracy:  0.54609375\n",
            "Epoch:  74  Loss:  8.094555600813171  Accuracy:  0.54765625\n",
            "Epoch:  75  Loss:  8.083251745019917  Accuracy:  0.5484375\n",
            "Epoch:  76  Loss:  8.071971480482679  Accuracy:  0.54921875\n",
            "Epoch:  77  Loss:  8.060715142098312  Accuracy:  0.5484375\n",
            "Epoch:  78  Loss:  8.049483075716783  Accuracy:  0.546875\n",
            "Epoch:  79  Loss:  8.038275634881433  Accuracy:  0.546875\n",
            "Epoch:  80  Loss:  8.027093177751535  Accuracy:  0.5453125\n",
            "Epoch:  81  Loss:  8.015936064252589  Accuracy:  0.54609375\n",
            "Epoch:  82  Loss:  8.004804653494116  Accuracy:  0.5453125\n",
            "Epoch:  83  Loss:  7.993699301488287  Accuracy:  0.54609375\n",
            "Epoch:  84  Loss:  7.982620359195272  Accuracy:  0.54609375\n",
            "Epoch:  85  Loss:  7.9715681709134785  Accuracy:  0.54609375\n",
            "Epoch:  86  Loss:  7.960543073024738  Accuracy:  0.54609375\n",
            "Epoch:  87  Loss:  7.949545393096317  Accuracy:  0.54609375\n",
            "Epoch:  88  Loss:  7.9385754493338165  Accuracy:  0.54609375\n",
            "Epoch:  89  Loss:  7.927633550371604  Accuracy:  0.54609375\n",
            "Epoch:  90  Loss:  7.916719995380684  Accuracy:  0.546875\n",
            "Epoch:  91  Loss:  7.9058350744681745  Accuracy:  0.54609375\n",
            "Epoch:  92  Loss:  7.894979069337751  Accuracy:  0.54609375\n",
            "Epoch:  93  Loss:  7.884152254176783  Accuracy:  0.5453125\n",
            "Epoch:  94  Loss:  7.87335489673344  Accuracy:  0.5453125\n",
            "Epoch:  95  Loss:  7.86258725954566  Accuracy:  0.546875\n",
            "Epoch:  96  Loss:  7.851849601283698  Accuracy:  0.54765625\n",
            "Epoch:  97  Loss:  7.841142178168605  Accuracy:  0.54765625\n",
            "Epoch:  98  Loss:  7.830465245430609  Accuracy:  0.5484375\n",
            "Epoch:  99  Loss:  7.8198190587735805  Accuracy:  0.5484375\n",
            "Epoch:  100  Loss:  7.809203875814477  Accuracy:  0.54765625\n",
            "Epoch:  101  Loss:  7.798619957469799  Accuracy:  0.54765625\n",
            "Epoch:  102  Loss:  7.788067569264443  Accuracy:  0.54765625\n",
            "Epoch:  103  Loss:  7.777546982541591  Accuracy:  0.5484375\n",
            "Epoch:  104  Loss:  7.767058475555906  Accuracy:  0.5484375\n",
            "Epoch:  105  Loss:  7.756602334435345  Accuracy:  0.54765625\n",
            "Epoch:  106  Loss:  7.7461788540002345  Accuracy:  0.5484375\n",
            "Epoch:  107  Loss:  7.73578833843113  Accuracy:  0.54921875\n",
            "Epoch:  108  Loss:  7.725431101779767  Accuracy:  0.5515625\n",
            "Epoch:  109  Loss:  7.715107468319988  Accuracy:  0.55078125\n",
            "Epoch:  110  Loss:  7.704817772737831  Accuracy:  0.5515625\n",
            "Epoch:  111  Loss:  7.694562360162246  Accuracy:  0.553125\n",
            "Epoch:  112  Loss:  7.684341586039743  Accuracy:  0.55234375\n",
            "Epoch:  113  Loss:  7.674155815858259  Accuracy:  0.553125\n",
            "Epoch:  114  Loss:  7.66400542472706  Accuracy:  0.55390625\n",
            "Epoch:  115  Loss:  7.653890796821125  Accuracy:  0.5546875\n",
            "Epoch:  116  Loss:  7.643812324699735  Accuracy:  0.55625\n",
            "Epoch:  117  Loss:  7.633770408510306  Accuracy:  0.55546875\n",
            "Epoch:  118  Loss:  7.623765455089503  Accuracy:  0.5546875\n",
            "Epoch:  119  Loss:  7.613797876974627  Accuracy:  0.55625\n",
            "Epoch:  120  Loss:  7.603868091339038  Accuracy:  0.5578125\n",
            "Epoch:  121  Loss:  7.593976518865901  Accuracy:  0.5578125\n",
            "Epoch:  122  Loss:  7.584123582574996  Accuracy:  0.5578125\n",
            "Epoch:  123  Loss:  7.57430970661747  Accuracy:  0.55703125\n",
            "Epoch:  124  Loss:  7.564535315053456  Accuracy:  0.55703125\n",
            "Epoch:  125  Loss:  7.5548008306272525  Accuracy:  0.55625\n",
            "Epoch:  126  Loss:  7.545106673554341  Accuracy:  0.55625\n",
            "Epoch:  127  Loss:  7.5354532603339  Accuracy:  0.55625\n",
            "Epoch:  128  Loss:  7.525841002599701  Accuracy:  0.55390625\n",
            "Epoch:  129  Loss:  7.516270306021231  Accuracy:  0.55390625\n",
            "Epoch:  130  Loss:  7.5067415692658015  Accuracy:  0.55390625\n",
            "Epoch:  131  Loss:  7.4972551830311165  Accuracy:  0.55390625\n",
            "Epoch:  132  Loss:  7.487811529156406  Accuracy:  0.5546875\n",
            "Epoch:  133  Loss:  7.478410979818852  Accuracy:  0.55390625\n",
            "Epoch:  134  Loss:  7.469053896820547  Accuracy:  0.5546875\n",
            "Epoch:  135  Loss:  7.459740630969849  Accuracy:  0.5546875\n",
            "Epoch:  136  Loss:  7.450471521559672  Accuracy:  0.5546875\n",
            "Epoch:  137  Loss:  7.441246895943917  Accuracy:  0.55546875\n",
            "Epoch:  138  Loss:  7.432067069212228  Accuracy:  0.55546875\n",
            "Epoch:  139  Loss:  7.422932343962205  Accuracy:  0.5546875\n",
            "Epoch:  140  Loss:  7.41384301016744  Accuracy:  0.55546875\n",
            "Epoch:  141  Loss:  7.404799345139094  Accuracy:  0.55625\n",
            "Epoch:  142  Loss:  7.39580161357824  Accuracy:  0.55625\n",
            "Epoch:  143  Loss:  7.386850067715892  Accuracy:  0.55625\n",
            "Epoch:  144  Loss:  7.377944947537339  Accuracy:  0.55703125\n",
            "Epoch:  145  Loss:  7.369086481087317  Accuracy:  0.55703125\n",
            "Epoch:  146  Loss:  7.360274884852361  Accuracy:  0.55625\n",
            "Epoch:  147  Loss:  7.351510364216514  Accuracy:  0.55625\n",
            "Epoch:  148  Loss:  7.342793113986376  Accuracy:  0.5546875\n",
            "Epoch:  149  Loss:  7.334123318980986  Accuracy:  0.55625\n",
            "Epoch:  150  Loss:  7.325501154681639  Accuracy:  0.55625\n",
            "Epoch:  151  Loss:  7.316926787935799  Accuracy:  0.55625\n",
            "Epoch:  152  Loss:  7.308400377708376  Accuracy:  0.55703125\n",
            "Epoch:  153  Loss:  7.2999220758723125  Accuracy:  0.55625\n",
            "Epoch:  154  Loss:  7.29149202802891  Accuracy:  0.55546875\n",
            "Epoch:  155  Loss:  7.283110374346547  Accuracy:  0.55546875\n",
            "Epoch:  156  Loss:  7.274777250404554  Accuracy:  0.55703125\n",
            "Epoch:  157  Loss:  7.266492788026818  Accuracy:  0.55703125\n",
            "Epoch:  158  Loss:  7.258257116087735  Accuracy:  0.55703125\n",
            "Epoch:  159  Loss:  7.250070361270997  Accuracy:  0.5578125\n",
            "Epoch:  160  Loss:  7.241932648759954  Accuracy:  0.56015625\n",
            "Epoch:  161  Loss:  7.233844102836869  Accuracy:  0.5609375\n",
            "Epoch:  162  Loss:  7.225804847367601  Accuracy:  0.56171875\n",
            "Epoch:  163  Loss:  7.217815006147966  Accuracy:  0.56015625\n",
            "Epoch:  164  Loss:  7.209874703088983  Accuracy:  0.5609375\n",
            "Epoch:  165  Loss:  7.201984062219867  Accuracy:  0.56171875\n",
            "Epoch:  166  Loss:  7.194143207490589  Accuracy:  0.5625\n",
            "Epoch:  167  Loss:  7.186352262359884  Accuracy:  0.5625\n",
            "Epoch:  168  Loss:  7.1786113491597945  Accuracy:  0.5609375\n",
            "Epoch:  169  Loss:  7.170920588233979  Accuracy:  0.56171875\n",
            "Epoch:  170  Loss:  7.163280096854008  Accuracy:  0.56328125\n",
            "Epoch:  171  Loss:  7.155689987925331  Accuracy:  0.56328125\n",
            "Epoch:  172  Loss:  7.148150368502195  Accuracy:  0.5640625\n",
            "Epoch:  173  Loss:  7.140661338138157  Accuracy:  0.565625\n",
            "Epoch:  174  Loss:  7.133222987105464  Accuracy:  0.56640625\n",
            "Epoch:  175  Loss:  7.125835394522243  Accuracy:  0.5671875\n",
            "Epoch:  176  Loss:  7.118498626430577  Accuracy:  0.5671875\n",
            "Epoch:  177  Loss:  7.1112127338711515  Accuracy:  0.5671875\n",
            "Epoch:  178  Loss:  7.103977751000865  Accuracy:  0.5671875\n",
            "Epoch:  179  Loss:  7.096793693298784  Accuracy:  0.56640625\n",
            "Epoch:  180  Loss:  7.089660555903009  Accuracy:  0.56640625\n",
            "Epoch:  181  Loss:  7.0825783121167065  Accuracy:  0.5671875\n",
            "Epoch:  182  Loss:  7.0755469121160575  Accuracy:  0.56796875\n",
            "Epoch:  183  Loss:  7.068566281886381  Accuracy:  0.56953125\n",
            "Epoch:  184  Loss:  7.061636322405713  Accuracy:  0.56875\n",
            "Epoch:  185  Loss:  7.054756909087951  Accuracy:  0.56875\n",
            "Epoch:  186  Loss:  7.047927891490613  Accuracy:  0.56875\n",
            "Epoch:  187  Loss:  7.041149093285682  Accuracy:  0.56953125\n",
            "Epoch:  188  Loss:  7.034420312485925  Accuracy:  0.5703125\n",
            "Epoch:  189  Loss:  7.0277413219139975  Accuracy:  0.57109375\n",
            "Epoch:  190  Loss:  7.021111869897223  Accuracy:  0.56953125\n",
            "Epoch:  191  Loss:  7.014531681167577  Accuracy:  0.56875\n",
            "Epoch:  192  Loss:  7.008000457943903  Accuracy:  0.56875\n",
            "Epoch:  193  Loss:  7.00151788117171  Accuracy:  0.56875\n",
            "Epoch:  194  Loss:  6.995083611895038  Accuracy:  0.56875\n",
            "Epoch:  195  Loss:  6.988697292734733  Accuracy:  0.56953125\n",
            "Epoch:  196  Loss:  6.9823585494477385  Accuracy:  0.56953125\n",
            "Epoch:  197  Loss:  6.976066992543025  Accuracy:  0.5703125\n",
            "Epoch:  198  Loss:  6.969822218930924  Accuracy:  0.56953125\n",
            "Epoch:  199  Loss:  6.963623813584183  Accuracy:  0.56953125\n",
            "Epoch:  200  Loss:  6.957471351190856  Accuracy:  0.57109375\n",
            "Epoch:  201  Loss:  6.951364397780918  Accuracy:  0.57109375\n",
            "Epoch:  202  Loss:  6.945302512310576  Accuracy:  0.571875\n",
            "Epoch:  203  Loss:  6.9392852481900835  Accuracy:  0.571875\n",
            "Epoch:  204  Loss:  6.933312154742938  Accuracy:  0.5703125\n",
            "Epoch:  205  Loss:  6.927382778586244  Accuracy:  0.5703125\n",
            "Epoch:  206  Loss:  6.921496664923796  Accuracy:  0.57265625\n",
            "Epoch:  207  Loss:  6.915653358745348  Accuracy:  0.5734375\n",
            "Epoch:  208  Loss:  6.90985240592706  Accuracy:  0.5734375\n",
            "Epoch:  209  Loss:  6.904093354229803  Accuracy:  0.571875\n",
            "Epoch:  210  Loss:  6.898375754193346  Accuracy:  0.57109375\n",
            "Epoch:  211  Loss:  6.892699159925907  Accuracy:  0.571875\n",
            "Epoch:  212  Loss:  6.887063129789686  Accuracy:  0.571875\n",
            "Epoch:  213  Loss:  6.881467226984137  Accuracy:  0.5734375\n",
            "Epoch:  214  Loss:  6.87591102002977  Accuracy:  0.5734375\n",
            "Epoch:  215  Loss:  6.870394083156123  Accuracy:  0.5734375\n",
            "Epoch:  216  Loss:  6.864915996598316  Accuracy:  0.571875\n",
            "Epoch:  217  Loss:  6.859476346807341  Accuracy:  0.571875\n",
            "Epoch:  218  Loss:  6.8540747265797135  Accuracy:  0.571875\n",
            "Epoch:  219  Loss:  6.848710735112649  Accuracy:  0.57265625\n",
            "Epoch:  220  Loss:  6.843383977991299  Accuracy:  0.57265625\n",
            "Epoch:  221  Loss:  6.838094067114795  Accuracy:  0.575\n",
            "Epoch:  222  Loss:  6.832840620568119  Accuracy:  0.57421875\n",
            "Epoch:  223  Loss:  6.827623262446835  Accuracy:  0.57421875\n",
            "Epoch:  224  Loss:  6.822441622641786  Accuracy:  0.57421875\n",
            "Epoch:  225  Loss:  6.817295336590751  Accuracy:  0.5734375\n",
            "Epoch:  226  Loss:  6.812184045003941  Accuracy:  0.575\n",
            "Epoch:  227  Loss:  6.807107393569908  Accuracy:  0.575\n",
            "Epoch:  228  Loss:  6.802065032648301  Accuracy:  0.57421875\n",
            "Epoch:  229  Loss:  6.797056616955365  Accuracy:  0.57421875\n",
            "Epoch:  230  Loss:  6.792081805247827  Accuracy:  0.57421875\n",
            "Epoch:  231  Loss:  6.7871402600102275  Accuracy:  0.575\n",
            "Epoch:  232  Loss:  6.782231647150371  Accuracy:  0.57421875\n",
            "Epoch:  233  Loss:  6.777355635706925  Accuracy:  0.575\n",
            "Epoch:  234  Loss:  6.772511897572731  Accuracy:  0.57421875\n",
            "Epoch:  235  Loss:  6.767700107236773  Accuracy:  0.57578125\n",
            "Epoch:  236  Loss:  6.7629199415471914  Accuracy:  0.57578125\n",
            "Epoch:  237  Loss:  6.758171079497181  Accuracy:  0.575\n",
            "Epoch:  238  Loss:  6.753453202035021  Accuracy:  0.57421875\n",
            "Epoch:  239  Loss:  6.748765991899002  Accuracy:  0.57421875\n",
            "Epoch:  240  Loss:  6.744109133477503  Accuracy:  0.57421875\n",
            "Epoch:  241  Loss:  6.7394823126940056  Accuracy:  0.5734375\n",
            "Epoch:  242  Loss:  6.734885216916436  Accuracy:  0.571875\n",
            "Epoch:  243  Loss:  6.730317534889856  Accuracy:  0.571875\n",
            "Epoch:  244  Loss:  6.725778956691201  Accuracy:  0.57265625\n",
            "Epoch:  245  Loss:  6.7212691737045365  Accuracy:  0.57265625\n",
            "Epoch:  246  Loss:  6.716787878615051  Accuracy:  0.57421875\n",
            "Epoch:  247  Loss:  6.712334765419894  Accuracy:  0.5734375\n",
            "Epoch:  248  Loss:  6.707909529453818  Accuracy:  0.57421875\n",
            "Epoch:  249  Loss:  6.703511867427591  Accuracy:  0.57421875\n",
            "Epoch:  250  Loss:  6.699141477477056  Accuracy:  0.57421875\n",
            "Epoch:  251  Loss:  6.69479805922083  Accuracy:  0.57421875\n",
            "Epoch:  252  Loss:  6.69048131382461  Accuracy:  0.57421875\n",
            "Epoch:  253  Loss:  6.6861909440702245  Accuracy:  0.57421875\n",
            "Epoch:  254  Loss:  6.681926654427644  Accuracy:  0.57421875\n",
            "Epoch:  255  Loss:  6.677688151128334  Accuracy:  0.575\n",
            "Epoch:  256  Loss:  6.673475142238473  Accuracy:  0.5765625\n",
            "Epoch:  257  Loss:  6.669287337730739  Accuracy:  0.5765625\n",
            "Epoch:  258  Loss:  6.665124449553565  Accuracy:  0.57578125\n",
            "Epoch:  259  Loss:  6.66098619169689  Accuracy:  0.57578125\n",
            "Epoch:  260  Loss:  6.656872280253718  Accuracy:  0.57578125\n",
            "Epoch:  261  Loss:  6.652782433476861  Accuracy:  0.5765625\n",
            "Epoch:  262  Loss:  6.6487163718305  Accuracy:  0.57734375\n",
            "Epoch:  263  Loss:  6.644673818036324  Accuracy:  0.57734375\n",
            "Epoch:  264  Loss:  6.640654497114215  Accuracy:  0.57734375\n",
            "Epoch:  265  Loss:  6.636658136417507  Accuracy:  0.57734375\n",
            "Epoch:  266  Loss:  6.632684465663043  Accuracy:  0.578125\n",
            "Epoch:  267  Loss:  6.628733216956368  Accuracy:  0.5796875\n",
            "Epoch:  268  Loss:  6.6248041248124565  Accuracy:  0.58046875\n",
            "Epoch:  269  Loss:  6.62089692617247  Accuracy:  0.5796875\n",
            "Epoch:  270  Loss:  6.617011360417133  Accuracy:  0.58046875\n",
            "Epoch:  271  Loss:  6.613147169377301  Accuracy:  0.5796875\n",
            "Epoch:  272  Loss:  6.609304097342383  Accuracy:  0.5796875\n",
            "Epoch:  273  Loss:  6.605481891067257  Accuracy:  0.5796875\n",
            "Epoch:  274  Loss:  6.601680299778297  Accuracy:  0.58046875\n",
            "Epoch:  275  Loss:  6.59789907517915  Accuracy:  0.578125\n",
            "Epoch:  276  Loss:  6.5941379714568145  Accuracy:  0.578125\n",
            "Epoch:  277  Loss:  6.590396745288541  Accuracy:  0.578125\n",
            "Epoch:  278  Loss:  6.586675155850005  Accuracy:  0.5765625\n",
            "Epoch:  279  Loss:  6.582972964825117  Accuracy:  0.57578125\n",
            "Epoch:  280  Loss:  6.579289936417749  Accuracy:  0.575\n",
            "Epoch:  281  Loss:  6.5756258373655445  Accuracy:  0.5734375\n",
            "Epoch:  282  Loss:  6.571980436955892  Accuracy:  0.57265625\n",
            "Epoch:  283  Loss:  6.5683535070440255  Accuracy:  0.57265625\n",
            "Epoch:  284  Loss:  6.564744822073069  Accuracy:  0.5734375\n",
            "Epoch:  285  Loss:  6.561154159095791  Accuracy:  0.5734375\n",
            "Epoch:  286  Loss:  6.557581297797649  Accuracy:  0.5734375\n",
            "Epoch:  287  Loss:  6.554026020520641  Accuracy:  0.5734375\n",
            "Epoch:  288  Loss:  6.550488112287361  Accuracy:  0.5734375\n",
            "Epoch:  289  Loss:  6.546967360824508  Accuracy:  0.5734375\n",
            "Epoch:  290  Loss:  6.5434635565850865  Accuracy:  0.57421875\n",
            "Epoch:  291  Loss:  6.539976492768384  Accuracy:  0.575\n",
            "Epoch:  292  Loss:  6.536505965336732  Accuracy:  0.57578125\n",
            "Epoch:  293  Loss:  6.5330517730280775  Accuracy:  0.57578125\n",
            "Epoch:  294  Loss:  6.529613717363224  Accuracy:  0.57578125\n",
            "Epoch:  295  Loss:  6.526191602646672  Accuracy:  0.57578125\n",
            "Epoch:  296  Loss:  6.522785235959918  Accuracy:  0.57578125\n",
            "Epoch:  297  Loss:  6.5193944271461195  Accuracy:  0.57578125\n",
            "Epoch:  298  Loss:  6.5160189887850155  Accuracy:  0.57578125\n",
            "Epoch:  299  Loss:  6.5126587361570945  Accuracy:  0.578125\n",
            "Epoch:  300  Loss:  6.509313487196057  Accuracy:  0.57734375\n",
            "Epoch:  301  Loss:  6.505983062428743  Accuracy:  0.57734375\n",
            "Epoch:  302  Loss:  6.502667284901827  Accuracy:  0.57734375\n",
            "Epoch:  303  Loss:  6.499365980094785  Accuracy:  0.57734375\n",
            "Epoch:  304  Loss:  6.4960789758188495  Accuracy:  0.57734375\n",
            "Epoch:  305  Loss:  6.492806102101888  Accuracy:  0.57734375\n",
            "Epoch:  306  Loss:  6.489547191059556  Accuracy:  0.57734375\n",
            "Epoch:  307  Loss:  6.486302076753283  Accuracy:  0.57734375\n",
            "Epoch:  308  Loss:  6.483070595036187  Accuracy:  0.57734375\n",
            "Epoch:  309  Loss:  6.479852583388377  Accuracy:  0.578125\n",
            "Epoch:  310  Loss:  6.476647880743679  Accuracy:  0.578125\n",
            "Epoch:  311  Loss:  6.473456327310319  Accuracy:  0.57890625\n",
            "Epoch:  312  Loss:  6.470277764388783  Accuracy:  0.5796875\n",
            "Epoch:  313  Loss:  6.467112034190615  Accuracy:  0.5796875\n",
            "Epoch:  314  Loss:  6.46395897966273  Accuracy:  0.5796875\n",
            "Epoch:  315  Loss:  6.4608184443224115  Accuracy:  0.5796875\n",
            "Epoch:  316  Loss:  6.45769027210891  Accuracy:  0.58046875\n",
            "Epoch:  317  Loss:  6.454574307258246  Accuracy:  0.58046875\n",
            "Epoch:  318  Loss:  6.451470394208412  Accuracy:  0.58046875\n",
            "Epoch:  319  Loss:  6.448378377542696  Accuracy:  0.58046875\n",
            "Epoch:  320  Loss:  6.445298101979228  Accuracy:  0.58046875\n",
            "Epoch:  321  Loss:  6.44222941241502  Accuracy:  0.58046875\n",
            "Epoch:  322  Loss:  6.439172154032699  Accuracy:  0.58125\n",
            "Epoch:  323  Loss:  6.43612617247768  Accuracy:  0.58125\n",
            "Epoch:  324  Loss:  6.433091314112817  Accuracy:  0.58125\n",
            "Epoch:  325  Loss:  6.430067426356226  Accuracy:  0.58203125\n",
            "Epoch:  326  Loss:  6.427054358106313  Accuracy:  0.58203125\n",
            "Epoch:  327  Loss:  6.424051960255772  Accuracy:  0.5828125\n",
            "Epoch:  328  Loss:  6.421060086293505  Accuracy:  0.5828125\n",
            "Epoch:  329  Loss:  6.418078592990213  Accuracy:  0.5828125\n",
            "Epoch:  330  Loss:  6.41510734115973  Accuracy:  0.5828125\n",
            "Epoch:  331  Loss:  6.412146196484274  Accuracy:  0.5828125\n",
            "Epoch:  332  Loss:  6.4091950303878775  Accuracy:  0.5828125\n",
            "Epoch:  333  Loss:  6.406253720938477  Accuracy:  0.584375\n",
            "Epoch:  334  Loss:  6.403322153755889  Accuracy:  0.58359375\n",
            "Epoch:  335  Loss:  6.40040022290052  Accuracy:  0.58203125\n",
            "Epoch:  336  Loss:  6.397487831716277  Accuracy:  0.58203125\n",
            "Epoch:  337  Loss:  6.394584893601197  Accuracy:  0.58203125\n",
            "Epoch:  338  Loss:  6.3916913326809635  Accuracy:  0.58125\n",
            "Epoch:  339  Loss:  6.3888070843636395  Accuracy:  0.58203125\n",
            "Epoch:  340  Loss:  6.385932095758726  Accuracy:  0.58203125\n",
            "Epoch:  341  Loss:  6.383066325949557  Accuracy:  0.58203125\n",
            "Epoch:  342  Loss:  6.380209746115006  Accuracy:  0.58203125\n",
            "Epoch:  343  Loss:  6.37736233950358  Accuracy:  0.58203125\n",
            "Epoch:  344  Loss:  6.374524101269986  Accuracy:  0.58203125\n",
            "Epoch:  345  Loss:  6.37169503819042  Accuracy:  0.58125\n",
            "Epoch:  346  Loss:  6.368875168277631  Accuracy:  0.58125\n",
            "Epoch:  347  Loss:  6.366064520320002  Accuracy:  0.58203125\n",
            "Epoch:  348  Loss:  6.363263133370102  Accuracy:  0.58203125\n",
            "Epoch:  349  Loss:  6.360471056207581  Accuracy:  0.58203125\n",
            "Epoch:  350  Loss:  6.3576883467988266  Accuracy:  0.58359375\n",
            "Epoch:  351  Loss:  6.354915071772118  Accuracy:  0.58359375\n",
            "Epoch:  352  Loss:  6.3521513059221535  Accuracy:  0.58359375\n",
            "Epoch:  353  Loss:  6.349397131752605  Accuracy:  0.5828125\n",
            "Epoch:  354  Loss:  6.346652639059956  Accuracy:  0.5828125\n",
            "Epoch:  355  Loss:  6.343917924556835  Accuracy:  0.5828125\n",
            "Epoch:  356  Loss:  6.341193091528667  Accuracy:  0.58359375\n",
            "Epoch:  357  Loss:  6.338478249513905  Accuracy:  0.5828125\n",
            "Epoch:  358  Loss:  6.335773513995508  Accuracy:  0.5828125\n",
            "Epoch:  359  Loss:  6.333079006089801  Accuracy:  0.5828125\n",
            "Epoch:  360  Loss:  6.330394852218182  Accuracy:  0.5828125\n",
            "Epoch:  361  Loss:  6.327721183747507  Accuracy:  0.58359375\n",
            "Epoch:  362  Loss:  6.325058136585971  Accuracy:  0.58359375\n",
            "Epoch:  363  Loss:  6.322405850723072  Accuracy:  0.58359375\n",
            "Epoch:  364  Loss:  6.31976446970447  Accuracy:  0.58359375\n",
            "Epoch:  365  Loss:  6.317134140035156  Accuracy:  0.58359375\n",
            "Epoch:  366  Loss:  6.314515010507326  Accuracy:  0.584375\n",
            "Epoch:  367  Loss:  6.311907231452379  Accuracy:  0.584375\n",
            "Epoch:  368  Loss:  6.309310953919697  Accuracy:  0.5859375\n",
            "Epoch:  369  Loss:  6.306726328787931  Accuracy:  0.58515625\n",
            "Epoch:  370  Loss:  6.304153505817607  Accuracy:  0.58515625\n",
            "Epoch:  371  Loss:  6.30159263265658  Accuracy:  0.58515625\n",
            "Epoch:  372  Loss:  6.29904385381245  Accuracy:  0.58515625\n",
            "Epoch:  373  Loss:  6.296507309608039  Accuracy:  0.584375\n",
            "Epoch:  374  Loss:  6.293983135137684  Accuracy:  0.584375\n",
            "Epoch:  375  Loss:  6.29147145924311  Accuracy:  0.584375\n",
            "Epoch:  376  Loss:  6.288972403528058  Accuracy:  0.584375\n",
            "Epoch:  377  Loss:  6.286486081430621  Accuracy:  0.584375\n",
            "Epoch:  378  Loss:  6.284012597371332  Accuracy:  0.58515625\n",
            "Epoch:  379  Loss:  6.2815520459935446  Accuracy:  0.58515625\n",
            "Epoch:  380  Loss:  6.27910451151054  Accuracy:  0.58515625\n",
            "Epoch:  381  Loss:  6.276670067171158  Accuracy:  0.58515625\n",
            "Epoch:  382  Loss:  6.2742487748528415  Accuracy:  0.58515625\n",
            "Epoch:  383  Loss:  6.271840684787663  Accuracy:  0.58515625\n",
            "Epoch:  384  Loss:  6.269445835423572  Accuracy:  0.58515625\n",
            "Epoch:  385  Loss:  6.267064253419774  Accuracy:  0.58515625\n",
            "Epoch:  386  Loss:  6.264695953771913  Accuracy:  0.5859375\n",
            "Epoch:  387  Loss:  6.262340940059883  Accuracy:  0.58671875\n",
            "Epoch:  388  Loss:  6.259999204808496  Accuracy:  0.5875\n",
            "Epoch:  389  Loss:  6.257670729949217  Accuracy:  0.5875\n",
            "Epoch:  390  Loss:  6.255355487369535  Accuracy:  0.5875\n",
            "Epoch:  391  Loss:  6.253053439535519  Accuracy:  0.58828125\n",
            "Epoch:  392  Loss:  6.250764540172531  Accuracy:  0.58828125\n",
            "Epoch:  393  Loss:  6.2484887349889915  Accuracy:  0.58828125\n",
            "Epoch:  394  Loss:  6.246225962428449  Accuracy:  0.58828125\n",
            "Epoch:  395  Loss:  6.243976154435905  Accuracy:  0.58828125\n",
            "Epoch:  396  Loss:  6.241739237225397  Accuracy:  0.58828125\n",
            "Epoch:  397  Loss:  6.239515132036995  Accuracy:  0.58828125\n",
            "Epoch:  398  Loss:  6.237303755872861  Accuracy:  0.5875\n",
            "Epoch:  399  Loss:  6.235105022203358  Accuracy:  0.5875\n",
            "Epoch:  400  Loss:  6.2329188416357955  Accuracy:  0.5875\n",
            "Epoch:  401  Loss:  6.2307451225397985  Accuracy:  0.5875\n",
            "Epoch:  402  Loss:  6.228583771624691  Accuracy:  0.58828125\n",
            "Epoch:  403  Loss:  6.226434694465608  Accuracy:  0.58984375\n",
            "Epoch:  404  Loss:  6.224297795976216  Accuracy:  0.590625\n",
            "Epoch:  405  Loss:  6.222172980826981  Accuracy:  0.58984375\n",
            "Epoch:  406  Loss:  6.220060153808861  Accuracy:  0.590625\n",
            "Epoch:  407  Loss:  6.21795922014306  Accuracy:  0.590625\n",
            "Epoch:  408  Loss:  6.21587008573818  Accuracy:  0.590625\n",
            "Epoch:  409  Loss:  6.213792657396626  Accuracy:  0.590625\n",
            "Epoch:  410  Loss:  6.211726842972564  Accuracy:  0.590625\n",
            "Epoch:  411  Loss:  6.209672551484102  Accuracy:  0.590625\n",
            "Epoch:  412  Loss:  6.207629693182577  Accuracy:  0.590625\n",
            "Epoch:  413  Loss:  6.205598179582072  Accuracy:  0.58984375\n",
            "Epoch:  414  Loss:  6.2035779234524115  Accuracy:  0.5921875\n",
            "Epoch:  415  Loss:  6.201568838778947  Accuracy:  0.59296875\n",
            "Epoch:  416  Loss:  6.1995708406925445  Accuracy:  0.59296875\n",
            "Epoch:  417  Loss:  6.197583845373172  Accuracy:  0.59453125\n",
            "Epoch:  418  Loss:  6.195607769930513  Accuracy:  0.59453125\n",
            "Epoch:  419  Loss:  6.193642532264982  Accuracy:  0.59453125\n",
            "Epoch:  420  Loss:  6.191688050912549  Accuracy:  0.59375\n",
            "Epoch:  421  Loss:  6.189744244876664  Accuracy:  0.59453125\n",
            "Epoch:  422  Loss:  6.187811033450577  Accuracy:  0.59453125\n",
            "Epoch:  423  Loss:  6.185888336033264  Accuracy:  0.59296875\n",
            "Epoch:  424  Loss:  6.183976071942095  Accuracy:  0.59296875\n",
            "Epoch:  425  Loss:  6.182074160225292  Accuracy:  0.59296875\n",
            "Epoch:  426  Loss:  6.180182519477133  Accuracy:  0.59375\n",
            "Epoch:  427  Loss:  6.178301067658731  Accuracy:  0.59453125\n",
            "Epoch:  428  Loss:  6.1764297219270805  Accuracy:  0.59296875\n",
            "Epoch:  429  Loss:  6.174568398474898  Accuracy:  0.5921875\n",
            "Epoch:  430  Loss:  6.172717012383631  Accuracy:  0.59140625\n",
            "Epoch:  431  Loss:  6.170875477491771  Accuracy:  0.59140625\n",
            "Epoch:  432  Loss:  6.169043706280407  Accuracy:  0.59140625\n",
            "Epoch:  433  Loss:  6.167221609777681  Accuracy:  0.5921875\n",
            "Epoch:  434  Loss:  6.165409097483564  Accuracy:  0.5921875\n",
            "Epoch:  435  Loss:  6.163606077316059  Accuracy:  0.5921875\n",
            "Epoch:  436  Loss:  6.161812455579633  Accuracy:  0.59296875\n",
            "Epoch:  437  Loss:  6.160028136956365  Accuracy:  0.59296875\n",
            "Epoch:  438  Loss:  6.158253024519976  Accuracy:  0.590625\n",
            "Epoch:  439  Loss:  6.156487019772546  Accuracy:  0.59140625\n",
            "Epoch:  440  Loss:  6.15473002270345  Accuracy:  0.59140625\n",
            "Epoch:  441  Loss:  6.152981931869672  Accuracy:  0.59140625\n",
            "Epoch:  442  Loss:  6.1512426444963895  Accuracy:  0.58984375\n",
            "Epoch:  443  Loss:  6.149512056596438  Accuracy:  0.58984375\n",
            "Epoch:  444  Loss:  6.147790063107012  Accuracy:  0.5890625\n",
            "Epoch:  445  Loss:  6.1460765580417105  Accuracy:  0.5890625\n",
            "Epoch:  446  Loss:  6.144371434655933  Accuracy:  0.5890625\n",
            "Epoch:  447  Loss:  6.1426745856233875  Accuracy:  0.58984375\n",
            "Epoch:  448  Loss:  6.140985903221443  Accuracy:  0.58984375\n",
            "Epoch:  449  Loss:  6.139305279523007  Accuracy:  0.5890625\n",
            "Epoch:  450  Loss:  6.137632606592523  Accuracy:  0.58984375\n",
            "Epoch:  451  Loss:  6.135967776683802  Accuracy:  0.58984375\n",
            "Epoch:  452  Loss:  6.134310682437421  Accuracy:  0.5890625\n",
            "Epoch:  453  Loss:  6.132661217075511  Accuracy:  0.5890625\n",
            "Epoch:  454  Loss:  6.131019274591938  Accuracy:  0.58984375\n",
            "Epoch:  455  Loss:  6.129384749936018  Accuracy:  0.58984375\n",
            "Epoch:  456  Loss:  6.127757539188085  Accuracy:  0.58984375\n",
            "Epoch:  457  Loss:  6.126137539725448  Accuracy:  0.58984375\n",
            "Epoch:  458  Loss:  6.1245246503774835  Accuracy:  0.590625\n",
            "Epoch:  459  Loss:  6.122918771568817  Accuracy:  0.590625\n",
            "Epoch:  460  Loss:  6.121319805449778  Accuracy:  0.590625\n",
            "Epoch:  461  Loss:  6.119727656013531  Accuracy:  0.590625\n",
            "Epoch:  462  Loss:  6.118142229199474  Accuracy:  0.590625\n",
            "Epoch:  463  Loss:  6.116563432982717  Accuracy:  0.59140625\n",
            "Epoch:  464  Loss:  6.114991177449612  Accuracy:  0.5921875\n",
            "Epoch:  465  Loss:  6.113425374859494  Accuracy:  0.5921875\n",
            "Epoch:  466  Loss:  6.111865939692922  Accuracy:  0.5921875\n",
            "Epoch:  467  Loss:  6.110312788686848  Accuracy:  0.59296875\n",
            "Epoch:  468  Loss:  6.1087658408572585  Accuracy:  0.59296875\n",
            "Epoch:  469  Loss:  6.107225017509906  Accuracy:  0.59296875\n",
            "Epoch:  470  Loss:  6.105690242239837  Accuracy:  0.59296875\n",
            "Epoch:  471  Loss:  6.104161440920487  Accuracy:  0.59296875\n",
            "Epoch:  472  Loss:  6.1026385416831115  Accuracy:  0.59140625\n",
            "Epoch:  473  Loss:  6.101121474887414  Accuracy:  0.59140625\n",
            "Epoch:  474  Loss:  6.099610173084163  Accuracy:  0.59140625\n",
            "Epoch:  475  Loss:  6.098104570970673  Accuracy:  0.59140625\n",
            "Epoch:  476  Loss:  6.096604605339936  Accuracy:  0.590625\n",
            "Epoch:  477  Loss:  6.09511021502423  Accuracy:  0.590625\n",
            "Epoch:  478  Loss:  6.093621340833971  Accuracy:  0.590625\n",
            "Epoch:  479  Loss:  6.092137925492555  Accuracy:  0.59140625\n",
            "Epoch:  480  Loss:  6.090659913567897  Accuracy:  0.590625\n",
            "Epoch:  481  Loss:  6.089187251401325  Accuracy:  0.590625\n",
            "Epoch:  482  Loss:  6.087719887034475  Accuracy:  0.590625\n",
            "Epoch:  483  Loss:  6.086257770134733  Accuracy:  0.590625\n",
            "Epoch:  484  Loss:  6.0848008519198  Accuracy:  0.590625\n",
            "Epoch:  485  Loss:  6.083349085081835  Accuracy:  0.590625\n",
            "Epoch:  486  Loss:  6.081902423711645  Accuracy:  0.590625\n",
            "Epoch:  487  Loss:  6.080460823223317  Accuracy:  0.590625\n",
            "Epoch:  488  Loss:  6.079024240279666  Accuracy:  0.590625\n",
            "Epoch:  489  Loss:  6.077592632718819  Accuracy:  0.59140625\n",
            "Epoch:  490  Loss:  6.076165959482208  Accuracy:  0.59140625\n",
            "Epoch:  491  Loss:  6.07474418054426  Accuracy:  0.59140625\n",
            "Epoch:  492  Loss:  6.07332725684397  Accuracy:  0.590625\n",
            "Epoch:  493  Loss:  6.0719151502185555  Accuracy:  0.590625\n",
            "Epoch:  494  Loss:  6.070507823339365  Accuracy:  0.590625\n",
            "Epoch:  495  Loss:  6.069105239650161  Accuracy:  0.590625\n",
            "Epoch:  496  Loss:  6.067707363307891  Accuracy:  0.59140625\n",
            "Epoch:  497  Loss:  6.0663141591260255  Accuracy:  0.590625\n",
            "Epoch:  498  Loss:  6.064925592520561  Accuracy:  0.58984375\n",
            "Epoch:  499  Loss:  6.063541629458673  Accuracy:  0.590625\n",
            "Epoch:  500  Loss:  6.062162236410123  Accuracy:  0.590625\n",
            "Epoch:  501  Loss:  6.060787380301383  Accuracy:  0.590625\n",
            "Epoch:  502  Loss:  6.0594170284724855  Accuracy:  0.590625\n",
            "Epoch:  503  Loss:  6.058051148636612  Accuracy:  0.59140625\n",
            "Epoch:  504  Loss:  6.056689708842365  Accuracy:  0.59140625\n",
            "Epoch:  505  Loss:  6.055332677438702  Accuracy:  0.590625\n",
            "Epoch:  506  Loss:  6.0539800230425  Accuracy:  0.590625\n",
            "Epoch:  507  Loss:  6.052631714508665  Accuracy:  0.58984375\n",
            "Epoch:  508  Loss:  6.051287720902769  Accuracy:  0.590625\n",
            "Epoch:  509  Loss:  6.049948011476105  Accuracy:  0.590625\n",
            "Epoch:  510  Loss:  6.048612555643122  Accuracy:  0.590625\n",
            "Epoch:  511  Loss:  6.047281322961146  Accuracy:  0.590625\n",
            "Epoch:  512  Loss:  6.045954283112299  Accuracy:  0.590625\n",
            "Epoch:  513  Loss:  6.044631405887547  Accuracy:  0.590625\n",
            "Epoch:  514  Loss:  6.043312661172775  Accuracy:  0.58984375\n",
            "Epoch:  515  Loss:  6.0419980189367966  Accuracy:  0.5890625\n",
            "Epoch:  516  Loss:  6.040687449221215  Accuracy:  0.5890625\n",
            "Epoch:  517  Loss:  6.039380922132027  Accuracy:  0.590625\n",
            "Epoch:  518  Loss:  6.038078407832886  Accuracy:  0.58984375\n",
            "Epoch:  519  Loss:  6.036779876539908  Accuracy:  0.58984375\n",
            "Epoch:  520  Loss:  6.03548529851795  Accuracy:  0.58984375\n",
            "Epoch:  521  Loss:  6.034194644078236  Accuracy:  0.58984375\n",
            "Epoch:  522  Loss:  6.032907883577257  Accuracy:  0.590625\n",
            "Epoch:  523  Loss:  6.031624987416837  Accuracy:  0.590625\n",
            "Epoch:  524  Loss:  6.030345926045269  Accuracy:  0.590625\n",
            "Epoch:  525  Loss:  6.029070669959454  Accuracy:  0.59140625\n",
            "Epoch:  526  Loss:  6.027799189707906  Accuracy:  0.59140625\n",
            "Epoch:  527  Loss:  6.026531455894604  Accuracy:  0.59140625\n",
            "Epoch:  528  Loss:  6.025267439183535  Accuracy:  0.5921875\n",
            "Epoch:  529  Loss:  6.0240071103039  Accuracy:  0.5921875\n",
            "Epoch:  530  Loss:  6.022750440055885  Accuracy:  0.5921875\n",
            "Epoch:  531  Loss:  6.0214973993169325  Accuracy:  0.5921875\n",
            "Epoch:  532  Loss:  6.0202479590484215  Accuracy:  0.5921875\n",
            "Epoch:  533  Loss:  6.019002090302725  Accuracy:  0.5921875\n",
            "Epoch:  534  Loss:  6.017759764230556  Accuracy:  0.5921875\n",
            "Epoch:  535  Loss:  6.016520952088559  Accuracy:  0.5921875\n",
            "Epoch:  536  Loss:  6.015285625247084  Accuracy:  0.59140625\n",
            "Epoch:  537  Loss:  6.014053755198103  Accuracy:  0.59140625\n",
            "Epoch:  538  Loss:  6.012825313563214  Accuracy:  0.590625\n",
            "Epoch:  539  Loss:  6.011600272101712  Accuracy:  0.59140625\n",
            "Epoch:  540  Loss:  6.010378602718665  Accuracy:  0.59140625\n",
            "Epoch:  541  Loss:  6.009160277472974  Accuracy:  0.5921875\n",
            "Epoch:  542  Loss:  6.007945268585396  Accuracy:  0.5921875\n",
            "Epoch:  543  Loss:  6.006733548446499  Accuracy:  0.59296875\n",
            "Epoch:  544  Loss:  6.005525089624529  Accuracy:  0.59375\n",
            "Epoch:  545  Loss:  6.0043198648731515  Accuracy:  0.59375\n",
            "Epoch:  546  Loss:  6.003117847139108  Accuracy:  0.59375\n",
            "Epoch:  547  Loss:  6.001919009569709  Accuracy:  0.59375\n",
            "Epoch:  548  Loss:  6.0007233255202  Accuracy:  0.59375\n",
            "Epoch:  549  Loss:  5.999530768560985  Accuracy:  0.59375\n",
            "Epoch:  550  Loss:  5.998341312484696  Accuracy:  0.59375\n",
            "Epoch:  551  Loss:  5.997154931313114  Accuracy:  0.5953125\n",
            "Epoch:  552  Loss:  5.995971599303937  Accuracy:  0.59453125\n",
            "Epoch:  553  Loss:  5.994791290957409  Accuracy:  0.59453125\n",
            "Epoch:  554  Loss:  5.993613981022798  Accuracy:  0.59453125\n",
            "Epoch:  555  Loss:  5.992439644504739  Accuracy:  0.59375\n",
            "Epoch:  556  Loss:  5.991268256669445  Accuracy:  0.59375\n",
            "Epoch:  557  Loss:  5.990099793050783  Accuracy:  0.59375\n",
            "Epoch:  558  Loss:  5.988934229456241  Accuracy:  0.5921875\n",
            "Epoch:  559  Loss:  5.987771541972774  Accuracy:  0.5921875\n",
            "Epoch:  560  Loss:  5.986611706972542  Accuracy:  0.5921875\n",
            "Epoch:  561  Loss:  5.985454701118554  Accuracy:  0.59140625\n",
            "Epoch:  562  Loss:  5.984300501370216  Accuracy:  0.59140625\n",
            "Epoch:  563  Loss:  5.983149084988774  Accuracy:  0.5921875\n",
            "Epoch:  564  Loss:  5.982000429542706  Accuracy:  0.59296875\n",
            "Epoch:  565  Loss:  5.980854512913  Accuracy:  0.59296875\n",
            "Epoch:  566  Loss:  5.979711313298379  Accuracy:  0.59296875\n",
            "Epoch:  567  Loss:  5.978570809220439  Accuracy:  0.5921875\n",
            "Epoch:  568  Loss:  5.977432979528713  Accuracy:  0.59140625\n",
            "Epoch:  569  Loss:  5.976297803405674  Accuracy:  0.59140625\n",
            "Epoch:  570  Loss:  5.975165260371648  Accuracy:  0.59140625\n",
            "Epoch:  571  Loss:  5.974035330289659  Accuracy:  0.59140625\n",
            "Epoch:  572  Loss:  5.972907993370191  Accuracy:  0.59140625\n",
            "Epoch:  573  Loss:  5.971783230175864  Accuracy:  0.59140625\n",
            "Epoch:  574  Loss:  5.970661021626027  Accuracy:  0.59140625\n",
            "Epoch:  575  Loss:  5.969541349001242  Accuracy:  0.59140625\n",
            "Epoch:  576  Loss:  5.968424193947676  Accuracy:  0.590625\n",
            "Epoch:  577  Loss:  5.967309538481374  Accuracy:  0.59140625\n",
            "Epoch:  578  Loss:  5.966197364992413  Accuracy:  0.59140625\n",
            "Epoch:  579  Loss:  5.965087656248929  Accuracy:  0.590625\n",
            "Epoch:  580  Loss:  5.963980395400995  Accuracy:  0.590625\n",
            "Epoch:  581  Loss:  5.9628755659843575  Accuracy:  0.590625\n",
            "Epoch:  582  Loss:  5.961773151924012  Accuracy:  0.590625\n",
            "Epoch:  583  Loss:  5.960673137537597  Accuracy:  0.590625\n",
            "Epoch:  584  Loss:  5.959575507538616  Accuracy:  0.59140625\n",
            "Epoch:  585  Loss:  5.95848024703946  Accuracy:  0.59140625\n",
            "Epoch:  586  Loss:  5.957387341554233  Accuracy:  0.59140625\n",
            "Epoch:  587  Loss:  5.95629677700135  Accuracy:  0.59140625\n",
            "Epoch:  588  Loss:  5.955208539705936  Accuracy:  0.590625\n",
            "Epoch:  589  Loss:  5.954122616401959  Accuracy:  0.59140625\n",
            "Epoch:  590  Loss:  5.953038994234155  Accuracy:  0.59140625\n",
            "Epoch:  591  Loss:  5.951957660759677  Accuracy:  0.58984375\n",
            "Epoch:  592  Loss:  5.950878603949503  Accuracy:  0.58984375\n",
            "Epoch:  593  Loss:  5.949801812189568  Accuracy:  0.58984375\n",
            "Epoch:  594  Loss:  5.948727274281636  Accuracy:  0.5890625\n",
            "Epoch:  595  Loss:  5.947654979443891  Accuracy:  0.5890625\n",
            "Epoch:  596  Loss:  5.946584917311247  Accuracy:  0.5890625\n",
            "Epoch:  597  Loss:  5.945517077935375  Accuracy:  0.5890625\n",
            "Epoch:  598  Loss:  5.944451451784441  Accuracy:  0.5890625\n",
            "Epoch:  599  Loss:  5.943388029742563  Accuracy:  0.5890625\n",
            "Epoch:  600  Loss:  5.942326803108967  Accuracy:  0.5890625\n",
            "Epoch:  601  Loss:  5.941267763596853  Accuracy:  0.5890625\n",
            "Epoch:  602  Loss:  5.940210903331975  Accuracy:  0.58984375\n",
            "Epoch:  603  Loss:  5.939156214850921  Accuracy:  0.590625\n",
            "Epoch:  604  Loss:  5.938103691099113  Accuracy:  0.590625\n",
            "Epoch:  605  Loss:  5.937053325428512  Accuracy:  0.590625\n",
            "Epoch:  606  Loss:  5.936005111595037  Accuracy:  0.590625\n",
            "Epoch:  607  Loss:  5.934959043755701  Accuracy:  0.590625\n",
            "Epoch:  608  Loss:  5.933915116465475  Accuracy:  0.590625\n",
            "Epoch:  609  Loss:  5.932873324673863  Accuracy:  0.590625\n",
            "Epoch:  610  Loss:  5.931833663721213  Accuracy:  0.59140625\n",
            "Epoch:  611  Loss:  5.930796129334764  Accuracy:  0.59140625\n",
            "Epoch:  612  Loss:  5.929760717624424  Accuracy:  0.59140625\n",
            "Epoch:  613  Loss:  5.928727425078293  Accuracy:  0.59140625\n",
            "Epoch:  614  Loss:  5.927696248557936  Accuracy:  0.59140625\n",
            "Epoch:  615  Loss:  5.926667185293418  Accuracy:  0.590625\n",
            "Epoch:  616  Loss:  5.925640232878083  Accuracy:  0.590625\n",
            "Epoch:  617  Loss:  5.924615389263116  Accuracy:  0.590625\n",
            "Epoch:  618  Loss:  5.923592652751876  Accuracy:  0.590625\n",
            "Epoch:  619  Loss:  5.9225720219940055  Accuracy:  0.590625\n",
            "Epoch:  620  Loss:  5.921553495979332  Accuracy:  0.590625\n",
            "Epoch:  621  Loss:  5.92053707403157  Accuracy:  0.590625\n",
            "Epoch:  622  Loss:  5.9195227558018155  Accuracy:  0.590625\n",
            "Epoch:  623  Loss:  5.918510541261865  Accuracy:  0.590625\n",
            "Epoch:  624  Loss:  5.917500430697345  Accuracy:  0.58984375\n",
            "Epoch:  625  Loss:  5.916492424700676  Accuracy:  0.58984375\n",
            "Epoch:  626  Loss:  5.915486524163867  Accuracy:  0.58984375\n",
            "Epoch:  627  Loss:  5.914482730271161  Accuracy:  0.58984375\n",
            "Epoch:  628  Loss:  5.9134810444915304  Accuracy:  0.58984375\n",
            "Epoch:  629  Loss:  5.912481468571039  Accuracy:  0.58984375\n",
            "Epoch:  630  Loss:  5.911484004525074  Accuracy:  0.58984375\n",
            "Epoch:  631  Loss:  5.91048865463046  Accuracy:  0.590625\n",
            "Epoch:  632  Loss:  5.909495421417463  Accuracy:  0.590625\n",
            "Epoch:  633  Loss:  5.908504307661704  Accuracy:  0.590625\n",
            "Epoch:  634  Loss:  5.907515316375969  Accuracy:  0.590625\n",
            "Epoch:  635  Loss:  5.906528450801952  Accuracy:  0.590625\n",
            "Epoch:  636  Loss:  5.90554371440192  Accuracy:  0.590625\n",
            "Epoch:  637  Loss:  5.9045611108503255  Accuracy:  0.590625\n",
            "Epoch:  638  Loss:  5.90358064402537  Accuracy:  0.59140625\n",
            "Epoch:  639  Loss:  5.902602318000514  Accuracy:  0.5921875\n",
            "Epoch:  640  Loss:  5.9016261370359855  Accuracy:  0.5921875\n",
            "Epoch:  641  Loss:  5.900652105570245  Accuracy:  0.5921875\n",
            "Epoch:  642  Loss:  5.899680228211453  Accuracy:  0.5921875\n",
            "Epoch:  643  Loss:  5.898710509728938  Accuracy:  0.5921875\n",
            "Epoch:  644  Loss:  5.897742955044685  Accuracy:  0.5921875\n",
            "Epoch:  645  Loss:  5.8967775692248265  Accuracy:  0.5921875\n",
            "Epoch:  646  Loss:  5.895814357471191  Accuracy:  0.5921875\n",
            "Epoch:  647  Loss:  5.894853325112869  Accuracy:  0.5921875\n",
            "Epoch:  648  Loss:  5.893894477597856  Accuracy:  0.5921875\n",
            "Epoch:  649  Loss:  5.892937820484734  Accuracy:  0.5921875\n",
            "Epoch:  650  Loss:  5.89198335943444  Accuracy:  0.5921875\n",
            "Epoch:  651  Loss:  5.891031100202112  Accuracy:  0.5921875\n",
            "Epoch:  652  Loss:  5.890081048629021  Accuracy:  0.5921875\n",
            "Epoch:  653  Loss:  5.889133210634602  Accuracy:  0.5921875\n",
            "Epoch:  654  Loss:  5.888187592208601  Accuracy:  0.5921875\n",
            "Epoch:  655  Loss:  5.887244199403322  Accuracy:  0.5921875\n",
            "Epoch:  656  Loss:  5.886303038326017  Accuracy:  0.5921875\n",
            "Epoch:  657  Loss:  5.88536411513139  Accuracy:  0.5921875\n",
            "Epoch:  658  Loss:  5.8844274360142546  Accuracy:  0.5921875\n",
            "Epoch:  659  Loss:  5.88349300720233  Accuracy:  0.5921875\n",
            "Epoch:  660  Loss:  5.882560834949192  Accuracy:  0.5921875\n",
            "Epoch:  661  Loss:  5.881630925527388  Accuracy:  0.5921875\n",
            "Epoch:  662  Loss:  5.880703285221718  Accuracy:  0.5921875\n",
            "Epoch:  663  Loss:  5.879777920322683  Accuracy:  0.59140625\n",
            "Epoch:  664  Loss:  5.878854837120116  Accuracy:  0.59140625\n",
            "Epoch:  665  Loss:  5.877934041897003  Accuracy:  0.59140625\n",
            "Epoch:  666  Loss:  5.87701554092348  Accuracy:  0.59140625\n",
            "Epoch:  667  Loss:  5.876099340451033  Accuracy:  0.59140625\n",
            "Epoch:  668  Loss:  5.875185446706887  Accuracy:  0.59140625\n",
            "Epoch:  669  Loss:  5.874273865888613  Accuracy:  0.59140625\n",
            "Epoch:  670  Loss:  5.873364604158917  Accuracy:  0.59140625\n",
            "Epoch:  671  Loss:  5.872457667640653  Accuracy:  0.59140625\n",
            "Epoch:  672  Loss:  5.871553062412045  Accuracy:  0.59140625\n",
            "Epoch:  673  Loss:  5.870650794502113  Accuracy:  0.59140625\n",
            "Epoch:  674  Loss:  5.869750869886326  Accuracy:  0.5921875\n",
            "Epoch:  675  Loss:  5.868853294482461  Accuracy:  0.5921875\n",
            "Epoch:  676  Loss:  5.867958074146685  Accuracy:  0.5921875\n",
            "Epoch:  677  Loss:  5.8670652146698545  Accuracy:  0.5921875\n",
            "Epoch:  678  Loss:  5.866174721774033  Accuracy:  0.5921875\n",
            "Epoch:  679  Loss:  5.8652866011092275  Accuracy:  0.5921875\n",
            "Epoch:  680  Loss:  5.86440085825034  Accuracy:  0.5921875\n",
            "Epoch:  681  Loss:  5.863517498694337  Accuracy:  0.59296875\n",
            "Epoch:  682  Loss:  5.862636527857641  Accuracy:  0.59296875\n",
            "Epoch:  683  Loss:  5.861757951073725  Accuracy:  0.59296875\n",
            "Epoch:  684  Loss:  5.8608817735909255  Accuracy:  0.59375\n",
            "Epoch:  685  Loss:  5.860008000570455  Accuracy:  0.59375\n",
            "Epoch:  686  Loss:  5.859136637084638  Accuracy:  0.59375\n",
            "Epoch:  687  Loss:  5.858267688115328  Accuracy:  0.59375\n",
            "Epoch:  688  Loss:  5.857401158552532  Accuracy:  0.59375\n",
            "Epoch:  689  Loss:  5.856537053193229  Accuracy:  0.59296875\n",
            "Epoch:  690  Loss:  5.855675376740367  Accuracy:  0.59296875\n",
            "Epoch:  691  Loss:  5.854816133802055  Accuracy:  0.59296875\n",
            "Epoch:  692  Loss:  5.853959328890916  Accuracy:  0.59296875\n",
            "Epoch:  693  Loss:  5.853104966423618  Accuracy:  0.59296875\n",
            "Epoch:  694  Loss:  5.852253050720559  Accuracy:  0.59296875\n",
            "Epoch:  695  Loss:  5.851403586005712  Accuracy:  0.59296875\n",
            "Epoch:  696  Loss:  5.850556576406609  Accuracy:  0.59296875\n",
            "Epoch:  697  Loss:  5.849712025954462  Accuracy:  0.59296875\n",
            "Epoch:  698  Loss:  5.848869938584402  Accuracy:  0.59296875\n",
            "Epoch:  699  Loss:  5.848030318135848  Accuracy:  0.59296875\n",
            "Epoch:  700  Loss:  5.847193168352963  Accuracy:  0.59296875\n",
            "Epoch:  701  Loss:  5.846358492885207  Accuracy:  0.59296875\n",
            "Epoch:  702  Loss:  5.845526295287971  Accuracy:  0.59296875\n",
            "Epoch:  703  Loss:  5.844696579023273  Accuracy:  0.59296875\n",
            "Epoch:  704  Loss:  5.843869347460506  Accuracy:  0.59296875\n",
            "Epoch:  705  Loss:  5.84304460387722  Accuracy:  0.59296875\n",
            "Epoch:  706  Loss:  5.842222351459929  Accuracy:  0.59296875\n",
            "Epoch:  707  Loss:  5.841402593304918  Accuracy:  0.59296875\n",
            "Epoch:  708  Loss:  5.840585332419039  Accuracy:  0.59296875\n",
            "Epoch:  709  Loss:  5.839770571720479  Accuracy:  0.59296875\n",
            "Epoch:  710  Loss:  5.838958314039473  Accuracy:  0.59296875\n",
            "Epoch:  711  Loss:  5.838148562118958  Accuracy:  0.59296875\n",
            "Epoch:  712  Loss:  5.837341318615131  Accuracy:  0.59296875\n",
            "Epoch:  713  Loss:  5.8365365860979095  Accuracy:  0.59296875\n",
            "Epoch:  714  Loss:  5.835734367051256  Accuracy:  0.5921875\n",
            "Epoch:  715  Loss:  5.834934663873358  Accuracy:  0.5921875\n",
            "Epoch:  716  Loss:  5.83413747887664  Accuracy:  0.5921875\n",
            "Epoch:  717  Loss:  5.83334281428759  Accuracy:  0.59140625\n",
            "Epoch:  718  Loss:  5.832550672246361  Accuracy:  0.5921875\n",
            "Epoch:  719  Loss:  5.831761054806156  Accuracy:  0.5921875\n",
            "Epoch:  720  Loss:  5.830973963932363  Accuracy:  0.59296875\n",
            "Epoch:  721  Loss:  5.830189401501398  Accuracy:  0.59296875\n",
            "Epoch:  722  Loss:  5.829407369299297  Accuracy:  0.59296875\n",
            "Epoch:  723  Loss:  5.828627869019966  Accuracy:  0.5921875\n",
            "Epoch:  724  Loss:  5.8278509022631315  Accuracy:  0.5921875\n",
            "Epoch:  725  Loss:  5.827076470531951  Accuracy:  0.59296875\n",
            "Epoch:  726  Loss:  5.826304575230276  Accuracy:  0.59296875\n",
            "Epoch:  727  Loss:  5.825535217659564  Accuracy:  0.59296875\n",
            "Epoch:  728  Loss:  5.824768399015415  Accuracy:  0.59296875\n",
            "Epoch:  729  Loss:  5.824004120383763  Accuracy:  0.59296875\n",
            "Epoch:  730  Loss:  5.823242382736676  Accuracy:  0.59296875\n",
            "Epoch:  731  Loss:  5.822483186927807  Accuracy:  0.59296875\n",
            "Epoch:  732  Loss:  5.8217265336874835  Accuracy:  0.59296875\n",
            "Epoch:  733  Loss:  5.820972423617433  Accuracy:  0.59375\n",
            "Epoch:  734  Loss:  5.820220857185194  Accuracy:  0.59375\n",
            "Epoch:  735  Loss:  5.819471834718185  Accuracy:  0.59453125\n",
            "Epoch:  736  Loss:  5.818725356397486  Accuracy:  0.5953125\n",
            "Epoch:  737  Loss:  5.817981422251338  Accuracy:  0.5953125\n",
            "Epoch:  738  Loss:  5.817240032148406  Accuracy:  0.59609375\n",
            "Epoch:  739  Loss:  5.8165011857908  Accuracy:  0.59609375\n",
            "Epoch:  740  Loss:  5.815764882706949  Accuracy:  0.5953125\n",
            "Epoch:  741  Loss:  5.815031122244301  Accuracy:  0.5953125\n",
            "Epoch:  742  Loss:  5.814299903561926  Accuracy:  0.59453125\n",
            "Epoch:  743  Loss:  5.813571225623065  Accuracy:  0.59453125\n",
            "Epoch:  744  Loss:  5.812845087187652  Accuracy:  0.59453125\n",
            "Epoch:  745  Loss:  5.812121486804861  Accuracy:  0.59453125\n",
            "Epoch:  746  Loss:  5.811400422805724  Accuracy:  0.59453125\n",
            "Epoch:  747  Loss:  5.810681893295877  Accuracy:  0.59453125\n",
            "Epoch:  748  Loss:  5.809965896148463  Accuracy:  0.59453125\n",
            "Epoch:  749  Loss:  5.8092524289972385  Accuracy:  0.59453125\n",
            "Epoch:  750  Loss:  5.808541489229953  Accuracy:  0.59453125\n",
            "Epoch:  751  Loss:  5.807833073982006  Accuracy:  0.59453125\n",
            "Epoch:  752  Loss:  5.807127180130452  Accuracy:  0.59453125\n",
            "Epoch:  753  Loss:  5.806423804288388  Accuracy:  0.59453125\n",
            "Epoch:  754  Loss:  5.805722942799731  Accuracy:  0.59453125\n",
            "Epoch:  755  Loss:  5.805024591734471  Accuracy:  0.59453125\n",
            "Epoch:  756  Loss:  5.804328746884366  Accuracy:  0.5953125\n",
            "Epoch:  757  Loss:  5.803635403759156  Accuracy:  0.59453125\n",
            "Epoch:  758  Loss:  5.802944557583292  Accuracy:  0.59453125\n",
            "Epoch:  759  Loss:  5.8022562032932  Accuracy:  0.59453125\n",
            "Epoch:  760  Loss:  5.801570335535101  Accuracy:  0.59453125\n",
            "Epoch:  761  Loss:  5.800886948663394  Accuracy:  0.5953125\n",
            "Epoch:  762  Loss:  5.800206036739601  Accuracy:  0.5953125\n",
            "Epoch:  763  Loss:  5.799527593531899  Accuracy:  0.59453125\n",
            "Epoch:  764  Loss:  5.7988516125151985  Accuracy:  0.59453125\n",
            "Epoch:  765  Loss:  5.798178086871819  Accuracy:  0.59453125\n",
            "Epoch:  766  Loss:  5.7975070094926915  Accuracy:  0.59453125\n",
            "Epoch:  767  Loss:  5.796838372979137  Accuracy:  0.59453125\n",
            "Epoch:  768  Loss:  5.796172169645167  Accuracy:  0.59453125\n",
            "Epoch:  769  Loss:  5.7955083915203005  Accuracy:  0.59453125\n",
            "Epoch:  770  Loss:  5.7948470303529005  Accuracy:  0.59453125\n",
            "Epoch:  771  Loss:  5.794188077613972  Accuracy:  0.59453125\n",
            "Epoch:  772  Loss:  5.793531524501444  Accuracy:  0.59453125\n",
            "Epoch:  773  Loss:  5.792877361944866  Accuracy:  0.59453125\n",
            "Epoch:  774  Loss:  5.792225580610553  Accuracy:  0.59453125\n",
            "Epoch:  775  Loss:  5.791576170907087  Accuracy:  0.59453125\n",
            "Epoch:  776  Loss:  5.790929122991214  Accuracy:  0.59453125\n",
            "Epoch:  777  Loss:  5.790284426774059  Accuracy:  0.59453125\n",
            "Epoch:  778  Loss:  5.789642071927678  Accuracy:  0.59453125\n",
            "Epoch:  779  Loss:  5.78900204789188  Accuracy:  0.59453125\n",
            "Epoch:  780  Loss:  5.788364343881328  Accuracy:  0.59453125\n",
            "Epoch:  781  Loss:  5.787728948892871  Accuracy:  0.59453125\n",
            "Epoch:  782  Loss:  5.787095851713091  Accuracy:  0.5953125\n",
            "Epoch:  783  Loss:  5.786465040926041  Accuracy:  0.5953125\n",
            "Epoch:  784  Loss:  5.785836504921148  Accuracy:  0.59609375\n",
            "Epoch:  785  Loss:  5.785210231901262  Accuracy:  0.5953125\n",
            "Epoch:  786  Loss:  5.784586209890814  Accuracy:  0.5953125\n",
            "Epoch:  787  Loss:  5.783964426744094  Accuracy:  0.5953125\n",
            "Epoch:  788  Loss:  5.7833448701535835  Accuracy:  0.59609375\n",
            "Epoch:  789  Loss:  5.782727527658361  Accuracy:  0.59609375\n",
            "Epoch:  790  Loss:  5.782112386652541  Accuracy:  0.59609375\n",
            "Epoch:  791  Loss:  5.781499434393742  Accuracy:  0.5953125\n",
            "Epoch:  792  Loss:  5.7808886580115395  Accuracy:  0.5953125\n",
            "Epoch:  793  Loss:  5.7802800445159335  Accuracy:  0.5953125\n",
            "Epoch:  794  Loss:  5.779673580805763  Accuracy:  0.59453125\n",
            "Epoch:  795  Loss:  5.779069253677093  Accuracy:  0.59453125\n",
            "Epoch:  796  Loss:  5.778467049831537  Accuracy:  0.5953125\n",
            "Epoch:  797  Loss:  5.777866955884515  Accuracy:  0.59453125\n",
            "Epoch:  798  Loss:  5.777268958373428  Accuracy:  0.59453125\n",
            "Epoch:  799  Loss:  5.776673043765736  Accuracy:  0.59453125\n",
            "Epoch:  800  Loss:  5.77607919846694  Accuracy:  0.59453125\n",
            "Epoch:  801  Loss:  5.77548740882844  Accuracy:  0.59375\n",
            "Epoch:  802  Loss:  5.774897661155276  Accuracy:  0.59375\n",
            "Epoch:  803  Loss:  5.7743099417137405  Accuracy:  0.59375\n",
            "Epoch:  804  Loss:  5.773724236738839  Accuracy:  0.59453125\n",
            "Epoch:  805  Loss:  5.77314053244162  Accuracy:  0.59453125\n",
            "Epoch:  806  Loss:  5.772558815016337  Accuracy:  0.59453125\n",
            "Epoch:  807  Loss:  5.771979070647461  Accuracy:  0.59453125\n",
            "Epoch:  808  Loss:  5.771401285516519  Accuracy:  0.59453125\n",
            "Epoch:  809  Loss:  5.770825445808769  Accuracy:  0.59375\n",
            "Epoch:  810  Loss:  5.770251537719702  Accuracy:  0.59375\n",
            "Epoch:  811  Loss:  5.769679547461356  Accuracy:  0.59375\n",
            "Epoch:  812  Loss:  5.769109461268461  Accuracy:  0.59375\n",
            "Epoch:  813  Loss:  5.7685412654043855  Accuracy:  0.59375\n",
            "Epoch:  814  Loss:  5.76797494616691  Accuracy:  0.59375\n",
            "Epoch:  815  Loss:  5.7674104898938  Accuracy:  0.59375\n",
            "Epoch:  816  Loss:  5.766847882968198  Accuracy:  0.59375\n",
            "Epoch:  817  Loss:  5.766287111823814  Accuracy:  0.59375\n",
            "Epoch:  818  Loss:  5.765728162949934  Accuracy:  0.59375\n",
            "Epoch:  819  Loss:  5.765171022896224  Accuracy:  0.59375\n",
            "Epoch:  820  Loss:  5.764615678277357  Accuracy:  0.59375\n",
            "Epoch:  821  Loss:  5.764062115777431  Accuracy:  0.59375\n",
            "Epoch:  822  Loss:  5.763510322154211  Accuracy:  0.59375\n",
            "Epoch:  823  Loss:  5.7629602842431655  Accuracy:  0.59375\n",
            "Epoch:  824  Loss:  5.762411988961331  Accuracy:  0.59375\n",
            "Epoch:  825  Loss:  5.76186542331097  Accuracy:  0.59375\n",
            "Epoch:  826  Loss:  5.761320574383063  Accuracy:  0.59375\n",
            "Epoch:  827  Loss:  5.760777429360606  Accuracy:  0.59375\n",
            "Epoch:  828  Loss:  5.7602359755217245  Accuracy:  0.59375\n",
            "Epoch:  829  Loss:  5.759696200242622  Accuracy:  0.59375\n",
            "Epoch:  830  Loss:  5.7591580910003435  Accuracy:  0.59375\n",
            "Epoch:  831  Loss:  5.758621635375365  Accuracy:  0.59375\n",
            "Epoch:  832  Loss:  5.758086821054028  Accuracy:  0.59296875\n",
            "Epoch:  833  Loss:  5.75755363583079  Accuracy:  0.59296875\n",
            "Epoch:  834  Loss:  5.757022067610327  Accuracy:  0.59375\n",
            "Epoch:  835  Loss:  5.756492104409476  Accuracy:  0.59375\n",
            "Epoch:  836  Loss:  5.755963734359014  Accuracy:  0.59453125\n",
            "Epoch:  837  Loss:  5.755436945705296  Accuracy:  0.59453125\n",
            "Epoch:  838  Loss:  5.754911726811743  Accuracy:  0.59453125\n",
            "Epoch:  839  Loss:  5.754388066160186  Accuracy:  0.59453125\n",
            "Epoch:  840  Loss:  5.753865952352074  Accuracy:  0.59453125\n",
            "Epoch:  841  Loss:  5.753345374109545  Accuracy:  0.59453125\n",
            "Epoch:  842  Loss:  5.752826320276367  Accuracy:  0.59453125\n",
            "Epoch:  843  Loss:  5.752308779818758  Accuracy:  0.59453125\n",
            "Epoch:  844  Loss:  5.751792741826075  Accuracy:  0.59453125\n",
            "Epoch:  845  Loss:  5.751278195511391  Accuracy:  0.5953125\n",
            "Epoch:  846  Loss:  5.75076513021196  Accuracy:  0.5953125\n",
            "Epoch:  847  Loss:  5.7502535353895645  Accuracy:  0.5953125\n",
            "Epoch:  848  Loss:  5.749743400630765  Accuracy:  0.5953125\n",
            "Epoch:  849  Loss:  5.749234715647047  Accuracy:  0.5953125\n",
            "Epoch:  850  Loss:  5.748727470274869  Accuracy:  0.5953125\n",
            "Epoch:  851  Loss:  5.748221654475617  Accuracy:  0.5953125\n",
            "Epoch:  852  Loss:  5.747717258335466  Accuracy:  0.5953125\n",
            "Epoch:  853  Loss:  5.747214272065163  Accuracy:  0.59609375\n",
            "Epoch:  854  Loss:  5.746712685999729  Accuracy:  0.59609375\n",
            "Epoch:  855  Loss:  5.746212490598067  Accuracy:  0.59609375\n",
            "Epoch:  856  Loss:  5.745713676442522  Accuracy:  0.59609375\n",
            "Epoch:  857  Loss:  5.74521623423834  Accuracy:  0.59609375\n",
            "Epoch:  858  Loss:  5.744720154813081  Accuracy:  0.59609375\n",
            "Epoch:  859  Loss:  5.744225429115964  Accuracy:  0.59609375\n",
            "Epoch:  860  Loss:  5.743732048217145  Accuracy:  0.59609375\n",
            "Epoch:  861  Loss:  5.743240003306934  Accuracy:  0.596875\n",
            "Epoch:  862  Loss:  5.742749285694978  Accuracy:  0.596875\n",
            "Epoch:  863  Loss:  5.742259886809364  Accuracy:  0.596875\n",
            "Epoch:  864  Loss:  5.741771798195698  Accuracy:  0.596875\n",
            "Epoch:  865  Loss:  5.741285011516117  Accuracy:  0.59609375\n",
            "Epoch:  866  Loss:  5.740799518548283  Accuracy:  0.596875\n",
            "Epoch:  867  Loss:  5.740315311184308  Accuracy:  0.596875\n",
            "Epoch:  868  Loss:  5.739832381429663  Accuracy:  0.59765625\n",
            "Epoch:  869  Loss:  5.739350721402046  Accuracy:  0.59765625\n",
            "Epoch:  870  Loss:  5.738870323330206  Accuracy:  0.5984375\n",
            "Epoch:  871  Loss:  5.738391179552757  Accuracy:  0.5984375\n",
            "Epoch:  872  Loss:  5.737913282516944  Accuracy:  0.59765625\n",
            "Epoch:  873  Loss:  5.737436624777399  Accuracy:  0.59765625\n",
            "Epoch:  874  Loss:  5.736961198994853  Accuracy:  0.59765625\n",
            "Epoch:  875  Loss:  5.736486997934848  Accuracy:  0.596875\n",
            "Epoch:  876  Loss:  5.736014014466417  Accuracy:  0.596875\n",
            "Epoch:  877  Loss:  5.735542241560745  Accuracy:  0.596875\n",
            "Epoch:  878  Loss:  5.735071672289815  Accuracy:  0.59765625\n",
            "Epoch:  879  Loss:  5.734602299825042  Accuracy:  0.59765625\n",
            "Epoch:  880  Loss:  5.73413411743589  Accuracy:  0.59765625\n",
            "Epoch:  881  Loss:  5.733667118488481  Accuracy:  0.59765625\n",
            "Epoch:  882  Loss:  5.733201296444185  Accuracy:  0.59765625\n",
            "Epoch:  883  Loss:  5.732736644858218  Accuracy:  0.59765625\n",
            "Epoch:  884  Loss:  5.732273157378215  Accuracy:  0.59765625\n",
            "Epoch:  885  Loss:  5.731810827742803  Accuracy:  0.59765625\n",
            "Epoch:  886  Loss:  5.731349649780178  Accuracy:  0.5984375\n",
            "Epoch:  887  Loss:  5.730889617406666  Accuracy:  0.59921875\n",
            "Epoch:  888  Loss:  5.7304307246252835  Accuracy:  0.59921875\n",
            "Epoch:  889  Loss:  5.729972965524308  Accuracy:  0.59921875\n",
            "Epoch:  890  Loss:  5.729516334275833  Accuracy:  0.59921875\n",
            "Epoch:  891  Loss:  5.729060825134333  Accuracy:  0.59921875\n",
            "Epoch:  892  Loss:  5.728606432435231  Accuracy:  0.59921875\n",
            "Epoch:  893  Loss:  5.728153150593458  Accuracy:  0.59921875\n",
            "Epoch:  894  Loss:  5.727700974102033  Accuracy:  0.59921875\n",
            "Epoch:  895  Loss:  5.727249897530634  Accuracy:  0.59921875\n",
            "Epoch:  896  Loss:  5.726799915524177  Accuracy:  0.59921875\n",
            "Epoch:  897  Loss:  5.726351022801403  Accuracy:  0.59921875\n",
            "Epoch:  898  Loss:  5.725903214153477  Accuracy:  0.59921875\n",
            "Epoch:  899  Loss:  5.725456484442581  Accuracy:  0.6\n",
            "Epoch:  900  Loss:  5.725010828600529  Accuracy:  0.6\n",
            "Epoch:  901  Loss:  5.724566241627381  Accuracy:  0.6\n",
            "Epoch:  902  Loss:  5.724122718590074  Accuracy:  0.6\n",
            "Epoch:  903  Loss:  5.723680254621056  Accuracy:  0.6\n",
            "Epoch:  904  Loss:  5.723238844916937  Accuracy:  0.59921875\n",
            "Epoch:  905  Loss:  5.722798484737149  Accuracy:  0.59921875\n",
            "Epoch:  906  Loss:  5.722359169402611  Accuracy:  0.59921875\n",
            "Epoch:  907  Loss:  5.721920894294424  Accuracy:  0.59921875\n",
            "Epoch:  908  Loss:  5.721483654852554  Accuracy:  0.59921875\n",
            "Epoch:  909  Loss:  5.721047446574549  Accuracy:  0.59921875\n",
            "Epoch:  910  Loss:  5.720612265014263  Accuracy:  0.5984375\n",
            "Epoch:  911  Loss:  5.720178105780588  Accuracy:  0.5984375\n",
            "Epoch:  912  Loss:  5.719744964536206  Accuracy:  0.59921875\n",
            "Epoch:  913  Loss:  5.71931283699636  Accuracy:  0.59921875\n",
            "Epoch:  914  Loss:  5.718881718927629  Accuracy:  0.59921875\n",
            "Epoch:  915  Loss:  5.718451606146731  Accuracy:  0.59921875\n",
            "Epoch:  916  Loss:  5.718022494519329  Accuracy:  0.59921875\n",
            "Epoch:  917  Loss:  5.717594379958865  Accuracy:  0.59921875\n",
            "Epoch:  918  Loss:  5.717167258425404  Accuracy:  0.59921875\n",
            "Epoch:  919  Loss:  5.716741125924495  Accuracy:  0.59921875\n",
            "Epoch:  920  Loss:  5.716315978506046  Accuracy:  0.59921875\n",
            "Epoch:  921  Loss:  5.715891812263232  Accuracy:  0.59921875\n",
            "Epoch:  922  Loss:  5.715468623331395  Accuracy:  0.6\n",
            "Epoch:  923  Loss:  5.715046407886986  Accuracy:  0.6\n",
            "Epoch:  924  Loss:  5.714625162146506  Accuracy:  0.6\n",
            "Epoch:  925  Loss:  5.71420488236548  Accuracy:  0.6\n",
            "Epoch:  926  Loss:  5.713785564837439  Accuracy:  0.6\n",
            "Epoch:  927  Loss:  5.713367205892922  Accuracy:  0.6\n",
            "Epoch:  928  Loss:  5.7129498018985085  Accuracy:  0.6\n",
            "Epoch:  929  Loss:  5.712533349255848  Accuracy:  0.6\n",
            "Epoch:  930  Loss:  5.712117844400728  Accuracy:  0.6\n",
            "Epoch:  931  Loss:  5.7117032838021515  Accuracy:  0.6\n",
            "Epoch:  932  Loss:  5.711289663961437  Accuracy:  0.6\n",
            "Epoch:  933  Loss:  5.710876981411332  Accuracy:  0.6\n",
            "Epoch:  934  Loss:  5.710465232715156  Accuracy:  0.6\n",
            "Epoch:  935  Loss:  5.710054414465956  Accuracy:  0.6\n",
            "Epoch:  936  Loss:  5.709644523285678  Accuracy:  0.6\n",
            "Epoch:  937  Loss:  5.709235555824369  Accuracy:  0.6\n",
            "Epoch:  938  Loss:  5.70882750875939  Accuracy:  0.6\n",
            "Epoch:  939  Loss:  5.708420378794645  Accuracy:  0.6\n",
            "Epoch:  940  Loss:  5.708014162659849  Accuracy:  0.6\n",
            "Epoch:  941  Loss:  5.707608857109789  Accuracy:  0.6\n",
            "Epoch:  942  Loss:  5.707204458923625  Accuracy:  0.6\n",
            "Epoch:  943  Loss:  5.706800964904202  Accuracy:  0.6\n",
            "Epoch:  944  Loss:  5.706398371877382  Accuracy:  0.6\n",
            "Epoch:  945  Loss:  5.705996676691397  Accuracy:  0.6\n",
            "Epoch:  946  Loss:  5.705595876216224  Accuracy:  0.6\n",
            "Epoch:  947  Loss:  5.70519596734297  Accuracy:  0.60078125\n",
            "Epoch:  948  Loss:  5.704796946983292  Accuracy:  0.6\n",
            "Epoch:  949  Loss:  5.70439881206882  Accuracy:  0.6\n",
            "Epoch:  950  Loss:  5.704001559550612  Accuracy:  0.59921875\n",
            "Epoch:  951  Loss:  5.703605186398618  Accuracy:  0.59921875\n",
            "Epoch:  952  Loss:  5.703209689601172  Accuracy:  0.59921875\n",
            "Epoch:  953  Loss:  5.7028150661644945  Accuracy:  0.59921875\n",
            "Epoch:  954  Loss:  5.702421313112223  Accuracy:  0.59921875\n",
            "Epoch:  955  Loss:  5.702028427484953  Accuracy:  0.5984375\n",
            "Epoch:  956  Loss:  5.7016364063398015  Accuracy:  0.5984375\n",
            "Epoch:  957  Loss:  5.701245246749987  Accuracy:  0.5984375\n",
            "Epoch:  958  Loss:  5.70085494580443  Accuracy:  0.5984375\n",
            "Epoch:  959  Loss:  5.70046550060737  Accuracy:  0.5984375\n",
            "Epoch:  960  Loss:  5.700076908278001  Accuracy:  0.5984375\n",
            "Epoch:  961  Loss:  5.699689165950121  Accuracy:  0.5984375\n",
            "Epoch:  962  Loss:  5.699302270771807  Accuracy:  0.5984375\n",
            "Epoch:  963  Loss:  5.6989162199050964  Accuracy:  0.5984375\n",
            "Epoch:  964  Loss:  5.698531010525697  Accuracy:  0.5984375\n",
            "Epoch:  965  Loss:  5.6981466398227  Accuracy:  0.5984375\n",
            "Epoch:  966  Loss:  5.697763104998327  Accuracy:  0.5984375\n",
            "Epoch:  967  Loss:  5.697380403267677  Accuracy:  0.5984375\n",
            "Epoch:  968  Loss:  5.696998531858494  Accuracy:  0.5984375\n",
            "Epoch:  969  Loss:  5.696617488010958  Accuracy:  0.5984375\n",
            "Epoch:  970  Loss:  5.696237268977479  Accuracy:  0.5984375\n",
            "Epoch:  971  Loss:  5.695857872022522  Accuracy:  0.5984375\n",
            "Epoch:  972  Loss:  5.695479294422424  Accuracy:  0.59921875\n",
            "Epoch:  973  Loss:  5.69510153346525  Accuracy:  0.59921875\n",
            "Epoch:  974  Loss:  5.694724586450645  Accuracy:  0.59921875\n",
            "Epoch:  975  Loss:  5.694348450689715  Accuracy:  0.5984375\n",
            "Epoch:  976  Loss:  5.693973123504899  Accuracy:  0.5984375\n",
            "Epoch:  977  Loss:  5.693598602229889  Accuracy:  0.5984375\n",
            "Epoch:  978  Loss:  5.69322488420952  Accuracy:  0.5984375\n",
            "Epoch:  979  Loss:  5.692851966799713  Accuracy:  0.5984375\n",
            "Epoch:  980  Loss:  5.692479847367403  Accuracy:  0.5984375\n",
            "Epoch:  981  Loss:  5.692108523290492  Accuracy:  0.5984375\n",
            "Epoch:  982  Loss:  5.691737991957806  Accuracy:  0.5984375\n",
            "Epoch:  983  Loss:  5.691368250769072  Accuracy:  0.5984375\n",
            "Epoch:  984  Loss:  5.690999297134894  Accuracy:  0.5984375\n",
            "Epoch:  985  Loss:  5.69063112847675  Accuracy:  0.5984375\n",
            "Epoch:  986  Loss:  5.690263742226994  Accuracy:  0.5984375\n",
            "Epoch:  987  Loss:  5.689897135828874  Accuracy:  0.5984375\n",
            "Epoch:  988  Loss:  5.689531306736537  Accuracy:  0.5984375\n",
            "Epoch:  989  Loss:  5.689166252415083  Accuracy:  0.5984375\n",
            "Epoch:  990  Loss:  5.688801970340587  Accuracy:  0.5984375\n",
            "Epoch:  991  Loss:  5.688438458000154  Accuracy:  0.59765625\n",
            "Epoch:  992  Loss:  5.68807571289197  Accuracy:  0.59765625\n",
            "Epoch:  993  Loss:  5.687713732525367  Accuracy:  0.59765625\n",
            "Epoch:  994  Loss:  5.687352514420895  Accuracy:  0.59765625\n",
            "Epoch:  995  Loss:  5.686992056110391  Accuracy:  0.596875\n",
            "Epoch:  996  Loss:  5.686632355137071  Accuracy:  0.596875\n",
            "Epoch:  997  Loss:  5.686273409055607  Accuracy:  0.596875\n",
            "Epoch:  998  Loss:  5.685915215432229  Accuracy:  0.596875\n",
            "Epoch:  999  Loss:  5.685557771844822  Accuracy:  0.596875\n"
          ]
        }
      ],
      "source": [
        "model = MLP_classification(0.1, \"tanh\", \"batch\", 2, [10, 10])\n",
        "model.train(train_x, train_y, validation_x, validation_y, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5hgIGj0Iwbb",
        "outputId": "04c97a10-0902-4add-856b-c04682ee58d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(200, 19)\n"
          ]
        }
      ],
      "source": [
        "# fit model on test data\n",
        "binary_predictions = model.predict(test_x)\n",
        "# print(binary_predictions.shape)\n",
        "# predictions.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "mM1FYnlmIwbc"
      },
      "outputs": [],
      "source": [
        "def hamming_distance_accuracy(true_labels, predicted_labels):\n",
        "    if true_labels.shape != predicted_labels.shape:\n",
        "        raise ValueError(\"Input vectors must have the same shape.\")\n",
        "\n",
        "    num_samples, num_classes = true_labels.shape\n",
        "\n",
        "    hamming_distances = np.sum(true_labels != predicted_labels, axis=1)\n",
        "\n",
        "    per_sample_accuracies = 1 - (hamming_distances / num_classes)\n",
        "\n",
        "    accuracy = np.mean(per_sample_accuracies)\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqJ30u8IIwbc",
        "outputId": "61338d35-41a5-4f8b-9d20-0b30090f10be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hamming distance accuracy:  0.62375\n"
          ]
        }
      ],
      "source": [
        "# find accuracy as the hamming distance accuracy\n",
        "print(\"Hamming distance accuracy: \", hamming_distance_accuracy(test_y, binary_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6YrWhZNIwbc",
        "outputId": "c83b4c9d-20f8-4829-af85-1fccd1b0b638"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.18      0.24        68\n",
            "           1       0.24      0.09      0.13        66\n",
            "           2       0.15      0.03      0.05        63\n",
            "           3       0.41      0.21      0.27        63\n",
            "           4       0.30      0.15      0.20        59\n",
            "           5       0.31      0.18      0.23        66\n",
            "           6       0.38      0.07      0.12        71\n",
            "           7       0.41      0.23      0.29        71\n",
            "\n",
            "   micro avg       0.33      0.14      0.20       527\n",
            "   macro avg       0.32      0.14      0.19       527\n",
            "weighted avg       0.32      0.14      0.19       527\n",
            " samples avg       0.22      0.12      0.15       527\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# printing sklearn classification report\n",
        "print(classification_report(test_y, binary_predictions, zero_division=0))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sklearn-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
